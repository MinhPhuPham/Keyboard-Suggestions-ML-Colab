{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TinyBERT Keyboard Suggestion Model Training\n",
                "\n",
                "This notebook trains a keyboard suggestion model using **TinyBERT** with multi-task learning.\n",
                "\n",
                "**Features:**\n",
                "1. Word Completion: \"Hel\" â†’ [\"Hello\", \"Help\", \"Helping\"]\n",
                "2. Next-Word Prediction: \"How are\" â†’ [\"you\", \"they\", \"we\"]\n",
                "3. Typo Correction: \"Thers\" â†’ [\"There\", \"Theirs\", \"Therapy\"]\n",
                "4. Gibberish Detection: Heuristic (no ML)\n",
                "\n",
                "**Model Specifications:**\n",
                "- Base: TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
                "- Target Size: <5MB (after INT8 quantization)\n",
                "- Latency: <50ms on mobile\n",
                "- RAM Usage: <30MB runtime\n",
                "- Deployment: iOS (CoreML) + Android (TFLite)\n",
                "\n",
                "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
                "\n",
                "**Data Sources (Google Drive):**\n",
                "- `single_word_freq.csv` - Word frequencies for completion\n",
                "- `keyboard_training_data.txt` - Custom corpus for next-word\n",
                "- `misspelled.csv` - Typo correction pairs\n",
                "\n",
                "---\n",
                "\n",
                "**Instructions:**\n",
                "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
                "2. Run all cells\n",
                "3. Model will be saved to Google Drive\n",
                "4. Download for mobile deployment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if running in Colab\n",
                "import os\n",
                "\n",
                "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"âœ“ Running in Google Colab\")\n",
                "    \n",
                "    # Mount Google Drive\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # Define Drive directory\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "    \n",
                "    # Create directories\n",
                "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
                "    \n",
                "    print(f\"âœ“ Google Drive mounted\")\n",
                "    print(f\"âœ“ Project directory: {DRIVE_DIR}\")\n",
                "else:\n",
                "    print(\"âœ“ Running locally\")\n",
                "    DRIVE_DIR = './data'  # Local fallback\n",
                "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers torch datasets accelerate\n",
                "!pip install -q scikit-learn tqdm\n",
                "print(\"âœ“ Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verify Datasets in Google Drive\n",
                "\n",
                "**Expected datasets in Google Drive:**\n",
                "- `{DRIVE_DIR}/datasets/single_word_freq.csv`\n",
                "- `{DRIVE_DIR}/datasets/keyboard_training_data.txt`\n",
                "- `{DRIVE_DIR}/datasets/misspelled.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "print(\"Checking datasets in Google Drive...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Define dataset paths\n",
                "WORD_FREQ_PATH = f\"{DRIVE_DIR}/datasets/single_word_freq.csv\"\n",
                "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
                "TYPO_PATH = f\"{DRIVE_DIR}/datasets/misspelled.csv\"\n",
                "\n",
                "# Check each dataset\n",
                "datasets_ok = True\n",
                "\n",
                "if os.path.exists(WORD_FREQ_PATH):\n",
                "    with open(WORD_FREQ_PATH, 'r', encoding='utf-8') as f:\n",
                "        word_count = sum(1 for _ in f) - 1  # Subtract header\n",
                "    print(f\"âœ“ single_word_freq.csv: {word_count:,} words\")\n",
                "else:\n",
                "    print(f\"âœ— Missing: {WORD_FREQ_PATH}\")\n",
                "    datasets_ok = False\n",
                "\n",
                "if os.path.exists(CORPUS_PATH):\n",
                "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
                "        line_count = sum(1 for _ in f)\n",
                "    print(f\"âœ“ keyboard_training_data.txt: {line_count:,} lines\")\n",
                "else:\n",
                "    print(f\"âœ— Missing: {CORPUS_PATH}\")\n",
                "    datasets_ok = False\n",
                "\n",
                "if os.path.exists(TYPO_PATH):\n",
                "    with open(TYPO_PATH, 'r', encoding='utf-8') as f:\n",
                "        typo_count = sum(1 for _ in f) - 1  # Subtract header\n",
                "    print(f\"âœ“ misspelled.csv: {typo_count:,} entries\")\n",
                "else:\n",
                "    print(f\"âœ— Missing: {TYPO_PATH}\")\n",
                "    datasets_ok = False\n",
                "\n",
                "print(\"=\"*60)\n",
                "if datasets_ok:\n",
                "    print(\"âœ… All datasets found!\")\n",
                "else:\n",
                "    print(\"âš ï¸  Some datasets are missing. Please upload them to Google Drive.\")\n",
                "    print(\"\\nExpected location: {DRIVE_DIR}/datasets/\")\n",
                "    print(\"Required files:\")\n",
                "    print(\"  - single_word_freq.csv (format: word,count_frequency)\")\n",
                "    print(\"  - keyboard_training_data.txt (plain text sentences)\")\n",
                "    print(\"  - misspelled.csv (format: number,correct_word,misspelled_words)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Training Data\n",
                "\n",
                "Generate training pairs for all 3 tasks from your existing datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import random\n",
                "import csv\n",
                "from typing import List, Tuple\n",
                "\n",
                "random.seed(42)\n",
                "\n",
                "def prepare_word_completion_data(word_freq_path: str, max_samples: int = 50000) -> List[dict]:\n",
                "    \"\"\"Generate word completion training pairs from single_word_freq.csv\"\"\"\n",
                "    print(\"\\nGenerating word completion data...\")\n",
                "    \n",
                "    samples = []\n",
                "    words_with_freq = []\n",
                "    \n",
                "    # Read words with frequencies\n",
                "    with open(word_freq_path, 'r', encoding='utf-8') as f:\n",
                "        reader = csv.DictReader(f)\n",
                "        for row in reader:\n",
                "            word = row['word'].strip().lower()\n",
                "            freq = int(row.get('count_frequency', 1))\n",
                "            if len(word) >= 3:  # Only words with 3+ chars\n",
                "                words_with_freq.append((word, freq))\n",
                "    \n",
                "    # Sort by frequency (higher first)\n",
                "    words_with_freq.sort(key=lambda x: x[1], reverse=True)\n",
                "    \n",
                "    # Generate samples (weighted by frequency)\n",
                "    for word, freq in words_with_freq:\n",
                "        if len(samples) >= max_samples:\n",
                "            break\n",
                "        \n",
                "        # Generate multiple prefixes for common words\n",
                "        num_samples = min(3, max(1, freq // 1000))  # More samples for frequent words\n",
                "        \n",
                "        for _ in range(num_samples):\n",
                "            if len(samples) >= max_samples:\n",
                "                break\n",
                "            \n",
                "            # Random prefix length (50-80% of word)\n",
                "            prefix_len = random.randint(max(1, len(word) // 2), max(2, int(len(word) * 0.8)))\n",
                "            prefix = word[:prefix_len]\n",
                "            \n",
                "            samples.append({\n",
                "                'input': prefix,\n",
                "                'target': word,\n",
                "                'task': 'completion'\n",
                "            })\n",
                "    \n",
                "    print(f\"  Generated {len(samples):,} completion pairs\")\n",
                "    return samples\n",
                "\n",
                "def prepare_nextword_data(corpus_path: str, max_samples: int = 100000, context_length: int = 3) -> List[dict]:\n",
                "    \"\"\"Generate next-word prediction pairs from keyboard_training_data.txt\"\"\"\n",
                "    print(\"\\nGenerating next-word prediction data...\")\n",
                "    \n",
                "    samples = []\n",
                "    \n",
                "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            if len(samples) >= max_samples:\n",
                "                break\n",
                "            \n",
                "            line = line.strip().lower()\n",
                "            words = line.split()\n",
                "            \n",
                "            # Skip short sentences\n",
                "            if len(words) < context_length + 1:\n",
                "                continue\n",
                "            \n",
                "            # Generate multiple samples from each sentence\n",
                "            for i in range(len(words) - context_length):\n",
                "                context = ' '.join(words[i:i+context_length])\n",
                "                target = words[i+context_length]\n",
                "                \n",
                "                # Filter out punctuation-only targets\n",
                "                if target.isalpha() and len(target) > 1:\n",
                "                    samples.append({\n",
                "                        'input': context,\n",
                "                        'target': target,\n",
                "                        'task': 'nextword'\n",
                "                    })\n",
                "                \n",
                "                if len(samples) >= max_samples:\n",
                "                    break\n",
                "    \n",
                "    print(f\"  Generated {len(samples):,} next-word pairs\")\n",
                "    return samples\n",
                "\n",
                "def prepare_typo_data(typo_path: str, max_samples: int = 20000) -> List[dict]:\n",
                "    \"\"\"Generate typo correction pairs from misspelled.csv\"\"\"\n",
                "    print(\"\\nGenerating typo correction data...\")\n",
                "    \n",
                "    samples = []\n",
                "    \n",
                "    with open(typo_path, 'r', encoding='utf-8') as f:\n",
                "        reader = csv.DictReader(f)\n",
                "        for row in reader:\n",
                "            if len(samples) >= max_samples:\n",
                "                break\n",
                "            \n",
                "            correct = row['correct_word'].strip().lower()\n",
                "            misspelled_list = row['misspelled_words'].strip().lower()\n",
                "            \n",
                "            # Split multiple misspellings (comma or space separated)\n",
                "            typos = [t.strip() for t in misspelled_list.replace(',', ' ').split() if t.strip()]\n",
                "            \n",
                "            for typo in typos:\n",
                "                if len(samples) >= max_samples:\n",
                "                    break\n",
                "                \n",
                "                if typo and typo != correct:\n",
                "                    samples.append({\n",
                "                        'input': typo,\n",
                "                        'target': correct,\n",
                "                        'task': 'typo'\n",
                "                    })\n",
                "    \n",
                "    print(f\"  Generated {len(samples):,} typo pairs\")\n",
                "    return samples\n",
                "\n",
                "# Generate all datasets\n",
                "print(\"Preparing training datasets...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "train_path = f\"{output_dir}/train.jsonl\"\n",
                "val_path = f\"{output_dir}/val.jsonl\"\n",
                "\n",
                "# Check if processed datasets already exist\n",
                "if os.path.exists(train_path) and os.path.exists(val_path):\n",
                "    print(\"âœ“ Processed datasets found in Drive!\")\n",
                "    print(f\"  Train: {train_path}\")\n",
                "    print(f\"  Val: {val_path}\")\n",
                "    \n",
                "    # Count samples\n",
                "    with open(train_path, 'r') as f:\n",
                "        train_count = sum(1 for _ in f)\n",
                "    with open(val_path, 'r') as f:\n",
                "        val_count = sum(1 for _ in f)\n",
                "    print(f\"  Train samples: {train_count:,}\")\n",
                "    print(f\"  Val samples: {val_count:,}\")\n",
                "else:\n",
                "    print(\"Generating training datasets from scratch...\")\n",
                "    \n",
                "    # Generate each task\n",
                "    completion_samples = prepare_word_completion_data(WORD_FREQ_PATH, max_samples=50000)\n",
                "    nextword_samples = prepare_nextword_data(CORPUS_PATH, max_samples=100000, context_length=3)\n",
                "    typo_samples = prepare_typo_data(TYPO_PATH, max_samples=20000)\n",
                "    \n",
                "    # Combine all samples\n",
                "    all_samples = completion_samples + nextword_samples + typo_samples\n",
                "    random.shuffle(all_samples)\n",
                "    \n",
                "    # Split train/val (90/10)\n",
                "    split_idx = int(len(all_samples) * 0.9)\n",
                "    train_samples = all_samples[:split_idx]\n",
                "    val_samples = all_samples[split_idx:]\n",
                "    \n",
                "    # Save to JSONL\n",
                "    with open(train_path, 'w', encoding='utf-8') as f:\n",
                "        for sample in train_samples:\n",
                "            f.write(json.dumps(sample) + '\\n')\n",
                "    \n",
                "    with open(val_path, 'w', encoding='utf-8') as f:\n",
                "        for sample in val_samples:\n",
                "            f.write(json.dumps(sample) + '\\n')\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"âœ“ Dataset generation complete!\")\n",
                "    print(f\"  Total samples: {len(all_samples):,}\")\n",
                "    print(f\"  Train: {len(train_samples):,} ({train_path})\")\n",
                "    print(f\"  Val: {len(val_samples):,} ({val_path})\")\n",
                "    print(f\"\\n  Task distribution:\")\n",
                "    print(f\"    Completion: {len(completion_samples):,}\")\n",
                "    print(f\"    Next-word: {len(nextword_samples):,}\")\n",
                "    print(f\"    Typo: {len(typo_samples):,}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ“ Datasets ready for training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load TinyBERT Model and Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
                "import torch\n",
                "\n",
                "print(\"Loading TinyBERT for Masked Language Modeling...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
                "MODEL_NAME = \"google/bert_uncased_L-4_H-312_A-12\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)\n",
                "\n",
                "print(f\"âœ“ Model loaded on {device}\")\n",
                "print(f\"  Model: {MODEL_NAME}\")\n",
                "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"  Layers: 4, Hidden: 312, Heads: 12\")\n",
                "print(f\"  Model size: ~55MB (FP32) â†’ ~5MB (INT8 quantized)\")\n",
                "print(f\"  Target latency: <50ms on mobile\")\n",
                "print(f\"  Target RAM: <30MB runtime\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Prepare Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "import json\n",
                "import torch\n",
                "\n",
                "class KeyboardDataset(Dataset):\n",
                "    \"\"\"BERT MLM dataset for keyboard suggestions.\"\"\"\n",
                "    \n",
                "    def __init__(self, data_path, tokenizer, max_length=16):\n",
                "        self.data = []\n",
                "        with open(data_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                self.data.append(json.loads(line))\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "        text_input = item['input']\n",
                "        target_word = item['target']\n",
                "        task = item.get('task', 'completion')\n",
                "        \n",
                "        # Add [MASK] for prediction\n",
                "        # - Completion: \"hel [MASK]\" â†’ predict \"lo\" (hello)\n",
                "        # - Next-word: \"how are [MASK]\" â†’ predict \"you\"\n",
                "        # - Typo: \"thers [MASK]\" â†’ predict \"there\"\n",
                "        text_input = f\"{text_input} {self.tokenizer.mask_token}\"\n",
                "        \n",
                "        # Tokenize\n",
                "        inputs = self.tokenizer(\n",
                "            text_input,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        # Labels: -100 everywhere except [MASK]\n",
                "        labels = torch.full(inputs['input_ids'].shape, -100, dtype=torch.long)\n",
                "        \n",
                "        # Get target token ID (first token of target word)\n",
                "        target_tokens = self.tokenizer.tokenize(target_word)\n",
                "        target_id = self.tokenizer.convert_tokens_to_ids(target_tokens[0]) if target_tokens else self.tokenizer.unk_token_id\n",
                "        \n",
                "        # Set label at [MASK] position\n",
                "        mask_positions = (inputs['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
                "        if len(mask_positions[1]) > 0:\n",
                "            labels[0, mask_positions[1][0]] = target_id\n",
                "        \n",
                "        return {\n",
                "            'input_ids': inputs['input_ids'].squeeze(),\n",
                "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
                "            'labels': labels.squeeze(),\n",
                "            'task': task\n",
                "        }\n",
                "\n",
                "print(\"Loading training data...\")\n",
                "train_dataset = KeyboardDataset(train_path, tokenizer, max_length=16)\n",
                "val_dataset = KeyboardDataset(val_path, tokenizer, max_length=16)\n",
                "\n",
                "print(f\"âœ“ Train samples: {len(train_dataset):,}\")\n",
                "print(f\"âœ“ Val samples: {len(val_dataset):,}\")\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
                "\n",
                "print(f\"âœ“ Batch size: 32 (train), 64 (val)\")\n",
                "print(f\"âœ“ Max sequence length: 16 tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.optim import AdamW\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "NUM_EPOCHS = 3\n",
                "LEARNING_RATE = 3e-5  # Lower LR for fine-tuning\n",
                "SAVE_STEPS = 1000\n",
                "\n",
                "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "print(\"Starting training...\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"Learning rate: {LEARNING_RATE}\")\n",
                "print(f\"Optimizer: AdamW\")\n",
                "print(f\"Save checkpoints every: {SAVE_STEPS} steps\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "global_step = 0\n",
                "best_val_loss = float('inf')\n",
                "\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
                "    \n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
                "    for batch in progress_bar:\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
                "        loss = outputs.loss\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        global_step += 1\n",
                "        progress_bar.set_postfix({'loss': loss.item()})\n",
                "        \n",
                "        if global_step % SAVE_STEPS == 0:\n",
                "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
                "            model.save_pretrained(checkpoint_dir)\n",
                "            tokenizer.save_pretrained(checkpoint_dir)\n",
                "            print(f\"\\nâœ“ Checkpoint saved: {checkpoint_dir}\")\n",
                "    \n",
                "    avg_train_loss = train_loss / len(train_loader)\n",
                "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
                "            val_loss += outputs.loss.item()\n",
                "    \n",
                "    avg_val_loss = val_loss / len(val_loader)\n",
                "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if avg_val_loss < best_val_loss:\n",
                "        best_val_loss = avg_val_loss\n",
                "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
                "        model.save_pretrained(best_model_dir)\n",
                "        tokenizer.save_pretrained(best_model_dir)\n",
                "        print(f\"  âœ“ Best model saved: {best_model_dir}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ“ Training complete!\")\n",
                "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
                "print(f\"  Total steps: {global_step}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export and Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export and save the BEST model (for iOS/Android deployment)\n",
                "import shutil\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Exporting BEST model for deployment...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load the best model (saved during training)\n",
                "best_model_path = f\"{DRIVE_DIR}/models/best_model\"\n",
                "print(f\"\\nLoading best model from: {best_model_path}\")\n",
                "\n",
                "# Load best model\n",
                "best_model = AutoModelForMaskedLM.from_pretrained(best_model_path)\n",
                "best_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
                "print(\"âœ“ Best model loaded\")\n",
                "\n",
                "# 1. Save to Google Drive as 'final' (for deployment)\n",
                "drive_model_dir = f\"{DRIVE_DIR}/models/tinybert_keyboard_final\"\n",
                "best_model.save_pretrained(drive_model_dir)\n",
                "best_tokenizer.save_pretrained(drive_model_dir)\n",
                "print(f\"\\nâœ… Saved to Google Drive: {drive_model_dir}\")\n",
                "\n",
                "# 2. Create downloadable zip\n",
                "local_model_dir = \"/content/tinybert_keyboard_model\"\n",
                "best_model.save_pretrained(local_model_dir)\n",
                "best_tokenizer.save_pretrained(local_model_dir)\n",
                "\n",
                "zip_path = \"/content/tinybert_keyboard_model.zip\"\n",
                "shutil.make_archive(\"/content/tinybert_keyboard_model\", 'zip', local_model_dir)\n",
                "print(f\"\\nâœ… Created zip: {zip_path}\")\n",
                "\n",
                "# 3. Download to local device\n",
                "if IN_COLAB:\n",
                "    print(\"\\nðŸ“¥ Downloading BEST model to your computer...\")\n",
                "    files.download(zip_path)\n",
                "    print(\"âœ… Download started! Check your Downloads folder.\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… Export complete!\")\n",
                "print(\"\\nâš ï¸  IMPORTANT: This is the BEST model (lowest validation loss)\")\n",
                "print(\"   Use this for iOS/Android deployment!\")\n",
                "print(\"\\nModel saved to:\")\n",
                "print(f\"  1. Google Drive: {drive_model_dir}\")\n",
                "print(f\"  2. Local download: tinybert_keyboard_model.zip\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"  1. Run Section 8 (Export to CoreML for iOS)\")\n",
                "print(\"  2. Run Section 9 (Export to TFLite for Android)\")\n",
                "print(\"\\nðŸ’¡ Both iOS and Android exports will use this BEST model!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export to CoreML (iOS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to CoreML for iOS with INT8 quantization\n",
                "!pip install -q coremltools\n",
                "\n",
                "import coremltools as ct\n",
                "from coremltools.models.neural_network import quantization_utils\n",
                "import torch\n",
                "\n",
                "print(\"Exporting to CoreML for iOS...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Prepare model for tracing\n",
                "print(\"\\n1. Preparing model for conversion...\")\n",
                "best_model.cpu().eval()\n",
                "\n",
                "# Create dummy inputs (batch_size=1, seq_length=16)\n",
                "dummy_input_ids = torch.zeros(1, 16, dtype=torch.long)\n",
                "dummy_attention_mask = torch.ones(1, 16, dtype=torch.long)\n",
                "\n",
                "# 2. Trace the model\n",
                "print(\"2. Tracing model...\")\n",
                "traced_model = torch.jit.trace(best_model, (dummy_input_ids, dummy_attention_mask))\n",
                "print(\"   âœ“ Model traced successfully\")\n",
                "\n",
                "# 3. Convert to CoreML (FP32)\n",
                "print(\"\\n3. Converting to CoreML (FP32)...\")\n",
                "mlmodel = ct.convert(\n",
                "    traced_model,\n",
                "    inputs=[\n",
                "        ct.TensorType(name=\"input_ids\", shape=(1, 16), dtype=int),\n",
                "        ct.TensorType(name=\"attention_mask\", shape=(1, 16), dtype=int)\n",
                "    ],\n",
                "    outputs=[ct.TensorType(name=\"logits\")]\n",
                ")\n",
                "print(\"   âœ“ Conversion successful\")\n",
                "\n",
                "# 4. Quantize to INT8\n",
                "print(\"\\n4. Quantizing to INT8...\")\n",
                "mlmodel_int8 = quantization_utils.quantize_weights(mlmodel, nbits=8)\n",
                "print(\"   âœ“ Quantization complete\")\n",
                "\n",
                "# 5. Save to Drive\n",
                "coreml_path = f\"{DRIVE_DIR}/models/TinyBERT_Keyboard_iOS.mlpackage\"\n",
                "mlmodel_int8.save(coreml_path)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… iOS CoreML export complete!\")\n",
                "print(f\"\\nSaved to: {coreml_path}\")\n",
                "print(f\"Model size: ~5MB (INT8 quantized)\")\n",
                "print(f\"Expected latency: <50ms on iPhone (with Neural Engine)\")\n",
                "print(f\"Expected RAM: <30MB runtime\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"  1. Download from Google Drive\")\n",
                "print(\"  2. Add to Xcode project\")\n",
                "print(\"  3. Use with Core ML framework\")\n",
                "print(\"\\nðŸ’¡ Tip: Enable Neural Engine for best performance!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export to TFLite (Android)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to TFLite for Android with INT8 quantization\n",
                "!pip install -q tensorflow onnx onnx-tf\n",
                "\n",
                "import tensorflow as tf\n",
                "from pathlib import Path\n",
                "import onnx\n",
                "from onnx_tf.backend import prepare\n",
                "\n",
                "print(\"Exporting to TFLite for Android...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Convert PyTorch to ONNX first\n",
                "print(\"\\n1. Converting PyTorch â†’ ONNX...\")\n",
                "best_model.cpu().eval()\n",
                "\n",
                "onnx_path = \"/content/model.onnx\"\n",
                "dummy_input_ids = torch.zeros(1, 16, dtype=torch.long)\n",
                "dummy_attention_mask = torch.ones(1, 16, dtype=torch.long)\n",
                "\n",
                "torch.onnx.export(\n",
                "    best_model,\n",
                "    (dummy_input_ids, dummy_attention_mask),\n",
                "    onnx_path,\n",
                "    input_names=['input_ids', 'attention_mask'],\n",
                "    output_names=['logits'],\n",
                "    dynamic_axes={\n",
                "        'input_ids': {0: 'batch_size'},\n",
                "        'attention_mask': {0: 'batch_size'},\n",
                "        'logits': {0: 'batch_size'}\n",
                "    },\n",
                "    opset_version=18\n",
                ")\n",
                "print(\"   âœ“ ONNX export successful\")\n",
                "\n",
                "# 2. Convert ONNX to TensorFlow SavedModel\n",
                "print(\"\\n2. Converting ONNX â†’ TensorFlow...\")\n",
                "onnx_model = onnx.load(onnx_path)\n",
                "tf_rep = prepare(onnx_model)\n",
                "tf_model_path = \"/content/tf_model\"\n",
                "tf_rep.export_graph(tf_model_path)\n",
                "print(\"   âœ“ TensorFlow conversion successful\")\n",
                "\n",
                "# 3. Convert to TFLite with INT8 quantization\n",
                "print(\"\\n3. Converting to TFLite with INT8 quantization...\")\n",
                "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
                "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # INT8 quantization\n",
                "tflite_model = converter.convert()\n",
                "print(\"   âœ“ TFLite conversion successful\")\n",
                "\n",
                "# 4. Save to Drive\n",
                "tflite_dir = Path(f\"{DRIVE_DIR}/models/tflite\")\n",
                "tflite_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "tflite_path = tflite_dir / \"keyboard_model_quantized.tflite\"\n",
                "with open(tflite_path, \"wb\") as f:\n",
                "    f.write(tflite_model)\n",
                "\n",
                "# 5. Save vocabulary\n",
                "best_tokenizer.save_vocabulary(str(tflite_dir))\n",
                "\n",
                "import os\n",
                "model_size_mb = os.path.getsize(tflite_path) / 1024 / 1024\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… Android TFLite export complete!\")\n",
                "print(f\"\\nSaved to: {tflite_path}\")\n",
                "print(f\"Model size: {model_size_mb:.2f}MB (INT8 quantized)\")\n",
                "print(f\"Expected latency: <50ms on Android (with NPU delegate)\")\n",
                "print(f\"Expected RAM: <30MB runtime\")\n",
                "print(\"\\nFiles created:\")\n",
                "print(f\"  â€¢ keyboard_model_quantized.tflite\")\n",
                "print(f\"  â€¢ vocab.txt\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"  1. Download from Google Drive\")\n",
                "print(\"  2. Add to Android project (assets/)\")\n",
                "print(\"  3. Use with TensorFlow Lite Interpreter\")\n",
                "print(\"\\nðŸ’¡ Tip: Enable GPU/NPU delegate for best performance!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}