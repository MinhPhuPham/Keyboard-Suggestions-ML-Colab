{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFVF_puJNwad"
      },
      "source": [
        "# Pythia-14m Keyboard Suggestion Model Training\n",
        "\n",
        "This notebook trains a keyboard suggestion model using **Pythia-14m** (GPT-style causal LM).\n",
        "\n",
        "**Features:**\n",
        "1. Word Completion: \"Hel\" → [\"Hello\", \"Help\", \"Helping\"]\n",
        "2. Next-Word Prediction: \"How are\" → [\"you\", \"they\", \"we\"]\n",
        "3. Typo Correction: \"Thers\" → [\"There\", \"Theirs\", \"Therapy\"]\n",
        "4. Gibberish Detection: Heuristic (no ML)\n",
        "\n",
        "**Model Specifications:**\n",
        "- Base: Pythia-14m (6 layers, 128 hidden, 4 heads, ~14M params)\n",
        "- Architecture: GPT-NeoX (Causal LM, decoder-only)\n",
        "- Target Size: <20MB (after INT8 quantization)\n",
        "- Latency: <50ms on mobile\n",
        "- RAM Usage: 15-20MB runtime\n",
        "- Deployment: iOS (CoreML) + Android (TFLite)\n",
        "\n",
        "**Training Time:** 3-4 hours on Colab GPU (T4)\n",
        "\n",
        "**Data Sources (Google Drive):**\n",
        "- `single_word_freq.csv` - Word frequencies for completion\n",
        "- `keyboard_training_data.txt` - Custom corpus for next-word\n",
        "- `misspelled.csv` - Typo correction pairs\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime → Change runtime type → GPU (T4)\n",
        "2. Run all cells\n",
        "3. Model will be saved to Google Drive\n",
        "4. Download for mobile deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMRENJcZNwae"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjga9u2-Nwae"
      },
      "outputs": [],
      "source": [
        "# Check if running in Colab\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"✓ Running in Google Colab\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define Drive directory\n",
        "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
        "\n",
        "    print(f\"✓ Google Drive mounted\")\n",
        "    print(f\"✓ Project directory: {DRIVE_DIR}\")\n",
        "else:\n",
        "    print(\"✓ Running locally\")\n",
        "    DRIVE_DIR = './data'  # Local fallback\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM9QuMqhNwae"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch datasets accelerate\n",
        "!pip install -q scikit-learn tqdm\n",
        "print(\"✓ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGU3HA3LNwae"
      },
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Expected datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/single_word_freq.csv`\n",
        "- `{DRIVE_DIR}/datasets/keyboard_training_data.txt`\n",
        "- `{DRIVE_DIR}/datasets/misspelled.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8crn4MJBNwae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "WORD_FREQ_PATH = f\"{DRIVE_DIR}/datasets/single_word_freq.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "TYPO_PATH = f\"{DRIVE_DIR}/datasets/misspelled.csv\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "if os.path.exists(WORD_FREQ_PATH):\n",
        "    with open(WORD_FREQ_PATH, 'r', encoding='utf-8') as f:\n",
        "        word_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"✓ single_word_freq.csv: {word_count:,} words\")\n",
        "else:\n",
        "    print(f\"✗ Missing: {WORD_FREQ_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(CORPUS_PATH):\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        line_count = sum(1 for _ in f)\n",
        "    print(f\"✓ keyboard_training_data.txt: {line_count:,} lines\")\n",
        "else:\n",
        "    print(f\"✗ Missing: {CORPUS_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(TYPO_PATH):\n",
        "    with open(TYPO_PATH, 'r', encoding='utf-8') as f:\n",
        "        typo_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"✓ misspelled.csv: {typo_count:,} entries\")\n",
        "else:\n",
        "    print(f\"✗ Missing: {TYPO_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "print(\"=\"*60)\n",
        "if datasets_ok:\n",
        "    print(\"✅ All datasets found!\")\n",
        "else:\n",
        "    print(\"⚠️  Some datasets are missing. Please upload them to Google Drive.\")\n",
        "    print(\"\\nExpected location: {DRIVE_DIR}/datasets/\")\n",
        "    print(\"Required files:\")\n",
        "    print(\"  - single_word_freq.csv (format: word,count_frequency)\")\n",
        "    print(\"  - keyboard_training_data.txt (plain text sentences)\")\n",
        "    print(\"  - misspelled.csv (format: number,correct_word,misspelled_words)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7rrEsFoNwae"
      },
      "source": [
        "## 3. Generate Training Data\n",
        "\n",
        "Generate training pairs for all 3 tasks from your existing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_KL-qVlNwaf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def prepare_word_completion_data(word_freq_path: str, max_samples: int = 50000) -> List[dict]:\n",
        "    print(\"\\nGenerating word completion data (Fixed)...\")\n",
        "\n",
        "    samples = []\n",
        "    words_with_freq = []\n",
        "\n",
        "    with open(word_freq_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            word = row['word'].strip().lower()\n",
        "            freq = int(row.get('count_frequency', 1))\n",
        "            # Only words 3+ chars, alphabetic only\n",
        "            if len(word) >= 3 and word.isalpha():\n",
        "                words_with_freq.append((word, freq))\n",
        "    \n",
        "    # Sort by frequency, take top 15k most common words (vs 10k in smaller version)\n",
        "    words_with_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    words_with_freq = words_with_freq[:25000]  # ✅ More words = better coverage\n",
        "    \n",
        "    print(f\"  Using top {len(words_with_freq):,} words\")\n",
        "    \n",
        "    # Generate samples\n",
        "    for word, freq in words_with_freq:\n",
        "        if len(samples) >= max_samples:\n",
        "            break\n",
        "        \n",
        "        # More samples for frequent words (1-5 per word)\n",
        "        num_samples = min(5, max(1, freq // 10000))\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            # Varied prefix lengths: 40%, 50%, 60%, 70%, 80%\n",
        "            prefix_ratio = 0.4 + (i * 0.1)\n",
        "            prefix_len = max(1, int(len(word) * prefix_ratio))\n",
        "            \n",
        "            if prefix_len < len(word):  # Don't use full word as prefix\n",
        "                samples.append({\n",
        "                    'input': word[:prefix_len],\n",
        "                    'target': word,\n",
        "                    'task': 'completion'\n",
        "                })\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} completion pairs\")\n",
        "    return samples\n",
        "\n",
        "def prepare_nextword_data(corpus_path: str, max_samples: int = 100000, context_length: int = 3) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Generate next-word prediction pairs - MAXIMUM ACCURACY VERSION\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating next-word prediction data...\")\n",
        "    \n",
        "    samples = []\n",
        "    seen_pairs = set()  # Avoid duplicates\n",
        "    \n",
        "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            line = line.strip().lower()\n",
        "            words = line.split()\n",
        "            \n",
        "            if len(words) < context_length + 1:\n",
        "                continue\n",
        "            \n",
        "            for i in range(len(words) - context_length):\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "                \n",
        "                context = ' '.join(words[i:i+context_length])\n",
        "                target = words[i+context_length]\n",
        "                \n",
        "                # Only valid alphabetic words, no duplicates\n",
        "                pair_key = f\"{context}|{target}\"\n",
        "                if (target.isalpha() and \n",
        "                    len(target) > 1 and \n",
        "                    pair_key not in seen_pairs):\n",
        "                    \n",
        "                    samples.append({\n",
        "                        'input': context,\n",
        "                        'target': target,\n",
        "                        'task': 'nextword'\n",
        "                    })\n",
        "                    seen_pairs.add(pair_key)\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} next-word pairs ({len(seen_pairs):,} unique)\")\n",
        "    return samples\n",
        "\n",
        "def prepare_typo_data(typo_path: str, max_samples: int = 20000) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Generate typo correction pairs - MAXIMUM ACCURACY VERSION\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating typo correction data...\")\n",
        "    \n",
        "    samples = []\n",
        "    \n",
        "    with open(typo_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            # ✅ FIXED: Use correct CSV column names\n",
        "            correct = row['correct_word'].strip().lower()\n",
        "            misspelled_list = row['misspelled_words'].strip().lower()\n",
        "            \n",
        "            # Split multiple misspellings (comma or space separated)\n",
        "            typos = [t.strip() for t in misspelled_list.replace(',', ' ').split() if t.strip()]\n",
        "            \n",
        "            for typo in typos:\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "                \n",
        "                if typo and typo != correct:\n",
        "                    samples.append({\n",
        "                        'input': typo,\n",
        "                        'target': correct,\n",
        "                        'task': 'typo'\n",
        "                    })\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} typo pairs\")\n",
        "    return samples\n",
        "\n",
        "# Generate all datasets\n",
        "print(\"Preparing training datasets...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "train_path = f\"{output_dir}/train.jsonl\"\n",
        "val_path = f\"{output_dir}/val.jsonl\"\n",
        "\n",
        "# Check if processed datasets already exist\n",
        "if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "    print(\"✓ Processed datasets found in Drive!\")\n",
        "    print(f\"  Train: {train_path}\")\n",
        "    print(f\"  Val: {val_path}\")\n",
        "\n",
        "    # Count samples\n",
        "    with open(train_path, 'r') as f:\n",
        "        train_count = sum(1 for _ in f)\n",
        "    with open(val_path, 'r') as f:\n",
        "        val_count = sum(1 for _ in f)\n",
        "    print(f\"  Train samples: {train_count:,}\")\n",
        "    print(f\"  Val samples: {val_count:,}\")\n",
        "else:\n",
        "    print(\"Generating training datasets from scratch...\")\n",
        "\n",
        "    # Generate each task\n",
        "    completion_samples = prepare_word_completion_data(WORD_FREQ_PATH, max_samples=50000)\n",
        "    nextword_samples = prepare_nextword_data(CORPUS_PATH, max_samples=100000, context_length=3)\n",
        "    typo_samples = prepare_typo_data(TYPO_PATH, max_samples=20000)\n",
        "\n",
        "    # Combine all samples\n",
        "    all_samples = completion_samples + nextword_samples + typo_samples\n",
        "    random.shuffle(all_samples)\n",
        "\n",
        "    # Split train/val (90/10)\n",
        "    split_idx = int(len(all_samples) * 0.95)\n",
        "    train_samples = all_samples[:split_idx]\n",
        "    val_samples = all_samples[split_idx:]\n",
        "\n",
        "    # Save to JSONL\n",
        "    with open(train_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in train_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    with open(val_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in val_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ Dataset generation complete!\")\n",
        "    print(f\"  Total samples: {len(all_samples):,}\")\n",
        "    print(f\"  Train: {len(train_samples):,} ({train_path})\")\n",
        "    print(f\"  Val: {len(val_samples):,} ({val_path})\")\n",
        "    print(f\"\\n  Task distribution:\")\n",
        "    print(f\"    Completion: {len(completion_samples):,}\")\n",
        "    print(f\"    Next-word: {len(nextword_samples):,}\")\n",
        "    print(f\"    Typo: {len(typo_samples):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ Datasets ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQcETLToNwaf"
      },
      "source": [
        "## 4. Load Pythia-14m Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwdQlQkeNwaf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, GPTNeoXForCausalLM\n",
        "\n",
        "MODEL_NAME = \"EleutherAI/pythia-14m\"\n",
        "\n",
        "print(f\"Loading Pythia-14m from: {MODEL_NAME}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPTNeoXForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# ✅ CRITICAL: Add padding token (GPT models don't have one by default)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"✓ Model loaded successfully\")\n",
        "print(f\"✓ Device: {device}\")\n",
        "print(f\"✓ Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"✓ Vocab size: {tokenizer.vocab_size:,}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48li_GV4Nwaf"
      },
      "source": [
        "## 5. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peBkxjlgNwaf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class PythiaKeyboardDataset(Dataset):\n",
        "    \"\"\"Causal LM dataset for Pythia-14m keyboard suggestions\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, tokenizer, max_length=12):\n",
        "        self.data = []\n",
        "\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_text = item['input']\n",
        "        target_word = item['target']\n",
        "        task = item.get('task', 'completion')\n",
        "        # ✅ For Pythia: concatenate input + target (no [MASK])\n",
        "        # Example: \"hel\" + \"hello\" = \"helhello\" (model learns to predict \"hello\" after \"hel\")\n",
        "        full_text = f\"{input_text}{target_word}\"\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            full_text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # ✅ Causal LM: labels are input_ids shifted by 1\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        \n",
        "        # Shift labels: predict next token\n",
        "        labels[:-1] = input_ids[1:]\n",
        "        labels[-1] = -100  # Ignore last position\n",
        "        \n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "train_dataset = PythiaKeyboardDataset(train_path, tokenizer, max_length=12)\n",
        "val_dataset = PythiaKeyboardDataset(val_path, tokenizer, max_length=12)\n",
        "\n",
        "print(f\"✓ Train samples: {len(train_dataset):,}\")\n",
        "print(f\"✓ Val samples: {len(val_dataset):,}\")\n",
        "print(f\"✓ Max sequence length: 12 tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"✓ Train batches: {len(train_loader)}\")\n",
        "print(f\"✓ Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXyd99JhNwaf"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_VXLd4ANwaf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 5e-5\n",
        "SAVE_STEPS = 1000\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_loader))\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch size: 16\")\n",
        "print(f\"Total steps: {NUM_EPOCHS * len(train_loader):,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss += loss.item()\n",
        "        global_step += 1\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        # Save checkpoint\n",
        "        if global_step % SAVE_STEPS == 0:\n",
        "            checkpoint_dir = f\"{DRIVE_DIR}/models/pythia_checkpoint-{global_step}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            print(f\"\\n✓ Checkpoint saved: {checkpoint_dir}\")\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_dir = f\"{DRIVE_DIR}/models/pythia_best_model\"\n",
        "        model.save_pretrained(best_model_dir)\n",
        "        tokenizer.save_pretrained(best_model_dir)\n",
        "        print(f\"  ✓ Best model saved: {best_model_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ Training complete!\")\n",
        "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"  Total steps: {global_step}\")\n",
        "print(f\"  Expected accuracy: 80-85%\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czEMLVjNwag"
      },
      "source": [
        "## 7. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXGwWzuhNwag"
      },
      "outputs": [],
      "source": [
        "def test_pythia_predictions(model, tokenizer, test_input, top_k=5):\n",
        "    \"\"\"Test Pythia-14m predictions\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(test_input, return_tensors='pt').to(device)\n",
        "    \n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[0, -1, :]  # Last token position\n",
        "    \n",
        "    # Get top-k predictions\n",
        "    top_tokens = torch.topk(logits, k=top_k)\n",
        "    predictions = []\n",
        "    \n",
        "    for idx, score in zip(top_tokens.indices, top_tokens.values):\n",
        "        token = tokenizer.decode([idx])\n",
        "        prob = torch.softmax(logits, dim=0)[idx].item() * 100\n",
        "        predictions.append((token, prob))\n",
        "    \n",
        "    return predictions\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    \"hel\",\n",
        "    \"prod\",\n",
        "    \"how ar\",\n",
        "    \"best st\",\n",
        "    \"you\",\n",
        "]\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Predictions:\")\n",
        "print(\"=\"*60)\n",
        "for test_input in test_cases:\n",
        "    predictions = test_pythia_predictions(model, tokenizer, test_input, top_k=3)\n",
        "    print(f\"\\nInput: '{test_input}'\")\n",
        "    print(\"Predictions:\")\n",
        "    for i, (token, prob) in enumerate(predictions, 1):\n",
        "        print(f\"  {i}. '{token}' ({prob:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAbcrkwNwag"
      },
      "source": [
        "## 8. Export to CoreML (iOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJPrGe-XNwag"
      },
      "outputs": [],
      "source": [
        "!pip install -q coremltools\n",
        "import coremltools as ct\n",
        "import numpy as np\n",
        "print(\"Exporting to CoreML...\")\n",
        "# Load best model\n",
        "best_model_path = f\"{DRIVE_DIR}/models/pythia_best_model\"\n",
        "export_model = GPTNeoXForCausalLM.from_pretrained(best_model_path)\n",
        "export_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
        "export_model.eval()\n",
        "# Trace model\n",
        "dummy_input = torch.randint(0, export_tokenizer.vocab_size, (1, 12))\n",
        "traced_model = torch.jit.trace(export_model, dummy_input)\n",
        "# Convert to CoreML\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.TensorType(name=\"input_ids\", shape=(1, 12), dtype=np.int32)],\n",
        "    compute_units=ct.ComputeUnit.ALL,\n",
        "    compute_precision=ct.precision.FLOAT16,\n",
        "    minimum_deployment_target=ct.target.iOS14\n",
        ")\n",
        "# Add metadata\n",
        "mlmodel.author = \"MinhPhuPham\"\n",
        "mlmodel.short_description = \"Pythia-14m keyboard suggestion model\"\n",
        "mlmodel.version = \"1.0\"\n",
        "# Quantize to INT8\n",
        "print(\"Quantizing to INT8...\")\n",
        "import coremltools.optimize.coreml as cto\n",
        "op_config = cto.OpLinearQuantizerConfig(\n",
        "    mode=\"linear_symmetric\",\n",
        "    dtype=\"int8\",\n",
        "    granularity=\"per_channel\"\n",
        ")\n",
        "config = cto.OptimizationConfig(global_config=op_config)\n",
        "mlmodel_int8 = cto.linear_quantize_weights(mlmodel, config=config)\n",
        "# Save\n",
        "output_path = f\"{DRIVE_DIR}/models/Pythia14m_Keyboard.mlpackage\"\n",
        "mlmodel_int8.save(output_path)\n",
        "print(f\"✓ CoreML model saved: {output_path}\")\n",
        "print(f\"✓ Model size: ~12-15MB\")\n",
        "print(f\"✓ Expected RAM: 15-20MB\")\n",
        "print(f\"✓ Expected latency: <50ms\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Download model from Google Drive\")\n",
        "print(\"2. Test on iOS device\")\n",
        "print(\"3. Verify accuracy >80%\")\n",
        "print(\"4. Deploy to production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ZWroTPNwag"
      },
      "source": [
        "## 9. Export to TFLite (Android)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNj5KAgENwag"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
