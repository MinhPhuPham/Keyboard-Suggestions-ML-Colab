{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFVF_puJNwad"
      },
      "source": [
        "# TinyBERT Keyboard Suggestion Model Training\n",
        "\n",
        "This notebook trains a keyboard suggestion model using **TinyBERT** with multi-task learning.\n",
        "\n",
        "**Features:**\n",
        "1. Word Completion: \"Hel\" â†’ [\"Hello\", \"Help\", \"Helping\"]\n",
        "2. Next-Word Prediction: \"How are\" â†’ [\"you\", \"they\", \"we\"]\n",
        "3. Typo Correction: \"Thers\" â†’ [\"There\", \"Theirs\", \"Therapy\"]\n",
        "4. Gibberish Detection: Heuristic (no ML)\n",
        "\n",
        "**Model Specifications:**\n",
        "- Base: TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
        "- Target Size: <50MB (after INT8 quantization)\n",
        "- Latency: <50ms on mobile\n",
        "- RAM Usage: <10MB runtime\n",
        "- Deployment: iOS (CoreML) + Android (TFLite)\n",
        "\n",
        "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
        "\n",
        "**Data Sources (Google Drive):**\n",
        "- `single_word_freq.csv` - Word frequencies for completion\n",
        "- `keyboard_training_data.txt` - Custom corpus for next-word\n",
        "- `misspelled.csv` - Typo correction pairs\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
        "2. Run all cells\n",
        "3. Model will be saved to Google Drive\n",
        "4. Download for mobile deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMRENJcZNwae"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjga9u2-Nwae"
      },
      "outputs": [],
      "source": [
        "# Check if running in Colab\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"âœ“ Running in Google Colab\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define Drive directory\n",
        "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
        "\n",
        "    print(f\"âœ“ Google Drive mounted\")\n",
        "    print(f\"âœ“ Project directory: {DRIVE_DIR}\")\n",
        "else:\n",
        "    print(\"âœ“ Running locally\")\n",
        "    DRIVE_DIR = './data'  # Local fallback\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM9QuMqhNwae"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch datasets accelerate\n",
        "!pip install -q scikit-learn tqdm\n",
        "print(\"âœ“ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGU3HA3LNwae"
      },
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Expected datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/single_word_freq.csv`\n",
        "- `{DRIVE_DIR}/datasets/keyboard_training_data.txt`\n",
        "- `{DRIVE_DIR}/datasets/misspelled.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8crn4MJBNwae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "WORD_FREQ_PATH = f\"{DRIVE_DIR}/datasets/single_word_freq.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "TYPO_PATH = f\"{DRIVE_DIR}/datasets/misspelled.csv\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "if os.path.exists(WORD_FREQ_PATH):\n",
        "    with open(WORD_FREQ_PATH, 'r', encoding='utf-8') as f:\n",
        "        word_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"âœ“ single_word_freq.csv: {word_count:,} words\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {WORD_FREQ_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(CORPUS_PATH):\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        line_count = sum(1 for _ in f)\n",
        "    print(f\"âœ“ keyboard_training_data.txt: {line_count:,} lines\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {CORPUS_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(TYPO_PATH):\n",
        "    with open(TYPO_PATH, 'r', encoding='utf-8') as f:\n",
        "        typo_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"âœ“ misspelled.csv: {typo_count:,} entries\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {TYPO_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "print(\"=\"*60)\n",
        "if datasets_ok:\n",
        "    print(\"âœ… All datasets found!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Some datasets are missing. Please upload them to Google Drive.\")\n",
        "    print(\"\\nExpected location: {DRIVE_DIR}/datasets/\")\n",
        "    print(\"Required files:\")\n",
        "    print(\"  - single_word_freq.csv (format: word,count_frequency)\")\n",
        "    print(\"  - keyboard_training_data.txt (plain text sentences)\")\n",
        "    print(\"  - misspelled.csv (format: number,correct_word,misspelled_words)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7rrEsFoNwae"
      },
      "source": [
        "## 3. Generate Training Data\n",
        "\n",
        "Generate training pairs for all 3 tasks from your existing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_KL-qVlNwaf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def prepare_word_completion_data(word_freq_path: str, max_samples: int = 50000) -> List[dict]:\n",
        "    print(\"\\nGenerating word completion data (Fixed)...\")\n",
        "\n",
        "    samples = []\n",
        "    words_with_freq = []\n",
        "\n",
        "    with open(word_freq_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            word = row['word'].strip().lower()\n",
        "            freq = int(row.get('count_frequency', 1))\n",
        "            # Only words 3+ chars, alphabetic only\n",
        "            if len(word) >= 3 and word.isalpha():\n",
        "                words_with_freq.append((word, freq))\n",
        "    \n",
        "    # Sort by frequency, take top 15k most common words (vs 10k in smaller version)\n",
        "    words_with_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "    words_with_freq = words_with_freq[:15000]  # âœ… More words = better coverage\n",
        "    \n",
        "    print(f\"  Using top {len(words_with_freq):,} words\")\n",
        "    \n",
        "    # Generate samples\n",
        "    for word, freq in words_with_freq:\n",
        "        if len(samples) >= max_samples:\n",
        "            break\n",
        "        \n",
        "        # More samples for frequent words (1-5 per word)\n",
        "        num_samples = min(5, max(1, freq // 10000))\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            # Varied prefix lengths: 40%, 50%, 60%, 70%, 80%\n",
        "            prefix_ratio = 0.4 + (i * 0.1)\n",
        "            prefix_len = max(1, int(len(word) * prefix_ratio))\n",
        "            \n",
        "            if prefix_len < len(word):  # Don't use full word as prefix\n",
        "                samples.append({\n",
        "                    'input': word[:prefix_len],\n",
        "                    'target': word,\n",
        "                    'task': 'completion'\n",
        "                })\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} completion pairs\")\n",
        "    return samples\n",
        "\n",
        "def prepare_nextword_data(corpus_path: str, max_samples: int = 100000, context_length: int = 3) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Generate next-word prediction pairs - MAXIMUM ACCURACY VERSION\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating next-word prediction data...\")\n",
        "    \n",
        "    samples = []\n",
        "    seen_pairs = set()  # Avoid duplicates\n",
        "    \n",
        "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            line = line.strip().lower()\n",
        "            words = line.split()\n",
        "            \n",
        "            if len(words) < context_length + 1:\n",
        "                continue\n",
        "            \n",
        "            for i in range(len(words) - context_length):\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "                \n",
        "                context = ' '.join(words[i:i+context_length])\n",
        "                target = words[i+context_length]\n",
        "                \n",
        "                # Only valid alphabetic words, no duplicates\n",
        "                pair_key = f\"{context}|{target}\"\n",
        "                if (target.isalpha() and \n",
        "                    len(target) > 1 and \n",
        "                    pair_key not in seen_pairs):\n",
        "                    \n",
        "                    samples.append({\n",
        "                        'input': context,\n",
        "                        'target': target,\n",
        "                        'task': 'nextword'\n",
        "                    })\n",
        "                    seen_pairs.add(pair_key)\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} next-word pairs ({len(seen_pairs):,} unique)\")\n",
        "    return samples\n",
        "\n",
        "def prepare_typo_data(typo_path: str, max_samples: int = 20000) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Generate typo correction pairs - MAXIMUM ACCURACY VERSION\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating typo correction data...\")\n",
        "    \n",
        "    samples = []\n",
        "    \n",
        "    with open(typo_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "            \n",
        "            # âœ… FIXED: Use correct CSV column names\n",
        "            correct = row['correct_word'].strip().lower()\n",
        "            misspelled_list = row['misspelled_words'].strip().lower()\n",
        "            \n",
        "            # Split multiple misspellings (comma or space separated)\n",
        "            typos = [t.strip() for t in misspelled_list.replace(',', ' ').split() if t.strip()]\n",
        "            \n",
        "            for typo in typos:\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "                \n",
        "                if typo and typo != correct:\n",
        "                    samples.append({\n",
        "                        'input': typo,\n",
        "                        'target': correct,\n",
        "                        'task': 'typo'\n",
        "                    })\n",
        "    \n",
        "    print(f\"  Generated {len(samples):,} typo pairs\")\n",
        "    return samples\n",
        "\n",
        "# Generate all datasets\n",
        "print(\"Preparing training datasets...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "train_path = f\"{output_dir}/train.jsonl\"\n",
        "val_path = f\"{output_dir}/val.jsonl\"\n",
        "\n",
        "# Check if processed datasets already exist\n",
        "if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "    print(\"âœ“ Processed datasets found in Drive!\")\n",
        "    print(f\"  Train: {train_path}\")\n",
        "    print(f\"  Val: {val_path}\")\n",
        "\n",
        "    # Count samples\n",
        "    with open(train_path, 'r') as f:\n",
        "        train_count = sum(1 for _ in f)\n",
        "    with open(val_path, 'r') as f:\n",
        "        val_count = sum(1 for _ in f)\n",
        "    print(f\"  Train samples: {train_count:,}\")\n",
        "    print(f\"  Val samples: {val_count:,}\")\n",
        "else:\n",
        "    print(\"Generating training datasets from scratch...\")\n",
        "\n",
        "    # Generate each task\n",
        "    completion_samples = prepare_word_completion_data(WORD_FREQ_PATH, max_samples=50000)\n",
        "    nextword_samples = prepare_nextword_data(CORPUS_PATH, max_samples=100000, context_length=3)\n",
        "    typo_samples = prepare_typo_data(TYPO_PATH, max_samples=20000)\n",
        "\n",
        "    # Combine all samples\n",
        "    all_samples = completion_samples + nextword_samples + typo_samples\n",
        "    random.shuffle(all_samples)\n",
        "\n",
        "    # Split train/val (90/10)\n",
        "    split_idx = int(len(all_samples) * 0.9)\n",
        "    train_samples = all_samples[:split_idx]\n",
        "    val_samples = all_samples[split_idx:]\n",
        "\n",
        "    # Save to JSONL\n",
        "    with open(train_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in train_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    with open(val_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in val_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ“ Dataset generation complete!\")\n",
        "    print(f\"  Total samples: {len(all_samples):,}\")\n",
        "    print(f\"  Train: {len(train_samples):,} ({train_path})\")\n",
        "    print(f\"  Val: {len(val_samples):,} ({val_path})\")\n",
        "    print(f\"\\n  Task distribution:\")\n",
        "    print(f\"    Completion: {len(completion_samples):,}\")\n",
        "    print(f\"    Next-word: {len(nextword_samples):,}\")\n",
        "    print(f\"    Typo: {len(typo_samples):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ“ Datasets ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQcETLToNwaf"
      },
      "source": [
        "## 4. Load TinyBERT Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwdQlQkeNwaf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "print(\"Loading TinyBERT for Masked Language Modeling...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
        "MODEL_NAME = \"google/bert_uncased_L-4_H-256_A-4\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"âœ“ Model loaded on {device}\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Layers: 4, Hidden: 312, Heads: 12\")\n",
        "print(f\"  Model size: ~55MB (FP32) â†’ ~5MB (INT8 quantized)\")\n",
        "print(f\"  Target latency: <50ms on mobile\")\n",
        "print(f\"  Target RAM: <30MB runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48li_GV4Nwaf"
      },
      "source": [
        "## 5. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peBkxjlgNwaf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import torch\n",
        "\n",
        "class KeyboardDataset(Dataset):\n",
        "    \"\"\"BERT MLM dataset for keyboard suggestions.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=12):\n",
        "        self.data = []\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text_input = item['input']\n",
        "        target_word = item['target']\n",
        "        task = item.get('task', 'completion')\n",
        "\n",
        "        # Add [MASK] for prediction\n",
        "        text_input = f\"{text_input} {self.tokenizer.mask_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            text_input,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Labels: -100 everywhere except [MASK]\n",
        "        labels = torch.full(inputs['input_ids'].shape, -100, dtype=torch.long)\n",
        "\n",
        "        # Get target token ID (first token of target word)\n",
        "        target_encoding = self.tokenizer.encode(\n",
        "            target_word,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "        target_id = target_encoding[0] if target_encoding else self.tokenizer.unk_token_id\n",
        "\n",
        "        # Set label at [MASK] position\n",
        "        mask_positions = (inputs['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "        if len(mask_positions[1]) > 0:\n",
        "            labels[0, mask_positions[1][0]] = target_id\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'task': task\n",
        "        }\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "train_dataset = KeyboardDataset(train_path, tokenizer)\n",
        "val_dataset = KeyboardDataset(val_path, tokenizer)\n",
        "\n",
        "print(f\"âœ“ Train samples: {len(train_dataset):,}\")\n",
        "print(f\"âœ“ Val samples: {len(val_dataset):,}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"âœ“ Batch size: 32 (train), 64 (val)\")\n",
        "print(f\"âœ“ Max sequence length: 16 tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXyd99JhNwaf"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_VXLd4ANwaf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NUM_EPOCHS = 5  # âœ… Increased from 3 for better learning\n",
        "LEARNING_RATE = 5e-5  # âœ… Slightly higher for faster convergence\n",
        "SAVE_STEPS = 1000\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "# âœ… ADD: Learning rate scheduler for better convergence\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_loader))\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Batch size: 16\")\n",
        "print(f\"Total steps: {NUM_EPOCHS * len(train_loader):,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # âœ… Update learning rate\n",
        "        train_loss += loss.item()\n",
        "        global_step += 1\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        # Save checkpoint\n",
        "        if global_step % SAVE_STEPS == 0:\n",
        "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            print(f\"\\nâœ“ Checkpoint saved: {checkpoint_dir}\")\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
        "        model.save_pretrained(best_model_dir)\n",
        "        tokenizer.save_pretrained(best_model_dir)\n",
        "        print(f\"  âœ“ Best model saved: {best_model_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ“ Training complete!\")\n",
        "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"  Total steps: {global_step}\")\n",
        "print(f\"  Expected accuracy: 80-85%\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czEMLVjNwag"
      },
      "source": [
        "## 7. Export and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXGwWzuhNwag"
      },
      "outputs": [],
      "source": [
        "# Export and save the BEST model (for iOS/Android deployment)\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Exporting BEST model for deployment...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the best model (saved during training)\n",
        "best_model_path = f\"{DRIVE_DIR}/models/best_model\"\n",
        "print(f\"\\nLoading best model from: {best_model_path}\")\n",
        "\n",
        "# Load best model\n",
        "best_model = AutoModelForMaskedLM.from_pretrained(best_model_path)\n",
        "best_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
        "print(\"âœ“ Best model loaded\")\n",
        "\n",
        "# 1. Save to Google Drive as 'final' (for deployment)\n",
        "drive_model_dir = f\"{DRIVE_DIR}/models/tinybert_keyboard_final\"\n",
        "best_model.save_pretrained(drive_model_dir)\n",
        "best_tokenizer.save_pretrained(drive_model_dir)\n",
        "print(f\"\\nâœ… Saved to Google Drive: {drive_model_dir}\")\n",
        "\n",
        "# 2. Create downloadable zip\n",
        "local_model_dir = \"/content/tinybert_keyboard_model\"\n",
        "best_model.save_pretrained(local_model_dir)\n",
        "best_tokenizer.save_pretrained(local_model_dir)\n",
        "\n",
        "zip_path = \"/content/tinybert_keyboard_model.zip\"\n",
        "shutil.make_archive(\"/content/tinybert_keyboard_model\", 'zip', local_model_dir)\n",
        "print(f\"\\nâœ… Created zip: {zip_path}\")\n",
        "\n",
        "# 3. Download to local device\n",
        "if IN_COLAB:\n",
        "    print(\"\\nðŸ“¥ Downloading BEST model to your computer...\")\n",
        "    files.download(zip_path)\n",
        "    print(\"âœ… Download started! Check your Downloads folder.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Export complete!\")\n",
        "print(\"\\nâš ï¸  IMPORTANT: This is the BEST model (lowest validation loss)\")\n",
        "print(\"   Use this for iOS/Android deployment!\")\n",
        "print(\"\\nModel saved to:\")\n",
        "print(f\"  1. Google Drive: {drive_model_dir}\")\n",
        "print(f\"  2. Local download: tinybert_keyboard_model.zip\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run Section 8 (Export to CoreML for iOS)\")\n",
        "print(\"  2. Run Section 9 (Export to TFLite for Android)\")\n",
        "print(\"\\nðŸ’¡ Both iOS and Android exports will use this BEST model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAbcrkwNwag"
      },
      "source": [
        "## 8. Export to CoreML (iOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJPrGe-XNwag"
      },
      "outputs": [],
      "source": [
        "# Export to CoreML for iOS with INT8 quantization\n",
        "!pip install -q coremltools\n",
        "import coremltools as ct\n",
        "import coremltools.optimize.coreml as cto\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Wrapper Class (Keep ensuring this is defined) ---\n",
        "class WrapperModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "# ... (Assume Steps 1-3 ran successfully and you have 'mlmodel') ...\n",
        "\n",
        "# 4. Quantize to INT8 (Corrected)\n",
        "print(\"\\n4. Quantizing to INT8...\")\n",
        "\n",
        "# A. Create the configuration\n",
        "# We define: INT8, Linear Symmetric mode, Per-Channel granularity (best accuracy)\n",
        "op_config = cto.OpLinearQuantizerConfig(\n",
        "    mode=\"linear_symmetric\",\n",
        "    dtype=\"int8\",\n",
        "    granularity=\"per_channel\"\n",
        ")\n",
        "config = cto.OptimizationConfig(global_config=op_config)\n",
        "\n",
        "# B. Apply the quantization using the config\n",
        "mlmodel_int8 = cto.linear_quantize_weights(mlmodel, config=config)\n",
        "\n",
        "print(\"   âœ“ Quantization complete\")\n",
        "\n",
        "# 5. Save Model AND Vocabulary\n",
        "coreml_dir = f\"{DRIVE_DIR}/models/ios_export\"\n",
        "import os\n",
        "os.makedirs(coreml_dir, exist_ok=True)\n",
        "\n",
        "# Save the CoreML Model\n",
        "coreml_path = f\"{coreml_dir}/TinyBERT_Keyboard_iOS.mlpackage\"\n",
        "mlmodel_int8.save(coreml_path)\n",
        "\n",
        "# Save the Vocabulary File (CRITICAL FOR iOS)\n",
        "best_tokenizer.save_vocabulary(coreml_dir)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… iOS CoreML export complete!\")\n",
        "print(f\"Directory: {coreml_dir}\")\n",
        "print(\"Files created:\")\n",
        "print(f\"  1. TinyBERT_Keyboard_iOS.mlpackage (The Model)\")\n",
        "print(f\"  2. vocab.txt (The Tokenizer Dictionary)\")\n",
        "\n",
        "# --- STEP 6: Add Metadata ---\n",
        "print(\"\\n6. Adding metadata...\")\n",
        "mlmodel.author = \"MinhPhuPham\"\n",
        "mlmodel.short_description = \"TinyBERT keyboard suggestion model for iOS\"\n",
        "mlmodel.version = \"1.0.0\"\n",
        "mlmodel.license = \"MIT\"\n",
        "mlmodel.user_defined_metadata = {\n",
        "    \"author\": \"MinhPhuPham\",\n",
        "    \"github\": \"https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab\",\n",
        "    \"description\": (\n",
        "        \"Optimized TinyBERT model for mobile keyboard suggestions. \"\n",
        "        \"Supports word completion, next-word prediction, and typo correction. \"\n",
        "        \"Trained on 100k samples with INT8 quantization for mobile deployment.\"\n",
        "    ),\n",
        "    \"model_architecture\": \"TinyBERT-4L-256H\",\n",
        "    \"training_date\": \"2026-01-20\",\n",
        "    \"target_platform\": \"iOS 14.0+\",\n",
        "    \"compute_unit\": \"Neural Engine + GPU\",\n",
        "    \"quantization\": \"INT8 + Float16\",\n",
        "    \"sequence_length\": str(SEQ_LENGTH),\n",
        "    \"vocabulary_size\": \"30522\",\n",
        "    \"model_size_mb\": \"9-10\",\n",
        "    \"runtime_ram_mb\": \"<10\",\n",
        "    \"target_latency_ms\": \"<50\",\n",
        "    \"features\": \"completion,next-word,typo-correction\"\n",
        "}\n",
        "print(\"   âœ“ Metadata added\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ZWroTPNwag"
      },
      "source": [
        "## 9. Export to TFLite (Android)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNj5KAgENwag"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Android TFLite Export: Direct PyTorch â†’ TensorFlow â†’ TFLite\n",
        "# Bypassing ONNX to avoid compatibility issues\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q optimum tensorflow transformers\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "print(\"\\nExporting to TFLite for Android (Direct PyTorch â†’ TF)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- STEP 1: Save PyTorch Model ---\n",
        "print(\"\\n1. Saving PyTorch model...\")\n",
        "staging_dir = Path(\"temp_staging_model\")\n",
        "if staging_dir.exists():\n",
        "    shutil.rmtree(staging_dir)\n",
        "staging_dir.mkdir()\n",
        "\n",
        "best_model.save_pretrained(staging_dir)\n",
        "best_tokenizer.save_pretrained(staging_dir)\n",
        "print(\"   âœ“ Model saved\")\n",
        "\n",
        "# --- STEP 2: Convert PyTorch â†’ TensorFlow ---\n",
        "print(\"\\n2. Converting PyTorch â†’ TensorFlow...\")\n",
        "\n",
        "# Load as TensorFlow model directly\n",
        "tf_model = TFAutoModelForMaskedLM.from_pretrained(\n",
        "    staging_dir,\n",
        "    from_pt=True  # Convert from PyTorch\n",
        ")\n",
        "print(\"   âœ“ Conversion successful\")\n",
        "\n",
        "# --- STEP 3: Save as TensorFlow SavedModel ---\n",
        "print(\"\\n3. Saving TensorFlow SavedModel...\")\n",
        "\n",
        "tf_saved_model_dir = \"tf_saved_model\"\n",
        "if os.path.exists(tf_saved_model_dir):\n",
        "    shutil.rmtree(tf_saved_model_dir)\n",
        "\n",
        "# Create input signature\n",
        "import numpy as np\n",
        "\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=(1, 16), dtype=tf.int32, name='input_ids'),\n",
        "    tf.TensorSpec(shape=(1, 16), dtype=tf.int32, name='attention_mask')\n",
        "])\n",
        "def serving_fn(input_ids, attention_mask):\n",
        "    outputs = tf_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    return {'logits': outputs.logits}\n",
        "\n",
        "# Save with concrete function\n",
        "tf.saved_model.save(\n",
        "    tf_model,\n",
        "    tf_saved_model_dir,\n",
        "    signatures={'serving_default': serving_fn}\n",
        ")\n",
        "print(\"   âœ“ SavedModel created\")\n",
        "\n",
        "# --- STEP 4: Convert to TFLite ---\n",
        "print(\"\\n4. Converting to TFLite (INT8 Quantization)...\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n",
        "\n",
        "# Configure quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "\n",
        "try:\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   âœ“ TFLite conversion successful (INT8)\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš  INT8 failed, trying FP16...\")\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   âœ“ TFLite conversion successful (FP16)\")\n",
        "\n",
        "# --- STEP 5: Save Final Model ---\n",
        "print(\"\\n5. Packaging for Android...\")\n",
        "\n",
        "tflite_dir = Path(f\"{DRIVE_DIR}/models/android\")\n",
        "tflite_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tflite_path = tflite_dir / \"keyboard_model_quantized.tflite\"\n",
        "with open(tflite_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "best_tokenizer.save_vocabulary(str(tflite_dir))\n",
        "\n",
        "model_size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
        "\n",
        "# --- STEP 6: Verification ---\n",
        "print(\"\\n6. Verifying model...\")\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(f\"   âœ“ Model verified\")\n",
        "    print(f\"   Inputs: {len(input_details)}\")\n",
        "    for inp in input_details:\n",
        "        print(f\"     - {inp['name']}: {inp['shape']}\")\n",
        "    print(f\"   Outputs: {len(output_details)}\")\n",
        "    for out in output_details:\n",
        "        print(f\"     - {out['name']}: {out['shape']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âš  Verification warning (OK on Android)\")\n",
        "\n",
        "# --- STEP 7: Cleanup ---\n",
        "print(\"\\n7. Cleaning up...\")\n",
        "for path in [staging_dir, tf_saved_model_dir]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Android TFLite export complete!\")\n",
        "print(f\"\\nSaved to: {tflite_dir}\")\n",
        "print(f\"Model: keyboard_model_quantized.tflite ({model_size_mb:.2f} MB)\")\n",
        "print(f\"\\nðŸ“¦ Files:\")\n",
        "print(f\"  1. keyboard_model_quantized.tflite\")\n",
        "print(f\"  2. vocab.txt\")\n",
        "print(f\"\\nðŸ“± build.gradle:\")\n",
        "print(f\"  implementation 'org.tensorflow:tensorflow-lite:2.14.0'\")\n",
        "print(f\"  implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.14.0'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
