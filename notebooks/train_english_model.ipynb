{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFVF_puJNwad"
      },
      "source": [
        "# TinyBERT Keyboard Suggestion Model Training\n",
        "\n",
        "This notebook trains a keyboard suggestion model using **TinyBERT** with multi-task learning.\n",
        "\n",
        "**Features:**\n",
        "1. Word Completion: \"Hel\" â†’ [\"Hello\", \"Help\", \"Helping\"]\n",
        "2. Next-Word Prediction: \"How are\" â†’ [\"you\", \"they\", \"we\"]\n",
        "3. Typo Correction: \"Thers\" â†’ [\"There\", \"Theirs\", \"Therapy\"]\n",
        "4. Gibberish Detection: Heuristic (no ML)\n",
        "\n",
        "**Model Specifications:**\n",
        "- Base: TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
        "- Target Size: <5MB (after INT8 quantization)\n",
        "- Latency: <50ms on mobile\n",
        "- RAM Usage: <30MB runtime\n",
        "- Deployment: iOS (CoreML) + Android (TFLite)\n",
        "\n",
        "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
        "\n",
        "**Data Sources (Google Drive):**\n",
        "- `single_word_freq.csv` - Word frequencies for completion\n",
        "- `keyboard_training_data.txt` - Custom corpus for next-word\n",
        "- `misspelled.csv` - Typo correction pairs\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
        "2. Run all cells\n",
        "3. Model will be saved to Google Drive\n",
        "4. Download for mobile deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMRENJcZNwae"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjga9u2-Nwae"
      },
      "outputs": [],
      "source": [
        "# Check if running in Colab\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"âœ“ Running in Google Colab\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define Drive directory\n",
        "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
        "\n",
        "    print(f\"âœ“ Google Drive mounted\")\n",
        "    print(f\"âœ“ Project directory: {DRIVE_DIR}\")\n",
        "else:\n",
        "    print(\"âœ“ Running locally\")\n",
        "    DRIVE_DIR = './data'  # Local fallback\n",
        "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/datasets/processed\", exist_ok=True)\n",
        "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM9QuMqhNwae"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch datasets accelerate\n",
        "!pip install -q scikit-learn tqdm\n",
        "print(\"âœ“ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGU3HA3LNwae"
      },
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Expected datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/single_word_freq.csv`\n",
        "- `{DRIVE_DIR}/datasets/keyboard_training_data.txt`\n",
        "- `{DRIVE_DIR}/datasets/misspelled.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8crn4MJBNwae"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "WORD_FREQ_PATH = f\"{DRIVE_DIR}/datasets/single_word_freq.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "TYPO_PATH = f\"{DRIVE_DIR}/datasets/misspelled.csv\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "if os.path.exists(WORD_FREQ_PATH):\n",
        "    with open(WORD_FREQ_PATH, 'r', encoding='utf-8') as f:\n",
        "        word_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"âœ“ single_word_freq.csv: {word_count:,} words\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {WORD_FREQ_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(CORPUS_PATH):\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        line_count = sum(1 for _ in f)\n",
        "    print(f\"âœ“ keyboard_training_data.txt: {line_count:,} lines\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {CORPUS_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "if os.path.exists(TYPO_PATH):\n",
        "    with open(TYPO_PATH, 'r', encoding='utf-8') as f:\n",
        "        typo_count = sum(1 for _ in f) - 1  # Subtract header\n",
        "    print(f\"âœ“ misspelled.csv: {typo_count:,} entries\")\n",
        "else:\n",
        "    print(f\"âœ— Missing: {TYPO_PATH}\")\n",
        "    datasets_ok = False\n",
        "\n",
        "print(\"=\"*60)\n",
        "if datasets_ok:\n",
        "    print(\"âœ… All datasets found!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Some datasets are missing. Please upload them to Google Drive.\")\n",
        "    print(\"\\nExpected location: {DRIVE_DIR}/datasets/\")\n",
        "    print(\"Required files:\")\n",
        "    print(\"  - single_word_freq.csv (format: word,count_frequency)\")\n",
        "    print(\"  - keyboard_training_data.txt (plain text sentences)\")\n",
        "    print(\"  - misspelled.csv (format: number,correct_word,misspelled_words)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7rrEsFoNwae"
      },
      "source": [
        "## 3. Generate Training Data\n",
        "\n",
        "Generate training pairs for all 3 tasks from your existing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_KL-qVlNwaf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import csv\n",
        "from typing import List, Tuple\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def prepare_word_completion_data(word_freq_path: str, max_samples: int = 50000) -> List[dict]:\n",
        "    \"\"\"Generate word completion training pairs from single_word_freq.csv\"\"\"\n",
        "    print(\"\\nGenerating word completion data...\")\n",
        "\n",
        "    samples = []\n",
        "    words_with_freq = []\n",
        "\n",
        "    # Read words with frequencies\n",
        "    with open(word_freq_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            word = row['word'].strip().lower()\n",
        "            freq = int(row.get('count_frequency', 1))\n",
        "            if len(word) >= 3:  # Only words with 3+ chars\n",
        "                words_with_freq.append((word, freq))\n",
        "\n",
        "    # Sort by frequency (higher first)\n",
        "    words_with_freq.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Generate samples (weighted by frequency)\n",
        "    for word, freq in words_with_freq:\n",
        "        if len(samples) >= max_samples:\n",
        "            break\n",
        "\n",
        "        # Generate multiple prefixes for common words\n",
        "        num_samples = min(3, max(1, freq // 1000))  # More samples for frequent words\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "\n",
        "            # Random prefix length (50-80% of word)\n",
        "            prefix_len = random.randint(max(1, len(word) // 2), max(2, int(len(word) * 0.8)))\n",
        "            prefix = word[:prefix_len]\n",
        "\n",
        "            samples.append({\n",
        "                'input': prefix,\n",
        "                'target': word,\n",
        "                'task': 'completion'\n",
        "            })\n",
        "\n",
        "    print(f\"  Generated {len(samples):,} completion pairs\")\n",
        "    return samples\n",
        "\n",
        "def prepare_nextword_data(corpus_path: str, max_samples: int = 100000, context_length: int = 3) -> List[dict]:\n",
        "    \"\"\"Generate next-word prediction pairs from keyboard_training_data.txt\"\"\"\n",
        "    print(\"\\nGenerating next-word prediction data...\")\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "\n",
        "            line = line.strip().lower()\n",
        "            words = line.split()\n",
        "\n",
        "            # Skip short sentences\n",
        "            if len(words) < context_length + 1:\n",
        "                continue\n",
        "\n",
        "            # Generate multiple samples from each sentence\n",
        "            for i in range(len(words) - context_length):\n",
        "                context = ' '.join(words[i:i+context_length])\n",
        "                target = words[i+context_length]\n",
        "\n",
        "                # Filter out punctuation-only targets\n",
        "                if target.isalpha() and len(target) > 1:\n",
        "                    samples.append({\n",
        "                        'input': context,\n",
        "                        'target': target,\n",
        "                        'task': 'nextword'\n",
        "                    })\n",
        "\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "\n",
        "    print(f\"  Generated {len(samples):,} next-word pairs\")\n",
        "    return samples\n",
        "\n",
        "def prepare_typo_data(typo_path: str, max_samples: int = 20000) -> List[dict]:\n",
        "    \"\"\"Generate typo correction pairs from misspelled.csv\"\"\"\n",
        "    print(\"\\nGenerating typo correction data...\")\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    with open(typo_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if len(samples) >= max_samples:\n",
        "                break\n",
        "\n",
        "            correct = row['label'].strip().lower()\n",
        "            misspelled_list = row['input'].strip().lower()\n",
        "\n",
        "            # Split multiple misspellings (comma or space separated)\n",
        "            typos = [t.strip() for t in misspelled_list.replace(',', ' ').split() if t.strip()]\n",
        "\n",
        "            for typo in typos:\n",
        "                if len(samples) >= max_samples:\n",
        "                    break\n",
        "\n",
        "                if typo and typo != correct:\n",
        "                    samples.append({\n",
        "                        'input': typo,\n",
        "                        'target': correct,\n",
        "                        'task': 'typo'\n",
        "                    })\n",
        "\n",
        "    print(f\"  Generated {len(samples):,} typo pairs\")\n",
        "    return samples\n",
        "\n",
        "# Generate all datasets\n",
        "print(\"Preparing training datasets...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "train_path = f\"{output_dir}/train.jsonl\"\n",
        "val_path = f\"{output_dir}/val.jsonl\"\n",
        "\n",
        "# Check if processed datasets already exist\n",
        "if os.path.exists(train_path) and os.path.exists(val_path):\n",
        "    print(\"âœ“ Processed datasets found in Drive!\")\n",
        "    print(f\"  Train: {train_path}\")\n",
        "    print(f\"  Val: {val_path}\")\n",
        "\n",
        "    # Count samples\n",
        "    with open(train_path, 'r') as f:\n",
        "        train_count = sum(1 for _ in f)\n",
        "    with open(val_path, 'r') as f:\n",
        "        val_count = sum(1 for _ in f)\n",
        "    print(f\"  Train samples: {train_count:,}\")\n",
        "    print(f\"  Val samples: {val_count:,}\")\n",
        "else:\n",
        "    print(\"Generating training datasets from scratch...\")\n",
        "\n",
        "    # Generate each task\n",
        "    completion_samples = prepare_word_completion_data(WORD_FREQ_PATH, max_samples=50000)\n",
        "    nextword_samples = prepare_nextword_data(CORPUS_PATH, max_samples=100000, context_length=3)\n",
        "    typo_samples = prepare_typo_data(TYPO_PATH, max_samples=20000)\n",
        "\n",
        "    # Combine all samples\n",
        "    all_samples = completion_samples + nextword_samples + typo_samples\n",
        "    random.shuffle(all_samples)\n",
        "\n",
        "    # Split train/val (90/10)\n",
        "    split_idx = int(len(all_samples) * 0.9)\n",
        "    train_samples = all_samples[:split_idx]\n",
        "    val_samples = all_samples[split_idx:]\n",
        "\n",
        "    # Save to JSONL\n",
        "    with open(train_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in train_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    with open(val_path, 'w', encoding='utf-8') as f:\n",
        "        for sample in val_samples:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ“ Dataset generation complete!\")\n",
        "    print(f\"  Total samples: {len(all_samples):,}\")\n",
        "    print(f\"  Train: {len(train_samples):,} ({train_path})\")\n",
        "    print(f\"  Val: {len(val_samples):,} ({val_path})\")\n",
        "    print(f\"\\n  Task distribution:\")\n",
        "    print(f\"    Completion: {len(completion_samples):,}\")\n",
        "    print(f\"    Next-word: {len(nextword_samples):,}\")\n",
        "    print(f\"    Typo: {len(typo_samples):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ“ Datasets ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQcETLToNwaf"
      },
      "source": [
        "## 4. Load TinyBERT Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwdQlQkeNwaf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "print(\"Loading TinyBERT for Masked Language Modeling...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load TinyBERT (4 layers, 312 hidden, 4 heads, ~14M params)\n",
        "MODEL_NAME = \"google/bert_uncased_L-4_H-256_A-4\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"âœ“ Model loaded on {device}\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Layers: 4, Hidden: 312, Heads: 12\")\n",
        "print(f\"  Model size: ~55MB (FP32) â†’ ~5MB (INT8 quantized)\")\n",
        "print(f\"  Target latency: <50ms on mobile\")\n",
        "print(f\"  Target RAM: <30MB runtime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48li_GV4Nwaf"
      },
      "source": [
        "## 5. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peBkxjlgNwaf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import torch\n",
        "\n",
        "class KeyboardDataset(Dataset):\n",
        "    \"\"\"BERT MLM dataset for keyboard suggestions.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=16):\n",
        "        self.data = []\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text_input = item['input']\n",
        "        target_word = item['target']\n",
        "        task = item.get('task', 'completion')\n",
        "\n",
        "        # Add [MASK] for prediction\n",
        "        # - Completion: \"hel [MASK]\" â†’ predict \"lo\" (hello)\n",
        "        # - Next-word: \"how are [MASK]\" â†’ predict \"you\"\n",
        "        # - Typo: \"thers [MASK]\" â†’ predict \"there\"\n",
        "        text_input = f\"{text_input} {self.tokenizer.mask_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            text_input,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Labels: -100 everywhere except [MASK]\n",
        "        labels = torch.full(inputs['input_ids'].shape, -100, dtype=torch.long)\n",
        "\n",
        "        # Get target token ID (first token of target word)\n",
        "        target_tokens = self.tokenizer.tokenize(target_word)\n",
        "        target_id = self.tokenizer.convert_tokens_to_ids(target_tokens[0]) if target_tokens else self.tokenizer.unk_token_id\n",
        "\n",
        "        # Set label at [MASK] position\n",
        "        mask_positions = (inputs['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "        if len(mask_positions[1]) > 0:\n",
        "            labels[0, mask_positions[1][0]] = target_id\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'task': task\n",
        "        }\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "train_dataset = KeyboardDataset(train_path, tokenizer, max_length=16)\n",
        "val_dataset = KeyboardDataset(val_path, tokenizer, max_length=16)\n",
        "\n",
        "print(f\"âœ“ Train samples: {len(train_dataset):,}\")\n",
        "print(f\"âœ“ Val samples: {len(val_dataset):,}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"âœ“ Batch size: 32 (train), 64 (val)\")\n",
        "print(f\"âœ“ Max sequence length: 16 tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXyd99JhNwaf"
      },
      "source": [
        "## 6. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_VXLd4ANwaf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 3e-5  # Lower LR for fine-tuning\n",
        "SAVE_STEPS = 1000\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"Optimizer: AdamW\")\n",
        "print(f\"Save checkpoints every: {SAVE_STEPS} steps\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        global_step += 1\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        if global_step % SAVE_STEPS == 0:\n",
        "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_dir)\n",
        "            print(f\"\\nâœ“ Checkpoint saved: {checkpoint_dir}\")\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
        "        model.save_pretrained(best_model_dir)\n",
        "        tokenizer.save_pretrained(best_model_dir)\n",
        "        print(f\"  âœ“ Best model saved: {best_model_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ“ Training complete!\")\n",
        "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"  Total steps: {global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czEMLVjNwag"
      },
      "source": [
        "## 7. Export and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXGwWzuhNwag"
      },
      "outputs": [],
      "source": [
        "# Export and save the BEST model (for iOS/Android deployment)\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Exporting BEST model for deployment...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the best model (saved during training)\n",
        "best_model_path = f\"{DRIVE_DIR}/models/best_model\"\n",
        "print(f\"\\nLoading best model from: {best_model_path}\")\n",
        "\n",
        "# Load best model\n",
        "best_model = AutoModelForMaskedLM.from_pretrained(best_model_path)\n",
        "best_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
        "print(\"âœ“ Best model loaded\")\n",
        "\n",
        "# 1. Save to Google Drive as 'final' (for deployment)\n",
        "drive_model_dir = f\"{DRIVE_DIR}/models/tinybert_keyboard_final\"\n",
        "best_model.save_pretrained(drive_model_dir)\n",
        "best_tokenizer.save_pretrained(drive_model_dir)\n",
        "print(f\"\\nâœ… Saved to Google Drive: {drive_model_dir}\")\n",
        "\n",
        "# 2. Create downloadable zip\n",
        "local_model_dir = \"/content/tinybert_keyboard_model\"\n",
        "best_model.save_pretrained(local_model_dir)\n",
        "best_tokenizer.save_pretrained(local_model_dir)\n",
        "\n",
        "zip_path = \"/content/tinybert_keyboard_model.zip\"\n",
        "shutil.make_archive(\"/content/tinybert_keyboard_model\", 'zip', local_model_dir)\n",
        "print(f\"\\nâœ… Created zip: {zip_path}\")\n",
        "\n",
        "# 3. Download to local device\n",
        "if IN_COLAB:\n",
        "    print(\"\\nðŸ“¥ Downloading BEST model to your computer...\")\n",
        "    files.download(zip_path)\n",
        "    print(\"âœ… Download started! Check your Downloads folder.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Export complete!\")\n",
        "print(\"\\nâš ï¸  IMPORTANT: This is the BEST model (lowest validation loss)\")\n",
        "print(\"   Use this for iOS/Android deployment!\")\n",
        "print(\"\\nModel saved to:\")\n",
        "print(f\"  1. Google Drive: {drive_model_dir}\")\n",
        "print(f\"  2. Local download: tinybert_keyboard_model.zip\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run Section 8 (Export to CoreML for iOS)\")\n",
        "print(\"  2. Run Section 9 (Export to TFLite for Android)\")\n",
        "print(\"\\nðŸ’¡ Both iOS and Android exports will use this BEST model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzAbcrkwNwag"
      },
      "source": [
        "## 8. Export to CoreML (iOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJPrGe-XNwag"
      },
      "outputs": [],
      "source": [
        "# Export to CoreML for iOS with INT8 quantization\n",
        "!pip install -q coremltools\n",
        "import coremltools as ct\n",
        "import coremltools.optimize.coreml as cto\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Wrapper Class (Keep ensuring this is defined) ---\n",
        "class WrapperModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "# ... (Assume Steps 1-3 ran successfully and you have 'mlmodel') ...\n",
        "\n",
        "# 4. Quantize to INT8 (Corrected)\n",
        "print(\"\\n4. Quantizing to INT8...\")\n",
        "\n",
        "# A. Create the configuration\n",
        "# We define: INT8, Linear Symmetric mode, Per-Channel granularity (best accuracy)\n",
        "op_config = cto.OpLinearQuantizerConfig(\n",
        "    mode=\"linear_symmetric\",\n",
        "    dtype=\"int8\",\n",
        "    granularity=\"per_channel\"\n",
        ")\n",
        "config = cto.OptimizationConfig(global_config=op_config)\n",
        "\n",
        "# B. Apply the quantization using the config\n",
        "mlmodel_int8 = cto.linear_quantize_weights(mlmodel, config=config)\n",
        "\n",
        "print(\"   âœ“ Quantization complete\")\n",
        "\n",
        "# 5. Save Model AND Vocabulary\n",
        "coreml_dir = f\"{DRIVE_DIR}/models/ios_export\"\n",
        "import os\n",
        "os.makedirs(coreml_dir, exist_ok=True)\n",
        "\n",
        "# Save the CoreML Model\n",
        "coreml_path = f\"{coreml_dir}/TinyBERT_Keyboard_iOS.mlpackage\"\n",
        "mlmodel_int8.save(coreml_path)\n",
        "\n",
        "# Save the Vocabulary File (CRITICAL FOR iOS)\n",
        "best_tokenizer.save_vocabulary(coreml_dir)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… iOS CoreML export complete!\")\n",
        "print(f\"Directory: {coreml_dir}\")\n",
        "print(\"Files created:\")\n",
        "print(f\"  1. TinyBERT_Keyboard_iOS.mlpackage (The Model)\")\n",
        "print(f\"  2. vocab.txt (The Tokenizer Dictionary)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ZWroTPNwag"
      },
      "source": [
        "## 9. Export to TFLite (Android)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNj5KAgENwag"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Android TFLite Export: Direct PyTorch â†’ TensorFlow â†’ TFLite\n",
        "# Bypassing ONNX to avoid compatibility issues\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q optimum tensorflow transformers\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "print(\"\\nExporting to TFLite for Android (Direct PyTorch â†’ TF)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- STEP 1: Save PyTorch Model ---\n",
        "print(\"\\n1. Saving PyTorch model...\")\n",
        "staging_dir = Path(\"temp_staging_model\")\n",
        "if staging_dir.exists():\n",
        "    shutil.rmtree(staging_dir)\n",
        "staging_dir.mkdir()\n",
        "\n",
        "best_model.save_pretrained(staging_dir)\n",
        "best_tokenizer.save_pretrained(staging_dir)\n",
        "print(\"   âœ“ Model saved\")\n",
        "\n",
        "# --- STEP 2: Convert PyTorch â†’ TensorFlow ---\n",
        "print(\"\\n2. Converting PyTorch â†’ TensorFlow...\")\n",
        "\n",
        "# Load as TensorFlow model directly\n",
        "tf_model = TFAutoModelForMaskedLM.from_pretrained(\n",
        "    staging_dir,\n",
        "    from_pt=True  # Convert from PyTorch\n",
        ")\n",
        "print(\"   âœ“ Conversion successful\")\n",
        "\n",
        "# --- STEP 3: Save as TensorFlow SavedModel ---\n",
        "print(\"\\n3. Saving TensorFlow SavedModel...\")\n",
        "\n",
        "tf_saved_model_dir = \"tf_saved_model\"\n",
        "if os.path.exists(tf_saved_model_dir):\n",
        "    shutil.rmtree(tf_saved_model_dir)\n",
        "\n",
        "# Create input signature\n",
        "import numpy as np\n",
        "\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=(1, 16), dtype=tf.int32, name='input_ids'),\n",
        "    tf.TensorSpec(shape=(1, 16), dtype=tf.int32, name='attention_mask')\n",
        "])\n",
        "def serving_fn(input_ids, attention_mask):\n",
        "    outputs = tf_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    return {'logits': outputs.logits}\n",
        "\n",
        "# Save with concrete function\n",
        "tf.saved_model.save(\n",
        "    tf_model,\n",
        "    tf_saved_model_dir,\n",
        "    signatures={'serving_default': serving_fn}\n",
        ")\n",
        "print(\"   âœ“ SavedModel created\")\n",
        "\n",
        "# --- STEP 4: Convert to TFLite ---\n",
        "print(\"\\n4. Converting to TFLite (INT8 Quantization)...\")\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n",
        "\n",
        "# Configure quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "\n",
        "try:\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   âœ“ TFLite conversion successful (INT8)\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš  INT8 failed, trying FP16...\")\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"   âœ“ TFLite conversion successful (FP16)\")\n",
        "\n",
        "# --- STEP 5: Save Final Model ---\n",
        "print(\"\\n5. Packaging for Android...\")\n",
        "\n",
        "tflite_dir = Path(f\"{DRIVE_DIR}/models/android\")\n",
        "tflite_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tflite_path = tflite_dir / \"keyboard_model_quantized.tflite\"\n",
        "with open(tflite_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "best_tokenizer.save_vocabulary(str(tflite_dir))\n",
        "\n",
        "model_size_mb = os.path.getsize(tflite_path) / (1024 * 1024)\n",
        "\n",
        "# --- STEP 6: Verification ---\n",
        "print(\"\\n6. Verifying model...\")\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(f\"   âœ“ Model verified\")\n",
        "    print(f\"   Inputs: {len(input_details)}\")\n",
        "    for inp in input_details:\n",
        "        print(f\"     - {inp['name']}: {inp['shape']}\")\n",
        "    print(f\"   Outputs: {len(output_details)}\")\n",
        "    for out in output_details:\n",
        "        print(f\"     - {out['name']}: {out['shape']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âš  Verification warning (OK on Android)\")\n",
        "\n",
        "# --- STEP 7: Cleanup ---\n",
        "print(\"\\n7. Cleaning up...\")\n",
        "for path in [staging_dir, tf_saved_model_dir]:\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Android TFLite export complete!\")\n",
        "print(f\"\\nSaved to: {tflite_dir}\")\n",
        "print(f\"Model: keyboard_model_quantized.tflite ({model_size_mb:.2f} MB)\")\n",
        "print(f\"\\nðŸ“¦ Files:\")\n",
        "print(f\"  1. keyboard_model_quantized.tflite\")\n",
        "print(f\"  2. vocab.txt\")\n",
        "print(f\"\\nðŸ“± build.gradle:\")\n",
        "print(f\"  implementation 'org.tensorflow:tensorflow-lite:2.14.0'\")\n",
        "print(f\"  implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.14.0'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}