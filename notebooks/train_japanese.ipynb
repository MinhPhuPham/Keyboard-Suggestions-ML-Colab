{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Keyboard Suggestion Model Training (with Google Drive Integration)\n",
    "\n",
    "This notebook trains a Japanese keyboard suggestion model using Qwen2-1.5B with LoRA fine-tuning.\n",
    "\n",
    "**Features:**\n",
    "- Automatic Google Drive data management\n",
    "- Checks for existing data before downloading\n",
    "- Email notifications on completion\n",
    "- Saves models to Drive for persistence\n",
    "\n",
    "**Target Specifications:**\n",
    "- Model Size: 40-60 MB (after optimization)\n",
    "- Latency: < 80 ms\n",
    "- Perplexity: < 20\n",
    "- Top-3 Accuracy: > 80%\n",
    "- IME Support: Romaji ‚Üí Kanji conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úì Running in Google Colab\")\n",
    "else:\n",
    "    print(\"‚úì Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    %cd Keyboard-Suggestions-ML-Colab\n",
    "    print(\"‚úì Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download UniDic for Japanese morphological analysis\n",
    "!python -m unidic download\n",
    "print(\"‚úì UniDic downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import fugashi\n",
    "\n",
    "# Import custom utilities\n",
    "from data_prep import clean_japanese_text, prepare_japanese_data\n",
    "from model_utils import (\n",
    "    load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "    prune_model, quantize_model, merge_lora_weights\n",
    ")\n",
    "from export_utils import (\n",
    "    export_to_onnx, export_to_coreml, verify_model_size,\n",
    "    benchmark_latency, package_for_download\n",
    ")\n",
    "from colab_data_manager import (\n",
    "    mount_google_drive, setup_japanese_data, send_notification_email\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    mount_success = mount_google_drive()\n",
    "    if not mount_success:\n",
    "        raise Exception(\"Failed to mount Google Drive\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Japanese training data\n",
    "# This will check Drive first, download if needed\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Phu's Data development\"\n",
    "    data_path = setup_japanese_data(DRIVE_BASE)\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"‚ö† Data setup failed. Please check errors above.\")\n",
    "        print(\"\\nManual setup instructions:\")\n",
    "        print(\"1. CC100 Japanese will be downloaded automatically from Hugging Face\")\n",
    "        print(\"2. If download fails, check internet connection and try again\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Data ready at: {data_path}\")\n",
    "else:\n",
    "    data_path = \"./data/japanese\"\n",
    "    print(f\"Using local data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Japanese morphological analyzer\n",
    "tagger = fugashi.Tagger('-Owakati')\n",
    "\n",
    "# Test morphological analysis\n",
    "test_text = \"‰ªäÊó•„ÅØÊò®Êó•„Çà„ÇäËâØ„ÅÑÊó•„Å†\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Morphemes: {tagger.parse(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample training data\n",
    "# For demonstration, using sample data\n",
    "# Replace with actual CC100 processing\n",
    "\n",
    "sample_sentences = [\n",
    "    \"‰ªäÊó•„ÅØËâØ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠\",\n",
    "    \"ÊòéÊó•‰ºöË≠∞„Åå„ÅÇ„Çä„Åæ„Åô\",\n",
    "    \"„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\",\n",
    "    \"„ÅäÁñ≤„ÇåÊßò„Åß„Åó„Åü\",\n",
    "    \"„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô\",\n",
    "    \"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\",\n",
    "    \"„Åì„Çì„Å´„Å°„ÅØ\",\n",
    "    \"„Åï„Çà„ÅÜ„Å™„Çâ\",\n",
    "]\n",
    "\n",
    "# Clean text\n",
    "cleaned = [clean_japanese_text(s) for s in sample_sentences]\n",
    "\n",
    "print(f\"Sample sentences:\")\n",
    "for sent in cleaned[:5]:\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with LoRA\n",
    "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model, tokenizer = load_model_with_lora(\n",
    "    model_name=MODEL_NAME,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # More modules for Japanese\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded with LoRA adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=8)\n",
    "\n",
    "# Create dataset\n",
    "train_data = Dataset.from_dict({'text': cleaned})\n",
    "train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"‚úì Training dataset prepared: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Set checkpoint directory to Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    checkpoint_dir = os.path.join(DRIVE_BASE, \"checkpoints\", \"japanese\")\n",
    "else:\n",
    "    checkpoint_dir = \"./checkpoints/japanese\"\n",
    "\n",
    "trainer = train_causal_lm(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    learning_rate=5e-6,  # Lower LR for Japanese\n",
    "    max_seq_length=8,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights\n",
    "model = merge_lora_weights(model)\n",
    "print(\"‚úì LoRA weights merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune model (more aggressive for larger model)\n",
    "model = prune_model(model, amount=0.4)\n",
    "print(\"‚úì Model pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "model = quantize_model(model, dtype=torch.qint8)\n",
    "print(\"‚úì Model quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model output directory to Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(DRIVE_BASE, \"models\", \"japanese\")\n",
    "else:\n",
    "    model_dir = \"./models/japanese\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = export_to_onnx(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=os.path.join(model_dir, \"japanese_model.onnx\"),\n",
    "    max_seq_length=8\n",
    ")\n",
    "\n",
    "print(f\"‚úì ONNX model saved to: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Core ML (for iOS)\n",
    "coreml_path = export_to_coreml(\n",
    "    onnx_path=onnx_path,\n",
    "    output_path=os.path.join(model_dir, \"japanese_model.mlmodel\"),\n",
    "    model_name=\"JapaneseKeyboardSuggestion\"\n",
    ")\n",
    "\n",
    "if coreml_path:\n",
    "    print(f\"‚úì Core ML model saved to: {coreml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model size\n",
    "size_mb, meets_req = verify_model_size(\n",
    "    model_path=onnx_path,\n",
    "    max_size_mb=60\n",
    ")\n",
    "\n",
    "if meets_req:\n",
    "    print(f\"‚úì Model size requirement met: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ö† Model size exceeds target: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IME functionality (romaji ‚Üí kanji)\n",
    "test_inputs = [\n",
    "    \"kyouha\",  # ‰ªäÊó•„ÅØ\n",
    "    \"arigatou\",  # „ÅÇ„Çä„Åå„Å®„ÅÜ\n",
    "]\n",
    "\n",
    "print(\"IME Test (requires additional IME layer):\")\n",
    "for inp in test_inputs:\n",
    "    print(f\"  {inp} ‚Üí [IME conversion needed]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for download\n",
    "zip_path = package_for_download(\n",
    "    model_dir=model_dir,\n",
    "    output_zip=\"japanese_model.zip\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model packaged: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send completion notification\n",
    "send_notification_email(\n",
    "    subject=\"Japanese Model Training Complete! üéâ\",\n",
    "    message=f\"\"\"\n",
    "Japanese keyboard suggestion model training has completed successfully!\n",
    "\n",
    "Model Details:\n",
    "- Size: {size_mb:.2f} MB\n",
    "- Location: {model_dir}\n",
    "- Package: {zip_path}\n",
    "\n",
    "The model is ready for integration into your keyboard app.\n",
    "\n",
    "Next steps:\n",
    "1. Download the model package\n",
    "2. Add IME layer for romaji ‚Üí kanji conversion\n",
    "3. Integrate into iOS/Android app\n",
    "4. Test on actual devices with Japanese input\n",
    "    \"\"\",\n",
    "    to_email=\"phamminhphueur@gmail.com\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Package: {zip_path}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading model package...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"‚úì Download started\")\n",
    "else:\n",
    "    print(f\"Model saved locally to: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
