{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Keyboard Suggestion Model Training (with Google Drive Integration)\n",
    "\n",
    "This notebook trains a Japanese keyboard suggestion model using Qwen2-1.5B with LoRA fine-tuning.\n",
    "\n",
    "**Features:**\n",
    "- Automatic Google Drive data management\n",
    "- Checks for existing data before downloading\n",
    "- Email notifications on completion\n",
    "- Saves models to Drive for persistence\n",
    "\n",
    "**Target Specifications:**\n",
    "- Model Size: 40-60 MB (after optimization)\n",
    "- Latency: < 80 ms\n",
    "- Perplexity: < 20\n",
    "- Top-3 Accuracy: > 80%\n",
    "- IME Support: Romaji \u2192 Kanji conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    # Clone repo\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    import os\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"\u2713 Repository cloned\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to Python path (works in both Colab and local)\n",
    "if IN_COLAB:\n",
    "    src_path = '/content/Keyboard-Suggestions-ML-Colab/src'\n",
    "else:\n",
    "    src_path = os.path.abspath('./src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"\u2713 Added {src_path} to Python path\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import fugashi\n",
    "\n",
    "\n",
    "# Import custom utilities\n",
    "from data_prep import clean_japanese_text, prepare_japanese_data\n",
    "from model_utils import (\n",
    "    load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "    prune_model, quantize_model, merge_lora_weights\n",
    ")\n",
    "from export_utils import (\n",
    "    export_to_onnx, export_to_coreml, verify_model_size,\n",
    "    benchmark_latency, package_for_download\n",
    ")\n",
    "from colab_data_manager import (\n",
    "    mount_google_drive, setup_japanese_data, send_notification_email\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to Python path\n",
    "if IN_COLAB:\n",
    "    if not os.getcwd().endswith('Keyboard-Suggestions-ML-Colab'):\n",
    "        os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    src_path = os.path.join(os.getcwd(), 'src')\n",
    "else:\n",
    "    src_path = os.path.abspath('./src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"\u2713 Python path: {src_path}\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import fugashi\n",
    "\n",
    "# Import custom utilities\n",
    "from data_prep import clean_japanese_text, prepare_japanese_data\n",
    "from model_utils import (\n",
    "    load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "    prune_model, quantize_model, merge_lora_weights\n",
    ")\n",
    "from export_utils import (\n",
    "    export_to_onnx, export_to_coreml, verify_model_size,\n",
    "    benchmark_latency, package_for_download\n",
    ")\n",
    "from colab_data_manager import (\n",
    "    mount_google_drive, setup_japanese_data, send_notification_email\n",
    ")\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f WARNING: GPU not available!\")\n",
    "    print(\"To enable GPU in Colab:\")\n",
    "    print(\"1. Runtime \u2192 Change runtime type\")\n",
    "    print(\"2. Hardware accelerator \u2192 GPU\")\n",
    "    print(\"3. Save\")\n",
    "    print(\"4. Restart runtime and re-run cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download UniDic for Japanese morphological analysis\n",
    "!python -m unidic download\n",
    "print(\"\u2713 UniDic downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "import fugashi\n",
    "\n",
    "# Import custom utilities\n",
    "from data_prep import clean_japanese_text, prepare_japanese_data\n",
    "from model_utils import (\n",
    "    load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "    prune_model, quantize_model, merge_lora_weights\n",
    ")\n",
    "from export_utils import (\n",
    "    export_to_onnx, export_to_coreml, verify_model_size,\n",
    "    benchmark_latency, package_for_download\n",
    ")\n",
    "from colab_data_manager import (\n",
    "    mount_google_drive, setup_japanese_data, send_notification_email\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    mount_success = mount_google_drive()\n",
    "    if not mount_success:\n",
    "        raise Exception(\"Failed to mount Google Drive\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Japanese training data\n",
    "# This will check Drive first, download if needed\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Phu's Data development\"\n",
    "    data_path = setup_japanese_data(DRIVE_BASE)\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"\u26a0 Data setup failed. Please check errors above.\")\n",
    "        print(\"\\nManual setup instructions:\")\n",
    "        print(\"1. CC100 Japanese will be downloaded automatically from Hugging Face\")\n",
    "        print(\"2. If download fails, check internet connection and try again\")\n",
    "    else:\n",
    "        print(f\"\\n\u2713 Data ready at: {data_path}\")\n",
    "else:\n",
    "    data_path = \"./data/japanese\"\n",
    "    print(f\"Using local data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Japanese morphological analyzer\n",
    "tagger = fugashi.Tagger('-Owakati')\n",
    "\n",
    "# Test morphological analysis\n",
    "test_text = \"\u4eca\u65e5\u306f\u6628\u65e5\u3088\u308a\u826f\u3044\u65e5\u3060\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Morphemes: {tagger.parse(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample training data\n",
    "# For demonstration, using sample data\n",
    "# Replace with actual CC100 processing\n",
    "\n",
    "sample_sentences = [\n",
    "    \"\u4eca\u65e5\u306f\u826f\u3044\u5929\u6c17\u3067\u3059\u306d\",\n",
    "    \"\u660e\u65e5\u4f1a\u8b70\u304c\u3042\u308a\u307e\u3059\",\n",
    "    \"\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\",\n",
    "    \"\u304a\u75b2\u308c\u69d8\u3067\u3057\u305f\",\n",
    "    \"\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3057\u307e\u3059\",\n",
    "    \"\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\",\n",
    "    \"\u3053\u3093\u306b\u3061\u306f\",\n",
    "    \"\u3055\u3088\u3046\u306a\u3089\",\n",
    "]\n",
    "\n",
    "# Clean text\n",
    "cleaned = [clean_japanese_text(s) for s in sample_sentences]\n",
    "\n",
    "print(f\"Sample sentences:\")\n",
    "for sent in cleaned[:5]:\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with LoRA\n",
    "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model, tokenizer = load_model_with_lora(\n",
    "    model_name=MODEL_NAME,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"]  # More modules for Japanese\n",
    ")\n",
    "\n",
    "print(\"\u2713 Model loaded with LoRA adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=8)\n",
    "\n",
    "# Create dataset\n",
    "train_data = Dataset.from_dict({'text': cleaned})\n",
    "train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"\u2713 Training dataset prepared: {len(train_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Set checkpoint directory to Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    checkpoint_dir = os.path.join(DRIVE_BASE, \"checkpoints\", \"japanese\")\n",
    "else:\n",
    "    checkpoint_dir = \"./checkpoints/japanese\"\n",
    "\n",
    "trainer = train_causal_lm(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=8,  # Adjust based on GPU memory\n",
    "    learning_rate=5e-6,  # Lower LR for Japanese\n",
    "    max_seq_length=8,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights\n",
    "model = merge_lora_weights(model)\n",
    "print(\"\u2713 LoRA weights merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune model (more aggressive for larger model)\n",
    "model = prune_model(model, amount=0.4)\n",
    "print(\"\u2713 Model pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model\n",
    "model = quantize_model(model, dtype=torch.qint8)\n",
    "print(\"\u2713 Model quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model output directory to Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(DRIVE_BASE, \"models\", \"japanese\")\n",
    "else:\n",
    "    model_dir = \"./models/japanese\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = export_to_onnx(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=os.path.join(model_dir, \"japanese_model.onnx\"),\n",
    "    max_seq_length=8\n",
    ")\n",
    "\n",
    "print(f\"\u2713 ONNX model saved to: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Core ML (for iOS)\n",
    "coreml_path = export_to_coreml(\n",
    "    onnx_path=onnx_path,\n",
    "    output_path=os.path.join(model_dir, \"japanese_model.mlmodel\"),\n",
    "    model_name=\"JapaneseKeyboardSuggestion\"\n",
    ")\n",
    "\n",
    "if coreml_path:\n",
    "    print(f\"\u2713 Core ML model saved to: {coreml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model size\n",
    "size_mb, meets_req = verify_model_size(\n",
    "    model_path=onnx_path,\n",
    "    max_size_mb=60\n",
    ")\n",
    "\n",
    "if meets_req:\n",
    "    print(f\"\u2713 Model size requirement met: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\u26a0 Model size exceeds target: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IME functionality (romaji \u2192 kanji)\n",
    "test_inputs = [\n",
    "    \"kyouha\",  # \u4eca\u65e5\u306f\n",
    "    \"arigatou\",  # \u3042\u308a\u304c\u3068\u3046\n",
    "]\n",
    "\n",
    "print(\"IME Test (requires additional IME layer):\")\n",
    "for inp in test_inputs:\n",
    "    print(f\"  {inp} \u2192 [IME conversion needed]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for download\n",
    "zip_path = package_for_download(\n",
    "    model_dir=model_dir,\n",
    "    output_zip=\"japanese_model.zip\"\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model packaged: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send completion notification\n",
    "send_notification_email(\n",
    "    subject=\"Japanese Model Training Complete! \ud83c\udf89\",\n",
    "    message=f\"\"\"\n",
    "Japanese keyboard suggestion model training has completed successfully!\n",
    "\n",
    "Model Details:\n",
    "- Size: {size_mb:.2f} MB\n",
    "- Location: {model_dir}\n",
    "- Package: {zip_path}\n",
    "\n",
    "The model is ready for integration into your keyboard app.\n",
    "\n",
    "Next steps:\n",
    "1. Download the model package\n",
    "2. Add IME layer for romaji \u2192 kanji conversion\n",
    "3. Integrate into iOS/Android app\n",
    "4. Test on actual devices with Japanese input\n",
    "    \"\"\",\n",
    "    to_email=\"phamminhphueur@gmail.com\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Package: {zip_path}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading model package...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"\u2713 Download started\")\n",
    "else:\n",
    "    print(f\"Model saved locally to: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}