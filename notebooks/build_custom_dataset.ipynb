{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Build Custom Japanese IME Dataset (10M Target)\n",
    "\n",
    "Create a high-quality 10M-item conversational dataset for keyboard IME training.\n",
    "\n",
    "**3-Layer Strategy:**\n",
    "\n",
    "| Layer | Source | Purpose | Target |\n",
    "|:---:|---|---|---:|\n",
    "| üß± Base | OSCAR Japanese web text | Grammar foundations | ~5M pairs |\n",
    "| üí¨ Conversation | Sh≈çsetsuka ni Nar≈ç (web novels) | Casual dialogue | ~3M pairs |\n",
    "| üî• Freshness | RSS feeds (2026 news/tech) | Modern vocabulary | ~2M pairs |\n",
    "\n",
    "**Pipeline:** Raw text ‚Üí Extract sentences ‚Üí Kill filter ‚Üí SudachiPy kana ‚Üí Augment slices ‚Üí JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Auto-detect platform\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "    PLATFORM = 'Colab'\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
    "elif os.path.exists('/kaggle/working'):\n",
    "    PLATFORM = 'Kaggle'\n",
    "    DRIVE_DIR = '/kaggle/working'\n",
    "else:\n",
    "    PLATFORM = 'Local'\n",
    "    DRIVE_DIR = './output'\n",
    "\n",
    "OUTPUT_DIR = f\"{DRIVE_DIR}/datasets\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
    "print(f\"üìÅ Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets tqdm sudachipy sudachidict_full feedparser requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# CONFIGURATION\n",
    "# ===========================================================\n",
    "\n",
    "# Total target\n",
    "TOTAL_TARGET = 10_000_000  # 10M training pairs\n",
    "\n",
    "# Per-layer targets\n",
    "LAYER_1_TARGET = 5_000_000   # OSCAR base (grammar)\n",
    "LAYER_2_TARGET = 3_000_000   # Nar≈ç novels (conversation)\n",
    "LAYER_3_TARGET = 2_000_000   # RSS feeds (freshness)\n",
    "\n",
    "# Raw lines to stream per layer (slicing creates ~5-8x pairs per sentence)\n",
    "LAYER_1_RAW_LINES = 1_500_000   # OSCAR\n",
    "LAYER_2_MAX_NOVELS = 5          # 2-5 novels (each has 100s of chapters with rich dialogue)\n",
    "LAYER_3_MAX_ARTICLES = 5_000    # RSS articles\n",
    "\n",
    "# Quality filters\n",
    "MIN_SENTENCE_LEN = 5\n",
    "MAX_SENTENCE_LEN = 120\n",
    "\n",
    "# Augmentation\n",
    "ENABLE_SLICING = True\n",
    "MIN_SLICE_WORDS = 2\n",
    "\n",
    "# Which layers to run (set False to skip)\n",
    "RUN_LAYER_1 = True   # OSCAR base\n",
    "RUN_LAYER_2 = True   # Nar≈ç novels\n",
    "RUN_LAYER_3 = True   # RSS feeds\n",
    "\n",
    "print(f\"üéØ Target: {TOTAL_TARGET:,} pairs\")\n",
    "print(f\"   Layer 1 (OSCAR):  {LAYER_1_TARGET:,} {'‚úÖ' if RUN_LAYER_1 else '‚è≠Ô∏è'}\")\n",
    "print(f\"   Layer 2 (Nar≈ç):   {LAYER_2_TARGET:,} {'‚úÖ' if RUN_LAYER_2 else '‚è≠Ô∏è'}\")\n",
    "print(f\"   Layer 3 (RSS):    {LAYER_3_TARGET:,} {'‚úÖ' if RUN_LAYER_3 else '‚è≠Ô∏è'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Pipeline: Reverse-Kana + Extraction + Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import tokenizer as sudachi_tokenizer, dictionary as sudachi_dictionary\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================================\n",
    "# SudachiPy setup (Mode C = longest splits, best for IME)\n",
    "# ==========================================================\n",
    "sudachi_dict = sudachi_dictionary.Dictionary(dict=\"full\")\n",
    "sudachi = sudachi_dict.create()\n",
    "SPLIT_MODE = sudachi_tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "print(\"‚úÖ SudachiPy (full dict, Mode C) ready\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Reverse-Kana Pipeline\n",
    "# ==========================================================\n",
    "\n",
    "def text_to_morphemes(text):\n",
    "    \"\"\"Analyze text into morphemes with readings.\"\"\"\n",
    "    tokens = sudachi.tokenize(text, SPLIT_MODE)\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        surface = t.surface()\n",
    "        reading = t.reading_form() or surface\n",
    "        pos = t.part_of_speech()[0]\n",
    "        result.append((surface, reading, pos))\n",
    "    return result\n",
    "\n",
    "\n",
    "def text_to_kana(text):\n",
    "    \"\"\"Convert text to full katakana reading.\"\"\"\n",
    "    return ''.join(r for _, r, _ in text_to_morphemes(text))\n",
    "\n",
    "\n",
    "def generate_pairs(text, left_context='', enable_slicing=True):\n",
    "    \"\"\"Generate training pairs (full + sliced) from text.\"\"\"\n",
    "    morphemes = text_to_morphemes(text)\n",
    "    if len(morphemes) < MIN_SLICE_WORDS:\n",
    "        return []\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    # Full sentence pair\n",
    "    full_kana = ''.join(r for _, r, _ in morphemes)\n",
    "    full_text = ''.join(s for s, _, _ in morphemes)\n",
    "    if full_kana != full_text:\n",
    "        pairs.append({\n",
    "            'left_context': left_context,\n",
    "            'input': full_kana,\n",
    "            'output': full_text\n",
    "        })\n",
    "    \n",
    "    if not enable_slicing:\n",
    "        return pairs\n",
    "    \n",
    "    # Sliced pairs at word boundaries (1, 2, 3 word slices)\n",
    "    for slice_size in [1, 2, 3]:\n",
    "        for i in range(0, len(morphemes) - slice_size + 1):\n",
    "            ctx_parts = ''.join(s for s, _, _ in morphemes[:i])\n",
    "            ctx = left_context + ctx_parts\n",
    "            \n",
    "            s_morphemes = morphemes[i:i + slice_size]\n",
    "            s_kana = ''.join(r for _, r, _ in s_morphemes)\n",
    "            s_text = ''.join(s for s, _, _ in s_morphemes)\n",
    "            \n",
    "            if s_kana == s_text or len(s_text) < 1:\n",
    "                continue\n",
    "            if re.match(r'^[„ÄÅ„ÄÇÔºüÔºÅ\\s]+$', s_text):\n",
    "                continue\n",
    "            \n",
    "            pairs.append({\n",
    "                'left_context': ctx[-60:] if len(ctx) > 60 else ctx,\n",
    "                'input': s_kana,\n",
    "                'output': s_text\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Quality Filters (Kill List)\n",
    "# ==========================================================\n",
    "\n",
    "KILL_PATTERNS = re.compile(\n",
    "    r'(ID:|„Ç¢„Ç§„Éá„Ç£:|ÊäïÁ®øÊó•|ÂêçÂâç:|ÂêçÁÑ°„Åó|'\n",
    "    r'\\d{2}:\\d{2}:\\d{2}|'\n",
    "    r'(http|www\\.|https)|'\n",
    "    r'ISBN|ISSN|'\n",
    "    r'„Ç®„Ç§„ÉÅ„ÉÜ„Ç£„Éº„ÉÜ„Ç£„Éº„Éî„Éº|'\n",
    "    r'„ÉÄ„Éñ„É™„É•„ÉÄ„Éñ„É™„É•)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "GARBAGE_RE = re.compile(r'[\\u0400-\\u04FF\\u0100-\\u024F]')\n",
    "ENCYCLOPEDIA_RE = re.compile(\n",
    "    r'(„Å´‰ΩçÁΩÆ„Åô„Çã|ÂåóÁ∑Ø\\d|ÂçóÁ∑Ø\\d|Êù±Áµå\\d|Ë•øÁµå\\d|Ê®ôÈ´ò\\d|Â≠¶Âêç|ÂàÜÈ°ûÂ≠¶|Á¨¨\\d+‰ª£|Á¥ÄÂÖÉÂâç)'\n",
    ")\n",
    "JAPANESE_RE = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]')\n",
    "DIALOGUE_RE = re.compile(r'„Äå([^„Äç]{2,80})„Äç')\n",
    "\n",
    "\n",
    "def extract_sentences(text):\n",
    "    \"\"\"Extract clean sentences. Returns [(sentence, is_dialogue), ...]\"\"\"\n",
    "    results = []\n",
    "    if not JAPANESE_RE.search(text):\n",
    "        return []\n",
    "    \n",
    "    # Dialogue first (highest quality)\n",
    "    for d in DIALOGUE_RE.findall(text):\n",
    "        d = d.strip()\n",
    "        if MIN_SENTENCE_LEN <= len(d) <= MAX_SENTENCE_LEN:\n",
    "            if not KILL_PATTERNS.search(d) and not GARBAGE_RE.search(d):\n",
    "                results.append((d, True))\n",
    "    \n",
    "    # Non-dialogue sentences\n",
    "    for s in re.split(r'[„ÄÇÔºÅÔºü\\n]+', text):\n",
    "        s = s.strip()\n",
    "        if not s or len(s) < MIN_SENTENCE_LEN or len(s) > MAX_SENTENCE_LEN:\n",
    "            continue\n",
    "        if KILL_PATTERNS.search(s) or GARBAGE_RE.search(s) or ENCYCLOPEDIA_RE.search(s):\n",
    "            continue\n",
    "        jp_ratio = len(JAPANESE_RE.findall(s)) / max(len(s), 1)\n",
    "        if jp_ratio < 0.3 or len(re.findall(r'\\d+', s)) >= 3:\n",
    "            continue\n",
    "        results.append((s, False))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pipeline + filters ready\")\n",
    "\n",
    "# Quick test\n",
    "test = 'Áî∞‰∏≠„Åï„Çì„ÅØ„Äå„Éû„Ç∏„ÅßÔºü„Äç„Å®Ë®Ä„Å£„Åü„ÄÇ‰ªäÊó•„ÅØÂ§©Ê∞ó„Åå„ÅÑ„ÅÑ„Åß„Åô„Å≠„ÄÇ'\n",
    "sents = extract_sentences(test)\n",
    "for s, d in sents:\n",
    "    tag = 'üí¨' if d else 'üìù'\n",
    "    kana = text_to_kana(s)\n",
    "    print(f\"  {tag} {s} ‚Üí {kana}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Shared processing function for all layers\n",
    "# ==========================================================\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def process_text_lines(lines_iter, output_file, max_pairs, layer_name,\n",
    "                       max_lines=None, resume_from=0):\n",
    "    \"\"\"Process text lines through the full pipeline.\n",
    "    \n",
    "    Args:\n",
    "        lines_iter: Iterator of text strings\n",
    "        output_file: Path to save JSONL\n",
    "        max_pairs: Stop after this many pairs\n",
    "        layer_name: For display\n",
    "        max_lines: Max input lines to process\n",
    "        resume_from: Resume from this line index\n",
    "    Returns: Stats dict\n",
    "    \"\"\"\n",
    "    progress_file = output_file + '.progress'\n",
    "    \n",
    "    stats = {\n",
    "        'lines_processed': 0,\n",
    "        'sentences': 0,\n",
    "        'dialogues': 0,\n",
    "        'pairs': 0,\n",
    "        'errors': 0,\n",
    "    }\n",
    "    \n",
    "    # Resume check\n",
    "    if resume_from > 0 and os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            saved = json.load(f)\n",
    "        stats.update(saved.get('stats', {}))\n",
    "        print(f\"üìÇ Resuming {layer_name} from line {resume_from:,} ({stats['pairs']:,} pairs so far)\")\n",
    "    \n",
    "    mode = 'a' if resume_from > 0 else 'w'\n",
    "    prev_sentence = ''\n",
    "    line_idx = 0\n",
    "    \n",
    "    with open(output_file, mode, encoding='utf-8') as out_f:\n",
    "        pbar = tqdm(desc=f\"{layer_name}\", total=max_lines)\n",
    "        \n",
    "        for text in lines_iter:\n",
    "            line_idx += 1\n",
    "            if line_idx <= resume_from:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            stats['lines_processed'] += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            sentences = extract_sentences(text)\n",
    "            prev_sentence = ''\n",
    "            \n",
    "            for sentence, is_dialogue in sentences:\n",
    "                stats['sentences'] += 1\n",
    "                if is_dialogue:\n",
    "                    stats['dialogues'] += 1\n",
    "                \n",
    "                try:\n",
    "                    pairs = generate_pairs(sentence, prev_sentence, ENABLE_SLICING)\n",
    "                    for p in pairs:\n",
    "                        out_f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n",
    "                    stats['pairs'] += len(pairs)\n",
    "                    prev_sentence = sentence\n",
    "                except Exception as e:\n",
    "                    stats['errors'] += 1\n",
    "                    if stats['errors'] <= 3:\n",
    "                        print(f\"  ‚ö† {e}: {sentence[:30]}\")\n",
    "            \n",
    "            # Save progress every 20K lines\n",
    "            if stats['lines_processed'] % 20_000 == 0:\n",
    "                with open(progress_file, 'w') as pf:\n",
    "                    json.dump({'line_idx': line_idx, 'stats': stats}, pf)\n",
    "                out_f.flush()\n",
    "                pbar.set_postfix(pairs=f\"{stats['pairs']:,}\")\n",
    "            \n",
    "            # Stop conditions\n",
    "            if stats['pairs'] >= max_pairs:\n",
    "                print(f\"\\n  ‚úì Reached {max_pairs:,} pairs target\")\n",
    "                break\n",
    "            if max_lines and stats['lines_processed'] >= max_lines:\n",
    "                break\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    # Final progress save\n",
    "    with open(progress_file, 'w') as pf:\n",
    "        json.dump({'line_idx': line_idx, 'stats': stats}, pf)\n",
    "    \n",
    "    print(f\"  ‚úÖ {layer_name}: {stats['pairs']:,} pairs from {stats['sentences']:,} sentences\")\n",
    "    print(f\"     Dialogues: {stats['dialogues']:,} | Errors: {stats['errors']}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß± Layer 1: CulturaX ‚Äî Base Grammar (~5M pairs)\n",
    "\n",
    "CulturaX is a cleaned, deduplicated web text corpus (6.3T tokens, 167 languages).\n",
    "\n",
    "**Why CulturaX?** `cc100`, `mc4`, `OSCAR` all use deprecated dataset scripts.\n",
    "CulturaX uses modern Parquet format ‚Äî works perfectly with latest HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LAYER_1:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"üì• Layer 1: Loading Japanese web text (streaming)...\")\n",
    "    print(\"   This streams data ‚Äî no full download needed.\")\n",
    "    \n",
    "    # Try sources in order (all use Parquet, no deprecated scripts)\n",
    "    base_ds = None\n",
    "    text_key = \"text\"\n",
    "    \n",
    "    # Option 1: CulturaX (cleaned web text, Parquet, no gating)\n",
    "    try:\n",
    "        print(\"   Trying CulturaX (ja)...\")\n",
    "        base_ds = load_dataset(\n",
    "            \"uonlp/CulturaX\",\n",
    "            \"ja\",\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "        text_key = \"text\"\n",
    "        print(\"   ‚úì CulturaX loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† CulturaX failed: {e}\")\n",
    "    \n",
    "    # Option 2: FineWeb-2 Japanese\n",
    "    if base_ds is None:\n",
    "        try:\n",
    "            print(\"   Trying FineWeb-2 (jpn_Jpan)...\")\n",
    "            base_ds = load_dataset(\n",
    "                \"HuggingFaceFW/fineweb-2\",\n",
    "                \"jpn_Jpan\",\n",
    "                split=\"train\",\n",
    "                streaming=True\n",
    "            )\n",
    "            text_key = \"text\"\n",
    "            print(\"   ‚úì FineWeb-2 loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö† FineWeb-2 failed: {e}\")\n",
    "    \n",
    "    # Option 3: Wikipedia Japanese (always works, Parquet)\n",
    "    if base_ds is None:\n",
    "        try:\n",
    "            print(\"   Trying Wikipedia Japanese (fallback)...\")\n",
    "            base_ds = load_dataset(\n",
    "                \"wikimedia/wikipedia\",\n",
    "                \"20231101.ja\",\n",
    "                split=\"train\",\n",
    "                streaming=True\n",
    "            )\n",
    "            text_key = \"text\"\n",
    "            print(\"   ‚úì Wikipedia-ja loaded (fallback)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå All sources failed: {e}\")\n",
    "            base_ds = None\n",
    "    \n",
    "    if base_ds is not None:\n",
    "        def base_lines():\n",
    "            for item in base_ds:\n",
    "                yield item.get(text_key, \"\")\n",
    "        \n",
    "        OUTPUT_L1 = f\"{OUTPUT_DIR}/layer1_base.jsonl\"\n",
    "        \n",
    "        stats_l1 = process_text_lines(\n",
    "            lines_iter=base_lines(),\n",
    "            output_file=OUTPUT_L1,\n",
    "            max_pairs=LAYER_1_TARGET,\n",
    "            max_lines=LAYER_1_RAW_LINES,\n",
    "            layer_name=\"üß± Base Grammar\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\nüíæ Layer 1 saved: {OUTPUT_L1}\")\n",
    "        print(f\"   Size: {os.path.getsize(OUTPUT_L1) / (1024**2):.1f} MB\")\n",
    "    else:\n",
    "        stats_l1 = {\"pairs\": 0}\n",
    "        print(\"‚ùå Layer 1 skipped (no data source available)\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Layer 1 skipped\")\n",
    "    stats_l1 = {\"pairs\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Layer 2: Sh≈çsetsuka ni Nar≈ç ‚Äî Conversation (~3M pairs)\n",
    "\n",
    "Web novels are the **#1 source** for modern Japanese dialogue.\n",
    "- Massive amounts of dialogue in `„Äå...„Äç` brackets\n",
    "- Modern grammar, slang, natural speech\n",
    "- Genres: Contemporary Drama (Áèæ‰ª£„Éâ„É©„Éû), Romance (ÊÅãÊÑõ), Daily Life (Êó•Â∏∏)\n",
    "\n",
    "API: `https://api.syosetu.com/novelapi/api/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LAYER_2:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import time\n",
    "    \n",
    "    # ==========================================================\n",
    "    # Nar≈ç Novel Scraper\n",
    "    # ==========================================================\n",
    "    \n",
    "    NARO_API = 'https://api.syosetu.com/novelapi/api/'\n",
    "    NARO_BASE = 'https://ncode.syosetu.com'\n",
    "    \n",
    "    # Genres for conversational text\n",
    "    # 101=Áï∞‰∏ñÁïå, 102=ÁèæÂÆü‰∏ñÁïå, 201=„Éè„Ç§„Éï„Ç°„É≥„Çø„Ç∏„Éº, 301=ÊÅãÊÑõ\n",
    "    # We want realistic/modern: 102 (ÁèæÂÆü‰∏ñÁïå), 301-302 (ÊÅãÊÑõ), 401 (Êó•Â∏∏)\n",
    "    NARO_GENRES = [102, 301, 302, 401, 9901, 9902]  # Realistic, Romance, Daily, Other\n",
    "    \n",
    "    def get_novel_list(genre, limit=50, order='hyoka'):\n",
    "        \"\"\"Get top-rated novels from Nar≈ç API.\n",
    "        order: hyoka=rating, favnovelcnt=favorites, weeklypoint=weekly\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'out': 'json',\n",
    "            'genre': genre,\n",
    "            'order': order,\n",
    "            'lim': limit,\n",
    "            'of': 'n-t-ga',  # ncode, title, general_all_no (total chapters)\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(NARO_API, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            # First item is metadata (allcount), rest are novels\n",
    "            return data[1:] if len(data) > 1 else []\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† API error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    \n",
    "    def scrape_chapter(ncode, chapter_no):\n",
    "        \"\"\"Scrape a single chapter's text from Nar≈ç.\"\"\"\n",
    "        url = f\"{NARO_BASE}/{ncode}/{chapter_no}/\"\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=30, headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (IME Dataset Research)'\n",
    "            })\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # Novel body is in <div id=\"novel_honbun\">\n",
    "            body = soup.find('div', id='novel_honbun')\n",
    "            if not body:\n",
    "                return ''\n",
    "            \n",
    "            # Get text from <p> tags\n",
    "            lines = []\n",
    "            for p in body.find_all('p'):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text:\n",
    "                    lines.append(text)\n",
    "            \n",
    "            return '\\n'.join(lines)\n",
    "        except Exception as e:\n",
    "            return ''\n",
    "    \n",
    "    \n",
    "    # Collect novel list\n",
    "    print(\"üì• Layer 2: Fetching Nar≈ç novel list...\")\n",
    "    all_novels = []\n",
    "    for genre in NARO_GENRES:\n",
    "        novels = get_novel_list(genre, limit=50, order='hyoka')\n",
    "        all_novels.extend(novels)\n",
    "        print(f\"  Genre {genre}: {len(novels)} novels\")\n",
    "        time.sleep(1)  # Be polite to API\n",
    "    \n",
    "    # Deduplicate by ncode\n",
    "    seen = set()\n",
    "    unique_novels = []\n",
    "    for n in all_novels:\n",
    "        ncode = n.get('ncode', '')\n",
    "        if ncode and ncode not in seen:\n",
    "            seen.add(ncode)\n",
    "            unique_novels.append(n)\n",
    "    \n",
    "    # Limit to configured max\n",
    "    unique_novels = unique_novels[:LAYER_2_MAX_NOVELS]\n",
    "    total_chapters = sum(n.get('general_all_no', 0) for n in unique_novels)\n",
    "    \n",
    "    print(f\"  ‚úì {len(unique_novels)} unique novels, ~{total_chapters:,} total chapters\")\n",
    "    print(f\"  Scraping chapters (1 req/sec, be polite)...\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Layer 2 skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LAYER_2:\n",
    "    OUTPUT_L2 = f\"{OUTPUT_DIR}/layer2_narou.jsonl\"\n",
    "    PROGRESS_L2 = f\"{OUTPUT_DIR}/layer2_progress.json\"\n",
    "    \n",
    "    # Resume support\n",
    "    processed_novels = set()\n",
    "    l2_pairs = 0\n",
    "    if os.path.exists(PROGRESS_L2):\n",
    "        with open(PROGRESS_L2, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "        processed_novels = set(progress.get('processed_ncodes', []))\n",
    "        l2_pairs = progress.get('pairs', 0)\n",
    "        print(f\"üìÇ Resuming: {len(processed_novels)} novels done, {l2_pairs:,} pairs\")\n",
    "    \n",
    "    l2_stats = {\n",
    "        'novels': len(processed_novels),\n",
    "        'chapters': 0,\n",
    "        'sentences': 0,\n",
    "        'dialogues': 0,\n",
    "        'pairs': l2_pairs,\n",
    "        'errors': 0,\n",
    "    }\n",
    "    \n",
    "    mode = 'a' if l2_pairs > 0 else 'w'\n",
    "    \n",
    "    with open(OUTPUT_L2, mode, encoding='utf-8') as out_f:\n",
    "        pbar = tqdm(unique_novels, desc=\"üí¨ Nar≈ç Novels\")\n",
    "        \n",
    "        for novel in pbar:\n",
    "            ncode = novel.get('ncode', '')\n",
    "            if not ncode or ncode in processed_novels:\n",
    "                continue\n",
    "            \n",
    "            title = novel.get('title', 'unknown')\n",
    "            total_ch = novel.get('general_all_no', 0)\n",
    "            max_ch = min(total_ch, 200)  # Up to 200 chapters per novel (novels are huge)\n",
    "            \n",
    "            for ch in range(1, max_ch + 1):\n",
    "                chapter_text = scrape_chapter(ncode, ch)\n",
    "                if not chapter_text:\n",
    "                    continue\n",
    "                \n",
    "                l2_stats['chapters'] += 1\n",
    "                sentences = extract_sentences(chapter_text)\n",
    "                prev = ''\n",
    "                \n",
    "                for sentence, is_dialogue in sentences:\n",
    "                    l2_stats['sentences'] += 1\n",
    "                    if is_dialogue:\n",
    "                        l2_stats['dialogues'] += 1\n",
    "                    \n",
    "                    try:\n",
    "                        pairs = generate_pairs(sentence, prev, ENABLE_SLICING)\n",
    "                        for p in pairs:\n",
    "                            p['source'] = 'narou'  # Tag source\n",
    "                            out_f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n",
    "                        l2_stats['pairs'] += len(pairs)\n",
    "                        prev = sentence\n",
    "                    except:\n",
    "                        l2_stats['errors'] += 1\n",
    "                \n",
    "                # Rate limit: 1 request per second\n",
    "                time.sleep(1)\n",
    "            \n",
    "            # Mark novel as done\n",
    "            processed_novels.add(ncode)\n",
    "            l2_stats['novels'] += 1\n",
    "            pbar.set_postfix(pairs=f\"{l2_stats['pairs']:,}\")\n",
    "            \n",
    "            # Save progress per novel\n",
    "            with open(PROGRESS_L2, 'w') as pf:\n",
    "                json.dump({\n",
    "                    'processed_ncodes': list(processed_novels),\n",
    "                    'pairs': l2_stats['pairs'],\n",
    "                }, pf)\n",
    "            out_f.flush()\n",
    "            \n",
    "            # Stop if target reached\n",
    "            if l2_stats['pairs'] >= LAYER_2_TARGET:\n",
    "                print(f\"\\n  ‚úì Reached {LAYER_2_TARGET:,} target\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Layer 2: {l2_stats['pairs']:,} pairs\")\n",
    "    print(f\"     Novels: {l2_stats['novels']} | Chapters: {l2_stats['chapters']:,}\")\n",
    "    print(f\"     Dialogues: {l2_stats['dialogues']:,}\")\n",
    "    print(f\"  üíæ Saved: {OUTPUT_L2}\")\n",
    "    print(f\"     Size: {os.path.getsize(OUTPUT_L2) / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    l2_stats = {'pairs': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî• Layer 3: NHK News RSS ‚Äî Freshness 2026 (~2M pairs)\n",
    "\n",
    "Scrape NHK News RSS feeds for modern, natural Japanese.\n",
    "\n",
    "Focus: **Daily conversation & grammar** (not technical jargon).\n",
    "NHK uses clean, standard Japanese perfect for IME training.\n",
    "\n",
    "Sources:\n",
    "- NHK News (general/main)\n",
    "- NHK Society (social/daily life)\n",
    "- NHK Science (science/education)\n",
    "- NHK Life (lifestyle/culture)\n",
    "- NHK Entertainment (culture/sports)\n",
    "- NHK Business (economy/daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LAYER_3:\n",
    "    import feedparser\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import time\n",
    "    \n",
    "    # ==========================================================\n",
    "    # RSS Feed Sources (NHK ‚Äî clean daily Japanese)\n",
    "    # Focus: daily conversation & grammar, NOT technical jargon\n",
    "    # ==========================================================\n",
    "    RSS_FEEDS = [\n",
    "        # NHK = clean, standard Japanese, perfect for IME\n",
    "        (\"NHK Main\",          \"https://www3.nhk.or.jp/rss/news/cat0.xml\"),\n",
    "        (\"NHK Society\",       \"https://www3.nhk.or.jp/rss/news/cat1.xml\"),\n",
    "        (\"NHK Science\",       \"https://www3.nhk.or.jp/rss/news/cat3.xml\"),\n",
    "        (\"NHK Life\",          \"https://www3.nhk.or.jp/rss/news/cat6.xml\"),\n",
    "        (\"NHK Entertainment\", \"https://www3.nhk.or.jp/rss/news/cat4.xml\"),\n",
    "        (\"NHK Business\",      \"https://www3.nhk.or.jp/rss/news/cat5.xml\"),\n",
    "        (\"NHK Sports\",        \"https://www3.nhk.or.jp/rss/news/cat2.xml\"),\n",
    "        (\"NHK Local\",         \"https://www3.nhk.or.jp/rss/news/cat7.xml\"),\n",
    "    ]\n",
    "    \n",
    "    def fetch_article_text(url):\n",
    "        \"\"\"Fetch and extract body text from a news article URL.\"\"\"\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=15, headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (IME Dataset Research)\"\n",
    "            })\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            \n",
    "            # Remove script, style, nav elements\n",
    "            for tag in soup.find_all([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # Try common article body selectors\n",
    "            body = (\n",
    "                soup.find(\"article\") or\n",
    "                soup.find(\"div\", class_=re.compile(r\"article|entry|content|body\", re.I)) or\n",
    "                soup.find(\"div\", id=re.compile(r\"article|entry|content|body\", re.I))\n",
    "            )\n",
    "            \n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                text = \"\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            else:\n",
    "                text = soup.get_text(separator=\"\n\", strip=True)\n",
    "            \n",
    "            # Only return if there is substantial Japanese text\n",
    "            if len(text) > 50 and len(JAPANESE_RE.findall(text)) > 20:\n",
    "                return text\n",
    "            return \"\"\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    \n",
    "    # Collect article URLs from all feeds\n",
    "    print(\"üì• Layer 3: Fetching NHK RSS feeds...\")\n",
    "    all_articles = []\n",
    "    \n",
    "    for name, url in RSS_FEEDS:\n",
    "        try:\n",
    "            feed = feedparser.parse(url)\n",
    "            entries = feed.entries[:100]  # Max 100 per feed\n",
    "            for entry in entries:\n",
    "                link = entry.get(\"link\", \"\")\n",
    "                title = entry.get(\"title\", \"\")\n",
    "                summary = entry.get(\"summary\", \"\")\n",
    "                if link:\n",
    "                    all_articles.append({\n",
    "                        \"url\": link,\n",
    "                        \"title\": title,\n",
    "                        \"summary\": summary,\n",
    "                        \"source\": name\n",
    "                    })\n",
    "            print(f\"  ‚úì {name}: {len(entries)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† {name}: {e}\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    seen_urls = set()\n",
    "    unique_articles = []\n",
    "    for a in all_articles:\n",
    "        if a[\"url\"] not in seen_urls:\n",
    "            seen_urls.add(a[\"url\"])\n",
    "            unique_articles.append(a)\n",
    "    \n",
    "    unique_articles = unique_articles[:LAYER_3_MAX_ARTICLES]\n",
    "    print(f\"  ‚úì Total unique articles: {len(unique_articles):,}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Layer 3 skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LAYER_3:\n",
    "    OUTPUT_L3 = f\"{OUTPUT_DIR}/layer3_rss.jsonl\"\n",
    "    PROGRESS_L3 = f\"{OUTPUT_DIR}/layer3_progress.json\"\n",
    "    \n",
    "    # Resume support\n",
    "    processed_urls = set()\n",
    "    l3_pairs = 0\n",
    "    if os.path.exists(PROGRESS_L3):\n",
    "        with open(PROGRESS_L3, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "        processed_urls = set(progress.get('processed_urls', []))\n",
    "        l3_pairs = progress.get('pairs', 0)\n",
    "        print(f\"üìÇ Resuming: {len(processed_urls)} articles done, {l3_pairs:,} pairs\")\n",
    "    \n",
    "    l3_stats = {\n",
    "        'articles': len(processed_urls),\n",
    "        'sentences': 0,\n",
    "        'pairs': l3_pairs,\n",
    "        'errors': 0,\n",
    "        'empty_articles': 0,\n",
    "    }\n",
    "    \n",
    "    mode = 'a' if l3_pairs > 0 else 'w'\n",
    "    \n",
    "    with open(OUTPUT_L3, mode, encoding='utf-8') as out_f:\n",
    "        pbar = tqdm(unique_articles, desc=\"üî• RSS Articles\")\n",
    "        \n",
    "        for article in pbar:\n",
    "            if article['url'] in processed_urls:\n",
    "                continue\n",
    "            \n",
    "            # First use title + summary (free, no scraping needed)\n",
    "            text_parts = []\n",
    "            if article.get('title'):\n",
    "                text_parts.append(article['title'])\n",
    "            if article.get('summary'):\n",
    "                # Clean HTML from summary\n",
    "                summary_soup = BeautifulSoup(article['summary'], 'html.parser')\n",
    "                text_parts.append(summary_soup.get_text(strip=True))\n",
    "            \n",
    "            # Then try to fetch full article\n",
    "            full_text = fetch_article_text(article['url'])\n",
    "            if full_text:\n",
    "                text_parts.append(full_text)\n",
    "            else:\n",
    "                l3_stats['empty_articles'] += 1\n",
    "            \n",
    "            combined = '\\n'.join(text_parts)\n",
    "            sentences = extract_sentences(combined)\n",
    "            prev = ''\n",
    "            \n",
    "            for sentence, is_dialogue in sentences:\n",
    "                l3_stats['sentences'] += 1\n",
    "                try:\n",
    "                    pairs = generate_pairs(sentence, prev, ENABLE_SLICING)\n",
    "                    for p in pairs:\n",
    "                        p['source'] = 'rss'\n",
    "                        out_f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n",
    "                    l3_stats['pairs'] += len(pairs)\n",
    "                    prev = sentence\n",
    "                except:\n",
    "                    l3_stats['errors'] += 1\n",
    "            \n",
    "            processed_urls.add(article['url'])\n",
    "            l3_stats['articles'] += 1\n",
    "            pbar.set_postfix(pairs=f\"{l3_stats['pairs']:,}\")\n",
    "            \n",
    "            # Save progress every 50 articles\n",
    "            if l3_stats['articles'] % 50 == 0:\n",
    "                with open(PROGRESS_L3, 'w') as pf:\n",
    "                    json.dump({\n",
    "                        'processed_urls': list(processed_urls),\n",
    "                        'pairs': l3_stats['pairs'],\n",
    "                    }, pf)\n",
    "                out_f.flush()\n",
    "            \n",
    "            # Rate limit: polite scraping\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # Stop if target reached\n",
    "            if l3_stats['pairs'] >= LAYER_3_TARGET:\n",
    "                print(f\"\\n  ‚úì Reached {LAYER_3_TARGET:,} target\")\n",
    "                break\n",
    "    \n",
    "    # Final save\n",
    "    with open(PROGRESS_L3, 'w') as pf:\n",
    "        json.dump({\n",
    "            'processed_urls': list(processed_urls),\n",
    "            'pairs': l3_stats['pairs'],\n",
    "        }, pf)\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Layer 3: {l3_stats['pairs']:,} pairs from {l3_stats['articles']} articles\")\n",
    "    print(f\"     Sentences: {l3_stats['sentences']:,} | Empty articles: {l3_stats['empty_articles']}\")\n",
    "    print(f\"  üíæ Saved: {OUTPUT_L3}\")\n",
    "    if os.path.exists(OUTPUT_L3):\n",
    "        print(f\"     Size: {os.path.getsize(OUTPUT_L3) / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    l3_stats = {'pairs': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Merge All Layers ‚Üí Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "FINAL_FILE = f\"{OUTPUT_DIR}/ime_dataset_10m.jsonl\"\n",
    "\n",
    "layer_files = []\n",
    "if RUN_LAYER_1 and os.path.exists(f\"{OUTPUT_DIR}/layer1_base.jsonl\"):\n",
    "    layer_files.append(('üß± Base', f\"{OUTPUT_DIR}/layer1_base.jsonl\"))\n",
    "if RUN_LAYER_2 and os.path.exists(f\"{OUTPUT_DIR}/layer2_narou.jsonl\"):\n",
    "    layer_files.append(('üí¨ Nar≈ç', f\"{OUTPUT_DIR}/layer2_narou.jsonl\"))\n",
    "if RUN_LAYER_3 and os.path.exists(f\"{OUTPUT_DIR}/layer3_rss.jsonl\"):\n",
    "    layer_files.append(('üî• RSS', f\"{OUTPUT_DIR}/layer3_rss.jsonl\"))\n",
    "\n",
    "if not layer_files:\n",
    "    print(\"‚ùå No layer files found. Run at least one layer first.\")\n",
    "else:\n",
    "    print(\"üì¶ Merging all layers...\")\n",
    "    \n",
    "    # Count per layer\n",
    "    layer_counts = {}\n",
    "    total_lines = 0\n",
    "    \n",
    "    with open(FINAL_FILE, 'w', encoding='utf-8') as out_f:\n",
    "        for name, filepath in layer_files:\n",
    "            count = 0\n",
    "            with open(filepath, 'r', encoding='utf-8') as in_f:\n",
    "                for line in in_f:\n",
    "                    out_f.write(line)\n",
    "                    count += 1\n",
    "            layer_counts[name] = count\n",
    "            total_lines += count\n",
    "            print(f\"  {name}: {count:,} pairs\")\n",
    "    \n",
    "    file_size = os.path.getsize(FINAL_FILE)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä FINAL DATASET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Total pairs:  {total_lines:,}\")\n",
    "    print(f\"  File size:    {file_size / (1024**2):.1f} MB\")\n",
    "    print(f\"  File:         {FINAL_FILE}\")\n",
    "    print(f\"\")\n",
    "    for name, count in layer_counts.items():\n",
    "        pct = count / max(total_lines, 1) * 100\n",
    "        bar = '‚ñà' * int(pct / 2)\n",
    "        print(f\"  {name}: {count:>10,} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    target_pct = total_lines / TOTAL_TARGET * 100\n",
    "    print(f\"\\n  üéØ Target progress: {total_lines:,} / {TOTAL_TARGET:,} ({target_pct:.1f}%)\")\n",
    "    if total_lines < TOTAL_TARGET:\n",
    "        print(f\"     Need {TOTAL_TARGET - total_lines:,} more pairs.\")\n",
    "        print(f\"     Options: increase LAYER_1_RAW_LINES or LAYER_2_MAX_NOVELS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples from each layer\n",
    "print(\"\\nüìù Sample Pairs from Each Layer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, filepath in layer_files:\n",
    "    print(f\"\\n{name}:\")\n",
    "    samples = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100:\n",
    "                break\n",
    "            samples.append(json.loads(line))\n",
    "    \n",
    "    # Pick 5 random diverse samples\n",
    "    random.seed(42)\n",
    "    picks = random.sample(samples, min(5, len(samples)))\n",
    "    for s in picks:\n",
    "        ctx = s.get('left_context', '')[:12] or ''\n",
    "        print(f\"  {ctx}<SEP>{s['input'][:15]} ‚Üí {s['output'][:25]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality spot-check\n",
    "print(\"\\nüîç Quality Spot-Check (random 10):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "spot_samples = []\n",
    "with open(FINAL_FILE, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 200:\n",
    "            break\n",
    "        spot_samples.append(json.loads(line))\n",
    "\n",
    "random.seed(123)\n",
    "for s in random.sample(spot_samples, min(10, len(spot_samples))):\n",
    "    # Verify kana conversion\n",
    "    re_kana = text_to_kana(s['output'])\n",
    "    match = re_kana == s['input']\n",
    "    status = '‚úÖ' if match else '‚ö†Ô∏è'\n",
    "    print(f\"  {status} {s['input'][:20]} ‚Üí {s['output'][:25]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use in training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã HOW TO USE IN TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Option A: Use as standalone dataset\n",
    "  DATASET_FILE = \"{FINAL_FILE}\"\n",
    "\n",
    "Option B: Combine with filtered zenz\n",
    "  files = [\n",
    "      \"{OUTPUT_DIR}/../filtered_data/filtered_high_quality.jsonl\",\n",
    "      \"{FINAL_FILE}\",\n",
    "  ]\n",
    "\n",
    "Option C: Layered fine-tuning (RECOMMENDED)\n",
    "  Step 1: Train on OSCAR base (layer1) ‚Äî full epochs\n",
    "  Step 2: Continue on Nar≈ç (layer2) ‚Äî 3-5 epochs  \n",
    "  Step 3: Fine-tune on RSS (layer3) ‚Äî 2-3 epochs\n",
    "  This gives grammar + conversation + 2026 freshness!\n",
    "\n",
    "Individual layer files:\n",
    "  - {OUTPUT_DIR}/layer1_base.jsonl  (grammar base)\n",
    "  - {OUTPUT_DIR}/layer2_narou.jsonl  (dialogue/conversation)\n",
    "  - {OUTPUT_DIR}/layer3_rss.jsonl    (2026 vocabulary)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nüßπ Cleanup:\")\n",
    "print(\"  Delete progress files after verifying:\")\n",
    "for f in [\n",
    "    f\"{OUTPUT_DIR}/layer1_base.jsonl.progress\",\n",
    "    f\"{OUTPUT_DIR}/layer2_progress.json\",\n",
    "    f\"{OUTPUT_DIR}/layer3_progress.json\",\n",
    "]:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"    {f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Done! Dataset ready for training.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}