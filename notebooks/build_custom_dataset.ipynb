{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì¶ Build Custom Japanese IME Dataset\n",
                "\n",
                "Create a high-quality conversational dataset for keyboard IME training.\n",
                "\n",
                "**Pipeline:**\n",
                "1. Load text from free sources (CC-100, OSCAR, custom text)\n",
                "2. Extract conversational sentences (dialogue in `„Äå„Äç`, casual speech)\n",
                "3. Reverse-kana pipeline: `kanji text ‚Üí kana reading` via SudachiPy\n",
                "4. Data augmentation: slice into NWP + KKC training pairs\n",
                "5. Quality filter + save to Drive as JSONL\n",
                "\n",
                "**Output format (same as zenz):**\n",
                "```json\n",
                "{\"left_context\": \"Ââç„ÅÆÊñá\", \"input\": \"„Ç´„Éä\", \"output\": \"Êº¢Â≠ó\"}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect platform\n",
                "if 'COLAB_RELEASE_TAG' in os.environ:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Local'\n",
                "    DRIVE_DIR = './output'\n",
                "\n",
                "OUTPUT_DIR = f\"{DRIVE_DIR}/datasets\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Output: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SudachiPy with full dictionary (best for modern words + readings)\n",
                "!pip install -q datasets tqdm sudachipy sudachidict_full"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "\n",
                "# --- Data Source ---\n",
                "# Options: 'cc100', 'oscar', 'mc4', 'custom'\n",
                "# cc100: Cleaned web text (good grammar base)\n",
                "# oscar: Web crawl (diverse, noisy)\n",
                "# mc4:   Cleaned multilingual C4 (high quality)\n",
                "# custom: Your own text lines (paste or load from file)\n",
                "DATA_SOURCE = 'cc100'\n",
                "\n",
                "# --- Processing limits ---\n",
                "MAX_RAW_LINES = 500_000     # Raw lines to stream from source\n",
                "MAX_OUTPUT_PAIRS = 2_000_000 # Max training pairs to generate\n",
                "\n",
                "# --- Quality filters ---\n",
                "MIN_SENTENCE_LEN = 5    # Min chars per sentence\n",
                "MAX_SENTENCE_LEN = 120  # Max chars (skip long paragraphs)\n",
                "PREFER_DIALOGUE = True  # Boost sentences from „Äå„Äç dialogue\n",
                "\n",
                "# --- Augmentation ---\n",
                "# Create sliced pairs for NWP (Next Word Prediction)\n",
                "ENABLE_SLICING = True   # Slice sentences into sub-pairs\n",
                "MIN_SLICE_WORDS = 2     # Min words in a slice\n",
                "\n",
                "print(f\"Source: {DATA_SOURCE}\")\n",
                "print(f\"Max lines: {MAX_RAW_LINES:,}\")\n",
                "print(f\"Max output pairs: {MAX_OUTPUT_PAIRS:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Reverse-Kana Pipeline (SudachiPy)\n",
                "\n",
                "Converts kanji/mixed text ‚Üí katakana readings.\n",
                "\n",
                "`2026Âπ¥„ÅÆÊñ∞‰Ωú„Ç≤„Éº„É†„ÅØÊúÄÈ´ò„Å†„ÄÇ` ‚Üí `„Éã„Çª„É≥„Éã„Ç∏„É•„Ç¶„É≠„ÇØ„Éç„É≥„Éé„Ç∑„É≥„Çµ„ÇØ„Ç≤„Éº„É†„Éè„Çµ„Ç§„Ç≥„Ç¶„ÉÄ„ÄÇ`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sudachipy import tokenizer as sudachi_tokenizer, dictionary as sudachi_dictionary\n",
                "import re\n",
                "import json\n",
                "\n",
                "# Setup SudachiPy with FULL dictionary\n",
                "# Mode C = longest possible splits (best for IME: treats compound words as single unit)\n",
                "sudachi_dict = sudachi_dictionary.Dictionary(dict=\"full\")\n",
                "sudachi = sudachi_dict.create()\n",
                "SPLIT_MODE = sudachi_tokenizer.Tokenizer.SplitMode.C\n",
                "\n",
                "\n",
                "def text_to_morphemes(text):\n",
                "    \"\"\"Analyze text into morphemes with readings.\n",
                "    Returns: [(surface, reading, pos), ...]\n",
                "    \"\"\"\n",
                "    tokens = sudachi.tokenize(text, SPLIT_MODE)\n",
                "    result = []\n",
                "    for t in tokens:\n",
                "        surface = t.surface()\n",
                "        reading = t.reading_form()  # Katakana reading\n",
                "        pos = t.part_of_speech()[0]  # Main POS\n",
                "        \n",
                "        # If reading is empty or same as surface (unknown word),\n",
                "        # keep the surface as-is\n",
                "        if not reading:\n",
                "            reading = surface\n",
                "        \n",
                "        result.append((surface, reading, pos))\n",
                "    return result\n",
                "\n",
                "\n",
                "def text_to_kana(text):\n",
                "    \"\"\"Convert text to full katakana reading.\"\"\"\n",
                "    morphemes = text_to_morphemes(text)\n",
                "    return ''.join(reading for _, reading, _ in morphemes)\n",
                "\n",
                "\n",
                "def generate_full_pair(text, left_context=''):\n",
                "    \"\"\"Generate a full sentence training pair.\n",
                "    Returns: {left_context, input (kana), output (kanji/mixed)}\n",
                "    \"\"\"\n",
                "    kana = text_to_kana(text)\n",
                "    if not kana or kana == text:  # No conversion needed (already kana)\n",
                "        return None\n",
                "    \n",
                "    return {\n",
                "        'left_context': left_context,\n",
                "        'input': kana,\n",
                "        'output': text\n",
                "    }\n",
                "\n",
                "\n",
                "def generate_sliced_pairs(text, left_context=''):\n",
                "    \"\"\"Generate multiple sliced training pairs from one sentence.\n",
                "    \n",
                "    This teaches \"Next Word Prediction\" by creating pairs at each word boundary.\n",
                "    \n",
                "    Example: \"2026Âπ¥„ÅÆÊñ∞‰Ωú„Ç≤„Éº„É†„ÅØÊúÄÈ´ò„Å†„ÄÇ\"\n",
                "    Slice 1: ctx=\"\"        input=\"„Éã„Çª„É≥„Éã„Ç∏„É•„Ç¶„É≠„ÇØ„Éç„É≥\" output=\"2026Âπ¥\"\n",
                "    Slice 2: ctx=\"2026Âπ¥\"  input=\"„Éé„Ç∑„É≥„Çµ„ÇØ\"           output=\"„ÅÆÊñ∞‰Ωú\"\n",
                "    Slice 3: ctx=\"2026Âπ¥„ÅÆ\" input=\"„Ç∑„É≥„Çµ„ÇØ„Ç≤„Éº„É†\"       output=\"Êñ∞‰Ωú„Ç≤„Éº„É†\"\n",
                "    ...\n",
                "    \"\"\"\n",
                "    morphemes = text_to_morphemes(text)\n",
                "    if len(morphemes) < MIN_SLICE_WORDS:\n",
                "        return []\n",
                "    \n",
                "    pairs = []\n",
                "    \n",
                "    # Full sentence pair\n",
                "    full_kana = ''.join(r for _, r, _ in morphemes)\n",
                "    full_text = ''.join(s for s, _, _ in morphemes)\n",
                "    if full_kana != full_text:\n",
                "        pairs.append({\n",
                "            'left_context': left_context,\n",
                "            'input': full_kana,\n",
                "            'output': full_text\n",
                "        })\n",
                "    \n",
                "    # Sliced pairs at word boundaries\n",
                "    # Slide through morphemes, creating pairs every 1-3 words\n",
                "    for slice_size in [1, 2, 3]:\n",
                "        for i in range(0, len(morphemes) - slice_size + 1):\n",
                "            # Context = everything before this slice\n",
                "            ctx_parts = [s for s, _, _ in morphemes[:i]]\n",
                "            ctx = left_context + ''.join(ctx_parts)\n",
                "            \n",
                "            # Slice = current words\n",
                "            slice_morphemes = morphemes[i:i + slice_size]\n",
                "            slice_kana = ''.join(r for _, r, _ in slice_morphemes)\n",
                "            slice_text = ''.join(s for s, _, _ in slice_morphemes)\n",
                "            \n",
                "            # Skip if kana == text (no conversion, e.g. pure katakana word)\n",
                "            if slice_kana == slice_text:\n",
                "                continue\n",
                "            \n",
                "            # Skip very short or punctuation-only\n",
                "            if len(slice_text) < 1:\n",
                "                continue\n",
                "            if re.match(r'^[„ÄÅ„ÄÇÔºüÔºÅ\\s]+$', slice_text):\n",
                "                continue\n",
                "            \n",
                "            pairs.append({\n",
                "                'left_context': ctx[-60:] if len(ctx) > 60 else ctx,  # Trim context\n",
                "                'input': slice_kana,\n",
                "                'output': slice_text\n",
                "            })\n",
                "    \n",
                "    return pairs\n",
                "\n",
                "\n",
                "# Quick test\n",
                "print(\"üß™ Reverse-Kana Pipeline Test:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "test_texts = [\n",
                "    '2026Âπ¥„ÅÆÊñ∞‰Ωú„Ç≤„Éº„É†„ÅØÊúÄÈ´ò„Å†„ÄÇ',\n",
                "    '„Éû„Ç∏„ÅßÔºü„Åù„Çå„Å£„Å¶„É§„Éê„Åè„Å™„ÅÑÔºü',\n",
                "    'Êñ∞„Åó„ÅÑiPhone„ÅÆ„Ç´„É°„É©„ÄÅ„Åô„Åî„Åè„Å™„ÅÑÔºü',\n",
                "    '‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇÊöë„ÅÑ„Åß„Åô„Å≠„ÄÇ',\n",
                "    'ÂèãÈÅî„Å®‰∏ÄÁ∑í„Å´Êò†Áîª„ÇíË¶ã„Å´Ë°å„Åç„Åæ„Åó„Åü„ÄÇ',\n",
                "]\n",
                "\n",
                "for text in test_texts:\n",
                "    kana = text_to_kana(text)\n",
                "    morphemes = text_to_morphemes(text)\n",
                "    print(f\"\\n  Text: {text}\")\n",
                "    print(f\"  Kana: {kana}\")\n",
                "    print(f\"  Morphemes: {[(s, r) for s, r, _ in morphemes[:6]]}{'...' if len(morphemes) > 6 else ''}\")\n",
                "    \n",
                "    if ENABLE_SLICING:\n",
                "        slices = generate_sliced_pairs(text)\n",
                "        print(f\"  Slices ({len(slices)}):\")\n",
                "        for p in slices[:3]:\n",
                "            ctx = p['left_context'][:10] or ''\n",
                "            print(f\"    ctx={ctx} | {p['input'][:15]} ‚Üí {p['output'][:15]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Extraction & Filtering\n",
                "\n",
                "Extract clean conversational sentences from raw text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================================\n",
                "# Sentence extraction + quality filters\n",
                "# ==========================================================\n",
                "\n",
                "# Kill patterns (same as filter_dataset_quality.ipynb)\n",
                "KILL_PATTERNS = re.compile(\n",
                "    r'(ID:|„Ç¢„Ç§„Éá„Ç£:|ÊäïÁ®øÊó•|ÂêçÂâç:|ÂêçÁÑ°„Åó|'\n",
                "    r'\\d{2}:\\d{2}:\\d{2}|'              # Timestamps\n",
                "    r'(http|www\\.|https)|'              # URLs\n",
                "    r'ISBN|ISSN|'\n",
                "    r'„Ç®„Ç§„ÉÅ„ÉÜ„Ç£„Éº„ÉÜ„Ç£„Éº„Éî„Éº|'\n",
                "    r'„ÉÄ„Éñ„É™„É•„ÉÄ„Éñ„É™„É•)',\n",
                "    re.IGNORECASE\n",
                ")\n",
                "\n",
                "# Garbage characters (Cyrillic, extended Latin, etc.)\n",
                "GARBAGE_RE = re.compile(r'[\\u0400-\\u04FF\\u0100-\\u024F]')\n",
                "\n",
                "# Encyclopedia patterns\n",
                "ENCYCLOPEDIA_RE = re.compile(\n",
                "    r'(„Å´‰ΩçÁΩÆ„Åô„Çã|„Å´ÊâÄÂú®„Åô„Çã|ÂåóÁ∑Ø\\d|ÂçóÁ∑Ø\\d|Êù±Áµå\\d|Ë•øÁµå\\d|'\n",
                "    r'Ê®ôÈ´ò\\d|Êµ∑Êäú\\d|Â≠¶Âêç|ÂàÜÈ°ûÂ≠¶|Á¨¨\\d+‰ª£|Á¥ÄÂÖÉÂâç)'\n",
                ")\n",
                "\n",
                "# Japanese text detection\n",
                "JAPANESE_RE = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]')\n",
                "\n",
                "# Dialogue brackets\n",
                "DIALOGUE_RE = re.compile(r'„Äå([^„Äç]{2,80})„Äç')\n",
                "\n",
                "\n",
                "def extract_sentences(text):\n",
                "    \"\"\"Extract clean sentences from a text block.\n",
                "    Returns list of (sentence, is_dialogue) tuples.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    # Quick reject: no Japanese at all\n",
                "    if not JAPANESE_RE.search(text):\n",
                "        return []\n",
                "    \n",
                "    # Extract dialogue first (highest quality conversational text)\n",
                "    dialogues = DIALOGUE_RE.findall(text)\n",
                "    for d in dialogues:\n",
                "        d = d.strip()\n",
                "        if len(d) >= MIN_SENTENCE_LEN and len(d) <= MAX_SENTENCE_LEN:\n",
                "            if not KILL_PATTERNS.search(d) and not GARBAGE_RE.search(d):\n",
                "                results.append((d, True))  # is_dialogue = True\n",
                "    \n",
                "    # Split by sentence endings for non-dialogue\n",
                "    sentences = re.split(r'[„ÄÇÔºÅÔºü\\n]+', text)\n",
                "    for s in sentences:\n",
                "        s = s.strip()\n",
                "        if not s:\n",
                "            continue\n",
                "        if len(s) < MIN_SENTENCE_LEN or len(s) > MAX_SENTENCE_LEN:\n",
                "            continue\n",
                "        \n",
                "        # Kill filters\n",
                "        if KILL_PATTERNS.search(s):\n",
                "            continue\n",
                "        if GARBAGE_RE.search(s):\n",
                "            continue\n",
                "        if ENCYCLOPEDIA_RE.search(s):\n",
                "            continue\n",
                "        \n",
                "        # Must have enough Japanese chars (not mostly ASCII/numbers)\n",
                "        jp_ratio = len(JAPANESE_RE.findall(s)) / max(len(s), 1)\n",
                "        if jp_ratio < 0.3:\n",
                "            continue\n",
                "        \n",
                "        # Too many numbers = stats/data table\n",
                "        num_count = len(re.findall(r'\\d+', s))\n",
                "        if num_count >= 3:\n",
                "            continue\n",
                "        \n",
                "        results.append((s, False))\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "# Quick test\n",
                "print(\"üß™ Sentence Extraction Test:\")\n",
                "test_block = \"\"\"Áî∞‰∏≠„Åï„Çì„ÅØ„Äå„Éû„Ç∏„ÅßÔºü„Åù„Çå„Å£„Å¶„É§„Éê„Åè„Å™„ÅÑÔºü„Äç„Å®Ë®Ä„Å£„Åü„ÄÇ\n",
                "ÂåóÁ∑Ø35Â∫¶„Å´‰ΩçÁΩÆ„Åô„ÇãÈÉΩÂ∏Ç„Åß„ÄÅ‰∫∫Âè£„ÅØÁ¥Ñ3500‰∏á‰∫∫„Åß„ÅÇ„Çã„ÄÇ\n",
                "Êñ∞„Åó„ÅÑiPhone„ÅÆ„Ç´„É°„É©„Åå„Åô„Åî„ÅÑ„ÄÇÂèãÈÅî„ÇÇË≤∑„Å£„Åü„Çâ„Åó„ÅÑ„ÄÇ\n",
                "„Äå‰ªäÊó•„ÅØÂ§©Ê∞ó„Åå„ÅÑ„ÅÑ„Åã„ÇâÊï£Ê≠©„Å´Ë°å„Åì„ÅÜ„Äç„Å®ÊØç„ÅåË®Ä„Å£„Åü„ÄÇ\n",
                "2023Âπ¥3Êúà15Êó•„ÅÆhttp://example.com„Å´„Çà„Çã„Å®„ÄÇ\n",
                "„Éá„Éê„ÉÉ„Ç∞„ÅÆ„Åü„ÇÅ„Å´app„Çí„Ç¢„ÉÉ„Éó„Éá„Éº„Éà„Åó„Åæ„Åó„Åü„ÄÇ\"\"\"\n",
                "\n",
                "extracted = extract_sentences(test_block)\n",
                "for sent, is_dialog in extracted:\n",
                "    tag = 'üí¨' if is_dialog else 'üìù'\n",
                "    print(f\"  {tag} {sent[:50]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Data Source\n",
                "\n",
                "Free sources available:\n",
                "- `cc100`: Japanese web text (grammar base, ~70GB streamed)\n",
                "- `oscar`: Web crawl (diverse)\n",
                "- `mc4`: Cleaned C4 (high quality)\n",
                "- `custom`: Your own text lines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "def load_data_source(source, max_lines):\n",
                "    \"\"\"Load raw text lines from various free sources.\n",
                "    Uses streaming to avoid downloading entire dataset.\n",
                "    \"\"\"\n",
                "    lines = []\n",
                "    \n",
                "    if source == 'cc100':\n",
                "        print(\"üì• Loading CC-100 Japanese (streaming)...\")\n",
                "        ds = load_dataset('cc100', lang='ja', split='train', streaming=True)\n",
                "        text_key = 'text'\n",
                "        \n",
                "    elif source == 'oscar':\n",
                "        print(\"üì• Loading OSCAR Japanese (streaming)...\")\n",
                "        ds = load_dataset('oscar-corpus/OSCAR-2301', 'ja',\n",
                "                          split='train', streaming=True,\n",
                "                          trust_remote_code=True)\n",
                "        text_key = 'text'\n",
                "        \n",
                "    elif source == 'mc4':\n",
                "        print(\"üì• Loading mC4 Japanese (streaming)...\")\n",
                "        ds = load_dataset('mc4', 'ja', split='train', streaming=True)\n",
                "        text_key = 'text'\n",
                "    \n",
                "    elif source == 'custom':\n",
                "        print(\"üìù Using custom text lines.\")\n",
                "        print(\"   Set CUSTOM_TEXTS list or load from file.\")\n",
                "        return CUSTOM_TEXTS if 'CUSTOM_TEXTS' in dir() else []\n",
                "    \n",
                "    else:\n",
                "        raise ValueError(f\"Unknown source: {source}\")\n",
                "    \n",
                "    # Stream and collect\n",
                "    for item in tqdm(ds, desc=f\"Loading {source}\", total=max_lines):\n",
                "        text = item.get(text_key, '')\n",
                "        if text and len(text) >= MIN_SENTENCE_LEN:\n",
                "            lines.append(text)\n",
                "        if len(lines) >= max_lines:\n",
                "            break\n",
                "    \n",
                "    print(f\"‚úì Loaded {len(lines):,} text blocks from {source}\")\n",
                "    return lines\n",
                "\n",
                "\n",
                "# --- Optional: Custom text for testing ---\n",
                "# Uncomment and add your own lines here:\n",
                "# CUSTOM_TEXTS = [\n",
                "#     'Êñ∞„Åó„ÅÑiPhone„ÅÆ„Ç´„É°„É©„Åå„Åô„Åî„ÅÑ„ÄÇÂèãÈÅî„ÇÇË≤∑„Å£„Åü„Çâ„Åó„ÅÑ„ÄÇ',\n",
                "#     '„Äå„Éû„Ç∏„ÅßÔºü„Åù„Çå„Å£„Å¶„É§„Éê„Åè„Å™„ÅÑÔºü„Äç„Å®Ë®Ä„Å£„Åü„ÄÇ',\n",
                "#     '‰ªäÊó•„ÅØÂ§©Ê∞ó„Åå„ÅÑ„ÅÑ„Åã„ÇâÊï£Ê≠©„Å´Ë°å„Åì„ÅÜ„Åã„Å™„ÄÇ',\n",
                "# ]\n",
                "# DATA_SOURCE = 'custom'\n",
                "\n",
                "raw_lines = load_data_source(DATA_SOURCE, MAX_RAW_LINES)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Process: Extract ‚Üí Kana ‚Üí Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Main processing pipeline\n",
                "# ============================================================\n",
                "\n",
                "OUTPUT_FILE = f\"{OUTPUT_DIR}/custom_ime_dataset.jsonl\"\n",
                "PROGRESS_FILE = f\"{OUTPUT_DIR}/build_progress.json\"\n",
                "\n",
                "# Stats\n",
                "stats = {\n",
                "    'raw_lines': len(raw_lines),\n",
                "    'sentences_extracted': 0,\n",
                "    'dialogue_count': 0,\n",
                "    'pairs_generated': 0,\n",
                "    'slices_generated': 0,\n",
                "    'errors': 0,\n",
                "}\n",
                "\n",
                "# Resume support\n",
                "start_from = 0\n",
                "if os.path.exists(PROGRESS_FILE):\n",
                "    with open(PROGRESS_FILE, 'r') as f:\n",
                "        progress = json.load(f)\n",
                "    start_from = progress.get('processed_lines', 0)\n",
                "    stats.update(progress.get('stats', {}))\n",
                "    print(f\"üìÇ Resuming from line {start_from:,}\")\n",
                "\n",
                "# Previous sentence for left_context (consecutive sentences in same block)\n",
                "prev_sentence = ''\n",
                "\n",
                "mode = 'a' if start_from > 0 else 'w'\n",
                "with open(OUTPUT_FILE, mode, encoding='utf-8') as out_f:\n",
                "    for line_idx in tqdm(range(start_from, len(raw_lines)), desc=\"Processing\"):\n",
                "        text_block = raw_lines[line_idx]\n",
                "        \n",
                "        # Extract sentences from this block\n",
                "        sentences = extract_sentences(text_block)\n",
                "        prev_sentence = ''  # Reset context between blocks\n",
                "        \n",
                "        for sentence, is_dialogue in sentences:\n",
                "            stats['sentences_extracted'] += 1\n",
                "            if is_dialogue:\n",
                "                stats['dialogue_count'] += 1\n",
                "            \n",
                "            try:\n",
                "                if ENABLE_SLICING:\n",
                "                    # Generate multiple sliced pairs\n",
                "                    pairs = generate_sliced_pairs(sentence, left_context=prev_sentence)\n",
                "                    for p in pairs:\n",
                "                        out_f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n",
                "                    stats['slices_generated'] += len(pairs)\n",
                "                    stats['pairs_generated'] += len(pairs)\n",
                "                else:\n",
                "                    # Generate single full pair\n",
                "                    pair = generate_full_pair(sentence, left_context=prev_sentence)\n",
                "                    if pair:\n",
                "                        out_f.write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
                "                        stats['pairs_generated'] += 1\n",
                "                \n",
                "                # Use this sentence as context for next one\n",
                "                prev_sentence = sentence\n",
                "                \n",
                "            except Exception as e:\n",
                "                stats['errors'] += 1\n",
                "                if stats['errors'] <= 5:\n",
                "                    print(f\"  ‚ö† Error: {e} | text: {sentence[:30]}\")\n",
                "        \n",
                "        # Save progress every 10K lines\n",
                "        if (line_idx - start_from) % 10_000 == 0 and line_idx > start_from:\n",
                "            with open(PROGRESS_FILE, 'w') as pf:\n",
                "                json.dump({'processed_lines': line_idx, 'stats': stats}, pf)\n",
                "            out_f.flush()\n",
                "        \n",
                "        # Stop if we have enough pairs\n",
                "        if stats['pairs_generated'] >= MAX_OUTPUT_PAIRS:\n",
                "            print(f\"\\n‚úì Reached {MAX_OUTPUT_PAIRS:,} pairs limit.\")\n",
                "            break\n",
                "\n",
                "# Final save\n",
                "with open(PROGRESS_FILE, 'w') as pf:\n",
                "    json.dump({'processed_lines': line_idx + 1, 'stats': stats}, pf)\n",
                "\n",
                "print(f\"\\n‚úì Done!\")\n",
                "print(f\"  Raw lines processed: {line_idx - start_from + 1:,}\")\n",
                "print(f\"  Sentences extracted: {stats['sentences_extracted']:,}\")\n",
                "print(f\"  Dialogues („Äå„Äç):    {stats['dialogue_count']:,}\")\n",
                "print(f\"  Training pairs:     {stats['pairs_generated']:,}\")\n",
                "print(f\"  Errors:             {stats['errors']:,}\")\n",
                "print(f\"üíæ Saved: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results & Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count final output\n",
                "total_pairs = 0\n",
                "samples = []\n",
                "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
                "    for line in f:\n",
                "        total_pairs += 1\n",
                "        if len(samples) < 50:\n",
                "            samples.append(json.loads(line))\n",
                "\n",
                "file_size = os.path.getsize(OUTPUT_FILE)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìä DATASET BUILD REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(f\"  Source:           {DATA_SOURCE}\")\n",
                "print(f\"  Raw lines:        {stats['raw_lines']:,}\")\n",
                "print(f\"  Sentences found:  {stats['sentences_extracted']:,}\")\n",
                "print(f\"  Dialogues („Äå„Äç): {stats['dialogue_count']:,}\")\n",
                "print(f\"  Training pairs:   {total_pairs:,}\")\n",
                "print(f\"  File size:        {file_size / (1024*1024):.1f} MB\")\n",
                "print(f\"  File:             {OUTPUT_FILE}\")\n",
                "\n",
                "# Pair type breakdown\n",
                "has_context = sum(1 for s in samples if s.get('left_context'))\n",
                "avg_input_len = sum(len(s['input']) for s in samples) / max(len(samples), 1)\n",
                "avg_output_len = sum(len(s['output']) for s in samples) / max(len(samples), 1)\n",
                "\n",
                "print(f\"\\nüìà Sample Stats (first {len(samples)} pairs):\")\n",
                "print(f\"  With context:     {has_context}/{len(samples)} ({has_context/max(len(samples),1)*100:.0f}%)\")\n",
                "print(f\"  Avg input len:    {avg_input_len:.1f} chars (kana)\")\n",
                "print(f\"  Avg output len:   {avg_output_len:.1f} chars (kanji)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show sample pairs\n",
                "print(\"\\nüìù Sample Training Pairs:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Show dialogue pairs first\n",
                "print(\"\\nüí¨ Dialogue-origin pairs:\")\n",
                "shown = 0\n",
                "for s in samples:\n",
                "    if shown >= 5: break\n",
                "    ctx = s['left_context'][:15] or ''\n",
                "    print(f\"  {ctx}<SEP>{s['input'][:20]} ‚Üí {s['output'][:25]}\")\n",
                "    shown += 1\n",
                "\n",
                "# Show pairs with context\n",
                "print(\"\\nüìù Pairs with context:\")\n",
                "shown = 0\n",
                "for s in samples:\n",
                "    if not s['left_context']: continue\n",
                "    if shown >= 5: break\n",
                "    ctx = s['left_context'][:15]\n",
                "    print(f\"  ctx={ctx} | {s['input'][:15]} ‚Üí {s['output'][:20]}\")\n",
                "    shown += 1\n",
                "\n",
                "# Show short pairs (single word conversions)\n",
                "print(\"\\nüî§ Single-word conversions:\")\n",
                "shown = 0\n",
                "for s in samples:\n",
                "    if len(s['output']) > 5: continue\n",
                "    if shown >= 5: break\n",
                "    ctx = s['left_context'][:15] or ''\n",
                "    print(f\"  {ctx}<SEP>{s['input']} ‚Üí {s['output']}\")\n",
                "    shown += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quality spot-check: verify kana‚Üíkanji conversions are correct\n",
                "import random\n",
                "\n",
                "print(\"\\nüîç Quality Spot-Check (random 10 pairs):\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "random.seed(42)\n",
                "check_samples = random.sample(samples, min(10, len(samples)))\n",
                "\n",
                "for s in check_samples:\n",
                "    # Verify: re-convert output to kana and compare with input\n",
                "    expected_kana = text_to_kana(s['output'])\n",
                "    match = expected_kana == s['input']\n",
                "    status = '‚úÖ' if match else '‚ö†Ô∏è'\n",
                "    \n",
                "    print(f\"  {status} {s['input'][:20]} ‚Üí {s['output'][:25]}\")\n",
                "    if not match:\n",
                "        print(f\"       Re-check: {expected_kana[:20]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# How to use in training notebooks\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìã HOW TO USE IN TRAINING\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\"\"\n",
                "Option A: Use ALONE (fresh data only)\n",
                "  dataset_file = \"{OUTPUT_FILE}\"\n",
                "\n",
                "Option B: COMBINE with filtered zenz (recommended)\n",
                "  files = [\n",
                "      \"{OUTPUT_DIR}/filtered_high_quality.jsonl\",  # Filtered zenz\n",
                "      \"{OUTPUT_FILE}\",  # Fresh custom data\n",
                "  ]\n",
                "  dataset = []\n",
                "  for f in files:\n",
                "      with open(f, 'r') as fh:\n",
                "          for line in fh:\n",
                "              dataset.append(json.loads(line))\n",
                "\n",
                "Option C: FINE-TUNE strategy\n",
                "  1. Train on filtered zenz (base grammar) ‚Äî full epochs\n",
                "  2. Fine-tune last 2-3 epochs on fresh data (modern vocab)\n",
                "  This makes \"fresh\" vocabulary more likely in predictions.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "print(\"\\nüßπ Cleanup options:\")\n",
                "print(f\"  Progress file: {PROGRESS_FILE}\")\n",
                "print(f\"  Delete after verifying: os.remove('{PROGRESS_FILE}')\")\n",
                "\n",
                "# Memory cleanup\n",
                "del raw_lines\n",
                "import gc; gc.collect()\n",
                "print(\"‚úì Released raw_lines from memory\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}