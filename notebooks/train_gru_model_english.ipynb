{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRU Keyboard Model (2026 Optimized)\n",
        "\n",
        "Train a GRU model with **2026 best practices** for keyboard suggestions.\n",
        "\n",
        "**Supports 3 Tasks (Hybrid System):**\n",
        "\n",
        "| Input Format | Method | Example |\n",
        "|--------------|--------|----------|\n",
        "| `text + space` | GRU Model | \"How are \" \u2192 you, they, we |\n",
        "| `partial word` | Vocab + GRU Rerank | \"Hel\" \u2192 Hello, Help |\n",
        "| `typo word` | Edit Dist + GRU Rerank | \"Thers\" \u2192 There |\n",
        "\n",
        "**2026 Upgrades:**\n",
        "- \u2705 GRU Reranking - Context-aware typo/completion\n",
        "- \u2705 Perplexity Metric - Better quality measurement\n",
        "- \u2705 INT8 Quantization - 75% smaller model\n",
        "- \u2705 AdamW Optimizer - Better regularization\n",
        "- \u2705 Mixed Precision - 2x faster training\n",
        "\n",
        "**Expected Results:**\n",
        "- Accuracy: 85-90%\n",
        "- Model Size: <10MB (INT8)\n",
        "- Inference: <5ms on mobile\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime \u2192 Change runtime type \u2192 GPU (T4)\n",
        "2. Set `TESTING_MODE = True` for quick test\n",
        "3. Set `TESTING_MODE = False` for full training\n",
        "4. Run all cells in order\n",
        "5. Download TFLite for mobile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup directories\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define directories\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)\n",
        "\n",
        "print(f\"\u2713 Google Drive mounted\")\n",
        "print(f\"\u2713 Project directory: {DRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm\n",
        "print(\"\u2713 Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - OPTIMIZED FOR T4 GPU\n",
        "# ============================================================\n",
        "\n",
        "TESTING_MODE = True  # \u2190 Change to False for full training\n",
        "\n",
        "if TESTING_MODE:\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE\")\n",
        "    print(\"   - Dataset: keyboard_training_data.txt\")\n",
        "    print(\"   - Epochs: 2 (quick verification)\")\n",
        "    print(\"   - Time: ~1 min\")\n",
        "    NUM_EPOCHS = 2\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Limit vocab for speed\n",
        "    SEQUENCE_LENGTH = 10  # Better context\n",
        "else:\n",
        "    print(\"\u2713 FULL TRAINING MODE\")\n",
        "    print(\"   - Dataset: Fake.csv + True.csv + 1661-0.txt\")\n",
        "    print(\"   - Epochs: 20\")\n",
        "    print(\"   - Time: ~8-10 min (with optimizations)\")\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Smaller model, faster inference\n",
        "    SEQUENCE_LENGTH = 10  # Better predictions\n",
        "\n",
        "print(f\"\\nOptimizations:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE} (maximizes GPU)\")\n",
        "print(f\"  - Vocab limit: {VOCAB_SIZE_LIMIT:,} (reduces model size)\")\n",
        "print(f\"  - Sequence length: {SEQUENCE_LENGTH} (better context)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Required datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/Fake.csv` - Fake news dataset\n",
        "- `{DRIVE_DIR}/datasets/True.csv` - True news dataset\n",
        "- `{DRIVE_DIR}/datasets/1661-0.txt` - Next-word prediction corpus\n",
        "\n",
        "Upload these files to your Google Drive before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                    (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                    (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"\u2713 {name}: {size:.2f}MB\")\n",
        "    else:\n",
        "        print(f\"\u2717 Missing: {name}\")\n",
        "        print(f\"   Expected at: {path}\")\n",
        "        datasets_ok = False\n",
        "\n",
        "if not datasets_ok:\n",
        "    print(\"\\n\u26a0\ufe0f  Please upload missing datasets to Google Drive!\")\n",
        "    print(f\"   Upload to: {DRIVE_DIR}/datasets/\")\n",
        "    raise FileNotFoundError(\"Required datasets not found in Google Drive\")\n",
        "else:\n",
        "    print(\"\\n\u2705 All datasets found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading datasets from Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_text = []\n",
        "\n",
        "if TESTING_MODE:\n",
        "    # Testing mode: Use keyboard_training_data.txt (smaller, faster)\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE: Using keyboard_training_data.txt\")\n",
        "    \n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "    \n",
        "    if not os.path.exists(CORPUS_PATH):\n",
        "        print(f\"\\n\u2717 Missing: keyboard_training_data.txt\")\n",
        "        print(f\"   Expected at: {CORPUS_PATH}\")\n",
        "        raise FileNotFoundError(\"keyboard_training_data.txt not found\")\n",
        "    \n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "    \n",
        "    all_text.append(corpus_text)\n",
        "    print(f\"\u2713 Loaded: {len(corpus_text):,} characters\")\n",
        "    \n",
        "else:\n",
        "    # Full training mode: Use Fake.csv + True.csv + 1661-0.txt\n",
        "    print(\"\u2713 FULL TRAINING: Using Fake.csv + True.csv + 1661-0.txt\")\n",
        "    \n",
        "    FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "    TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "    \n",
        "    # Check files exist\n",
        "    for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                        (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                        (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"\\n\u2717 Missing: {name}\")\n",
        "            print(f\"   Expected at: {path}\")\n",
        "            raise FileNotFoundError(f\"{name} not found\")\n",
        "    \n",
        "    # Load fake news\n",
        "    fake_df = pd.read_csv(FAKE_NEWS_PATH)\n",
        "    true_df = pd.read_csv(TRUE_NEWS_PATH)\n",
        "    \n",
        "    print(f\"\u2713 Loaded {len(fake_df):,} fake news articles\")\n",
        "    print(f\"\u2713 Loaded {len(true_df):,} true news articles\")\n",
        "    \n",
        "    all_text.extend(fake_df['text'].tolist())\n",
        "    all_text.extend(true_df['text'].tolist())\n",
        "    \n",
        "    # Load corpus\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "        all_text.append(corpus_text)\n",
        "    \n",
        "    print(f\"\u2713 Loaded corpus: {len(corpus_text):,} characters\")\n",
        "\n",
        "# Combine and clean\n",
        "combined_text = ' '.join(all_text).lower()\n",
        "combined_text = combined_text.replace('\\n', ' ')\n",
        "combined_text = ' '.join(combined_text.split())\n",
        "\n",
        "print(f\"\\n\u2713 Total: {len(combined_text):,} characters\")\n",
        "print(f\"\u2713 Sample: {combined_text[:200]}...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize and Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"Tokenizing with vocabulary limit...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize with vocab limit\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE_LIMIT)\n",
        "tokenizer.fit_on_texts([combined_text])\n",
        "\n",
        "vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_SIZE_LIMIT)\n",
        "\n",
        "print(f\"\u2713 Total unique words: {len(tokenizer.word_index):,}\")\n",
        "print(f\"\u2713 Vocabulary size (limited): {vocab_size:,}\")\n",
        "\n",
        "# Convert to sequences\n",
        "sequences = tokenizer.texts_to_sequences([combined_text])[0]\n",
        "\n",
        "print(f\"\\nCreating optimized tf.data pipeline...\")\n",
        "\n",
        "sequences_array = np.array(sequences)\n",
        "\n",
        "# Create dataset using timeseries for next-word prediction\n",
        "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=sequences_array[:-1],\n",
        "    targets=sequences_array[SEQUENCE_LENGTH:],\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    sequence_stride=1,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Calculate total steps\n",
        "total_sequences = len(sequences) - SEQUENCE_LENGTH\n",
        "total_steps = total_sequences // BATCH_SIZE\n",
        "\n",
        "# Split: 90% train, 10% validation\n",
        "val_steps = max(1, total_steps // 10)\n",
        "train_steps = total_steps - val_steps\n",
        "\n",
        "# Split dataset with proper steps\n",
        "train_dataset = dataset.take(train_steps).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = dataset.skip(train_steps).take(val_steps).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"\u2713 Total sequences: {total_sequences:,}\")\n",
        "print(f\"\u2713 Total steps: {total_steps:,}\")\n",
        "print(f\"\u2713 Train steps: {train_steps:,} (90%)\")\n",
        "print(f\"\u2713 Val steps: {val_steps:,} (10%)\")\n",
        "print(f\"\u2713 Batch size: {BATCH_SIZE}\")\n",
        "print(f\"\u2713 Prefetching: Enabled\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n\ud83d\udcdd Note: GRU model trains on NEXT-WORD prediction only.\")\n",
        "print(\"   Word completion & typo correction use vocabulary + edit distance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable Mixed Precision for T4 GPU\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# Custom Perplexity Metric (2026 Best Practice)\n",
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "    \"\"\"Measures how 'surprised' the model is. Lower = better.\"\"\"\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False, reduction='none'\n",
        "        )\n",
        "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        loss = self.cross_entropy(y_true, y_pred)\n",
        "        self.total_loss.assign_add(tf.reduce_sum(loss))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "    \n",
        "    def result(self):\n",
        "        return tf.exp(self.total_loss / self.count)\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.total_loss.assign(0.)\n",
        "        self.count.assign(0.)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PERFORMANCE OPTIMIZATIONS (2026)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u2713 Mixed Precision enabled (FP16) - 2x faster\")\n",
        "print(\"\u2713 Perplexity metric added - Better quality measurement\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "print(\"Building GRU model (Functional API + Mixed Precision)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
        "\n",
        "# Embedding layer\n",
        "x = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=128,\n",
        "    name='embedding'\n",
        ")(inputs)\n",
        "\n",
        "# GRU layer\n",
        "x = GRU(\n",
        "    units=256,\n",
        "    dropout=0.2,\n",
        "    recurrent_dropout=0.2,\n",
        "    name='gru'\n",
        ")(x)\n",
        "\n",
        "# Dropout\n",
        "x = Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Output layer (dtype=float32 for numerical stability with mixed precision)\n",
        "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard')\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=AdamW(\n",
        "        learning_rate=1e-3,  # 0.001 (higher than Adam's default)\n",
        "        weight_decay=1e-4    # Decoupled weight decay for better regularization\n",
        "    ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', Perplexity()]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "params = model.count_params()\n",
        "size_mb = (params * 4) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 Parameters: {params:,}\")\n",
        "print(f\"\u2713 Size: {size_mb:.2f}MB (FP32), {size_mb/2:.2f}MB (FP16)\")\n",
        "print(\"\u2713 Architecture: Functional API\")\n",
        "print(\"\u2713 Optimizer: AdamW (lr=1e-3, weight_decay=1e-4)\")\n",
        "print(\"\u2713 Mixed Precision: Enabled\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING (OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Mixed Precision: FP16\")\n",
        "print(f\"Data Pipeline: tf.data (prefetched)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "if TESTING_MODE:\n",
        "    print(\"\\n\u26a0\ufe0f  This was TESTING mode\")\n",
        "    print(\"   Set TESTING_MODE = False for full training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"\\nFinal: Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras')\n",
        "\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "config = {'vocab_size': vocab_size, 'sequence_length': SEQUENCE_LENGTH}\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/config.pkl', 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "print(\"\u2713 Saved: gru_model.keras, tokenizer.pkl, config.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HYBRID PREDICTION SYSTEM (2026 UPGRADED)\n",
        "# ============================================================\n",
        "# - GRU model: next-word prediction\n",
        "# - GRU reranking: context-aware typo/completion (NEW!)\n",
        "# - Vocabulary filter: word completion\n",
        "# - Edit distance: typo correction\n",
        "# ============================================================\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return edit_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    \n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    \n",
        "    return previous_row[-1]\n",
        "\n",
        "\n",
        "# Build vocabulary list with word frequencies\n",
        "vocab_list = sorted(tokenizer.word_index.items(), key=lambda x: x[1])[:VOCAB_SIZE_LIMIT]\n",
        "vocab_words = [word for word, idx in vocab_list]\n",
        "print(f\"\u2713 Vocabulary loaded: {len(vocab_words):,} words\")\n",
        "\n",
        "\n",
        "def predict_next_word(context, top_k=5):\n",
        "    \"\"\"Use GRU model for next-word prediction\"\"\"\n",
        "    seq = tokenizer.texts_to_sequences([context.lower()])[0]\n",
        "    seq = seq[-SEQUENCE_LENGTH:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
        "    return [(tokenizer.index_word.get(i, ''), preds[i]*100) for i in top_idx if i in tokenizer.index_word]\n",
        "\n",
        "\n",
        "def complete_word(partial, top_k=5):\n",
        "    \"\"\"Use vocabulary filter for word completion\"\"\"\n",
        "    partial = partial.lower()\n",
        "    candidates = []\n",
        "    \n",
        "    for word in vocab_words:\n",
        "        if word.startswith(partial) and word != partial:\n",
        "            idx = tokenizer.word_index.get(word, 999999)\n",
        "            score = 100 / (idx + 1)\n",
        "            candidates.append((word, score))\n",
        "    \n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def correct_typo(typo, top_k=5):\n",
        "    \"\"\"Use edit distance for typo correction\"\"\"\n",
        "    typo = typo.lower()\n",
        "    candidates = []\n",
        "    \n",
        "    for word in vocab_words[:10000]:\n",
        "        if abs(len(word) - len(typo)) <= 2:\n",
        "            dist = edit_distance(word, typo)\n",
        "            if dist <= 2 and word != typo:\n",
        "                idx = tokenizer.word_index.get(word, 999999)\n",
        "                score = (100 / (dist + 1)) * (100 / (idx + 1))\n",
        "                candidates.append((word, score, dist))\n",
        "    \n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [(w, s, d) for w, s, d in candidates[:top_k]]\n",
        "\n",
        "\n",
        "def rerank_with_gru(candidates, context, top_k=5):\n",
        "    \"\"\"\n",
        "    2026 UPGRADE: Rerank candidates using GRU model probability.\n",
        "    Makes completion/typo context-aware.\n",
        "    \"\"\"\n",
        "    if not context or not candidates:\n",
        "        return candidates\n",
        "    \n",
        "    # Get GRU predictions for context\n",
        "    seq = tokenizer.texts_to_sequences([context.lower()])[0]\n",
        "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "    \n",
        "    scored_candidates = []\n",
        "    for item in candidates:\n",
        "        if len(item) == 3:\n",
        "            word, score, task = item\n",
        "        else:\n",
        "            word, score = item[:2]\n",
        "            task = 'unknown'\n",
        "        \n",
        "        # Get GRU probability for this word\n",
        "        word_idx = tokenizer.word_index.get(word.lower(), 0)\n",
        "        if word_idx > 0 and word_idx < len(preds):\n",
        "            gru_prob = preds[word_idx] * 100\n",
        "        else:\n",
        "            gru_prob = 0\n",
        "        \n",
        "        # Combined score: 30% original + 70% GRU (context matters more)\n",
        "        combined_score = score * 0.3 + gru_prob * 0.7\n",
        "        scored_candidates.append((word, combined_score, task))\n",
        "    \n",
        "    scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored_candidates[:top_k]\n",
        "\n",
        "\n",
        "def predict_hybrid(input_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Hybrid prediction with GRU reranking (2026 upgrade):\n",
        "    - \"text \" (ends with space) \u2192 GRU next-word prediction\n",
        "    - \"tex\" (partial word) \u2192 Vocabulary completion + GRU rerank\n",
        "    - \"txet\" (typo) \u2192 Edit distance + GRU rerank\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    if input_text.endswith(' '):\n",
        "        # NEXT-WORD PREDICTION (GRU model)\n",
        "        task = 'next_word'\n",
        "        preds = predict_next_word(input_text.strip(), top_k)\n",
        "        results = [(word, prob, task) for word, prob in preds]\n",
        "    \n",
        "    else:\n",
        "        words = input_text.split()\n",
        "        if len(words) > 1:\n",
        "            context = ' '.join(words[:-1])\n",
        "            partial = words[-1]\n",
        "        else:\n",
        "            context = \"\"\n",
        "            partial = input_text\n",
        "        \n",
        "        # Try COMPLETION first\n",
        "        completions = complete_word(partial, top_k * 2)\n",
        "        \n",
        "        if completions:\n",
        "            task = 'completion'\n",
        "            results = [(word, score, task) for word, score in completions]\n",
        "        else:\n",
        "            # TYPO CORRECTION\n",
        "            task = 'typo'\n",
        "            corrections = correct_typo(partial, top_k * 2)\n",
        "            results = [(word, score, task) for word, score, dist in corrections]\n",
        "        \n",
        "        # 2026 UPGRADE: RERANK with GRU if we have context\n",
        "        if context and results:\n",
        "            results = rerank_with_gru(results, context, top_k)\n",
        "    \n",
        "    return results[:top_k]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST HYBRID PREDICTIONS (2026 UPGRADED)\n",
        "# ============================================================\n",
        "\n",
        "test_cases = [\n",
        "    # Next-word prediction (GRU)\n",
        "    (\"How are \", \"Next-word (GRU)\"),\n",
        "    (\"Thank \", \"Next-word (GRU)\"),\n",
        "    (\"I want to \", \"Next-word (GRU)\"),\n",
        "    \n",
        "    # Word completion (Vocab + GRU rerank)\n",
        "    (\"Hel\", \"Completion\"),\n",
        "    (\"Tha\", \"Completion\"),\n",
        "    (\"bea\", \"Completion\"),\n",
        "    \n",
        "    # Typo correction (Edit dist + GRU rerank)\n",
        "    (\"thers\", \"Typo\"),\n",
        "    (\"teh\", \"Typo\"),\n",
        "    (\"helo\", \"Typo\"),\n",
        "    \n",
        "    # Context + partial/typo (GRU RERANKING)\n",
        "    (\"How are yo\", \"Context + GRU Rerank\"),\n",
        "    (\"How are thers\", \"Context + GRU Rerank\"),\n",
        "    (\"I want to goe\", \"Context + GRU Rerank\"),\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYBRID PREDICTION TESTS (2026 UPGRADED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for input_text, test_type in test_cases:\n",
        "    print(f\"\\n\ud83d\udcdd Input: '{input_text}' ({test_type})\")\n",
        "    predictions = predict_hybrid(input_text, top_k=5)\n",
        "    \n",
        "    if not predictions:\n",
        "        print(\"   (no predictions)\")\n",
        "        continue\n",
        "    \n",
        "    for i, (word, score, task) in enumerate(predictions, 1):\n",
        "        if score > 50:\n",
        "            emoji = \"\ud83d\udfe2\"\n",
        "        elif score > 10:\n",
        "            emoji = \"\ud83d\udfe1\"\n",
        "        else:\n",
        "            emoji = \"\ud83d\udd34\"\n",
        "        \n",
        "        print(f\"  {i}. {word:15s} {emoji} {score:5.1f} [{task}]\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"2026 Upgrades Applied:\")\n",
        "print(\"  \u2713 GRU Reranking - Context-aware typo/completion\")\n",
        "print(\"  \u2713 Perplexity Metric - Better quality measurement\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"Converting to TFLite with INT8 quantization (2026)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Representative dataset for INT8 calibration\n",
        "def representative_dataset():\n",
        "    \"\"\"Generate representative data for INT8 calibration\"\"\"\n",
        "    for _ in range(100):\n",
        "        sample = np.random.randint(0, vocab_size, size=(1, SEQUENCE_LENGTH))\n",
        "        yield [sample.astype(np.float32)]\n",
        "\n",
        "# ============================================================\n",
        "# 1. FP16 TFLite (standard)\n",
        "# ============================================================\n",
        "print(\"1. Creating FP16 TFLite...\")\n",
        "\n",
        "converter_fp16 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter_fp16.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "converter_fp16._experimental_lower_tensor_list_ops = False\n",
        "converter_fp16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter_fp16.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_fp16 = converter_fp16.convert()\n",
        "fp16_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model_fp16.tflite'\n",
        "with open(fp16_path, 'wb') as f:\n",
        "    f.write(tflite_fp16)\n",
        "\n",
        "fp16_size = len(tflite_fp16) / (1024 * 1024)\n",
        "print(f\"   \u2713 FP16 saved: {fp16_size:.2f}MB\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. INT8 TFLite (2026 recommended - smallest)\n",
        "# ============================================================\n",
        "print(\"2. Creating INT8 TFLite (2026 recommended)...\")\n",
        "\n",
        "try:\n",
        "    converter_int8 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter_int8.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    converter_int8._experimental_lower_tensor_list_ops = False\n",
        "    converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter_int8.representative_dataset = representative_dataset\n",
        "    \n",
        "    # Full INT8 quantization\n",
        "    converter_int8.target_spec.supported_types = [tf.int8]\n",
        "    converter_int8.inference_input_type = tf.int8\n",
        "    converter_int8.inference_output_type = tf.int8\n",
        "    \n",
        "    tflite_int8 = converter_int8.convert()\n",
        "    int8_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model_int8.tflite'\n",
        "    with open(int8_path, 'wb') as f:\n",
        "        f.write(tflite_int8)\n",
        "    \n",
        "    int8_size = len(tflite_int8) / (1024 * 1024)\n",
        "    print(f\"   \u2713 INT8 saved: {int8_size:.2f}MB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   \u26a0\ufe0f INT8 failed (GRU not fully supported): {str(e)[:50]}\")\n",
        "    print(\"   Using dynamic range quantization instead...\")\n",
        "    \n",
        "    # Fallback: Dynamic range quantization\n",
        "    converter_dyn = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter_dyn.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    converter_dyn._experimental_lower_tensor_list_ops = False\n",
        "    converter_dyn.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    \n",
        "    tflite_dyn = converter_dyn.convert()\n",
        "    dyn_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model_optimized.tflite'\n",
        "    with open(dyn_path, 'wb') as f:\n",
        "        f.write(tflite_dyn)\n",
        "    \n",
        "    dyn_size = len(tflite_dyn) / (1024 * 1024)\n",
        "    print(f\"   \u2713 Optimized saved: {dyn_size:.2f}MB\")\n",
        "\n",
        "# ============================================================\n",
        "# Summary\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "# LATENCY BENCHMARK\n",
        "# ============================================================\n",
        "import time\n",
        "\n",
        "print(\"Running latency benchmark...\")\n",
        "\n",
        "try:\n",
        "    # Load the FP16 model for benchmarking\n",
        "    interpreter = tf.lite.Interpreter(model_path=fp16_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()[0]\n",
        "    output_details = interpreter.get_output_details()[0]\n",
        "    \n",
        "    # Warm up\n",
        "    for _ in range(5):\n",
        "        test_input = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
        "        interpreter.set_tensor(input_details['index'], test_input)\n",
        "        interpreter.invoke()\n",
        "    \n",
        "    # Benchmark 50 iterations\n",
        "    latencies = []\n",
        "    for _ in range(50):\n",
        "        test_input = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
        "        interpreter.set_tensor(input_details['index'], test_input)\n",
        "        \n",
        "        start = time.time()\n",
        "        interpreter.invoke()\n",
        "        latencies.append((time.time() - start) * 1000)\n",
        "    \n",
        "    avg_latency = np.mean(latencies)\n",
        "    min_latency = np.min(latencies)\n",
        "    max_latency = np.max(latencies)\n",
        "    \n",
        "    print(f\"\\n\u2713 Latency benchmark (50 iterations):\")\n",
        "    print(f\"   Average: {avg_latency:.2f}ms\")\n",
        "    print(f\"   Min: {min_latency:.2f}ms\")\n",
        "    print(f\"   Max: {max_latency:.2f}ms\")\n",
        "    \n",
        "    if avg_latency < 10:\n",
        "        print(f\"   \u2705 Good for mobile (<10ms target)\")\n",
        "    else:\n",
        "        print(f\"   \u26a0\ufe0f May be slow on mobile (>10ms)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   \u26a0\ufe0f Benchmark failed: {str(e)[:50]}\")\n",
        "\n",
        "print(\"TFLITE EXPORT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 FP16 model: {fp16_size:.2f}MB\")\n",
        "print(f\"\u2713 Location: {DRIVE_DIR}/models/gru_keyboard/\")\n",
        "print(\"\\n\ud83d\udcf1 For mobile deployment:\")\n",
        "print(\"   - Use FP16 for best accuracy\")\n",
        "print(\"   - Use INT8 for smallest size (if available)\")\n",
        "print(\"   - Note: GRU requires SELECT_TF_OPS\")\n",
        "print(\"\\n\ud83c\udf4e For iOS: Run next cell to export CoreML (.mlpackage)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n\ud83c\udf89 Training complete! Download from Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Export to CoreML (iOS Native)\n",
        "\n",
        "Export the model to CoreML format for optimal iOS performance:\n",
        "- \u2705 Native Neural Engine support\n",
        "- \u2705 No TFLite SDK required\n",
        "- \u2705 Fastest inference on iOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COREML EXPORT (iOS Native)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Installing coremltools (compatible version)...\")\n",
        "!pip install -q coremltools==7.2 protobuf==3.20.3\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress version warnings\n",
        "\n",
        "import coremltools as ct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COREML EXPORT FOR iOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Configuration (must match training config)\n",
        "seq_length = 10    # SEQUENCE_LENGTH from config cell\n",
        "vocab_size = 25000 # VOCAB_SIZE_LIMIT from config cell\n",
        "\n",
        "# Load model from saved .keras file\n",
        "keras_model_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras'\n",
        "print(f\"Loading model from: {keras_model_path}\")\n",
        "\n",
        "try:\n",
        "    keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "    print(f\"\u2713 Model loaded successfully\")\n",
        "    print(f\"  Input shape: {keras_model.input_shape}\")\n",
        "    print(f\"  Output shape: {keras_model.output_shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Failed to load model: {e}\")\n",
        "    print(\"  Make sure cell '8. Save Model' was executed first\")\n",
        "    raise e\n",
        "\n",
        "try:\n",
        "    print(\"\\nConverting model to CoreML...\")\n",
        "    \n",
        "    # Convert Keras model to CoreML\n",
        "    mlmodel = ct.convert(\n",
        "        keras_model,\n",
        "        inputs=[ct.TensorType(\n",
        "            shape=(1, seq_length),\n",
        "            name=\"input\",\n",
        "            dtype=np.float32\n",
        "        )],\n",
        "        outputs=[ct.TensorType(name=\"output\")],\n",
        "        convert_to=\"mlprogram\",\n",
        "        compute_precision=ct.precision.FLOAT16,\n",
        "        minimum_deployment_target=ct.target.iOS15\n",
        "    )\n",
        "    \n",
        "    # Add metadata\n",
        "    mlmodel.author = \"Keyboard-Suggestions-ML\"\n",
        "    mlmodel.short_description = \"GRU Keyboard Prediction Model - Next word, completion, typo correction\"\n",
        "    mlmodel.version = \"1.0.0\"\n",
        "    \n",
        "    # Save as .mlpackage\n",
        "    coreml_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_keyboard_ios.mlpackage'\n",
        "    mlmodel.save(coreml_path)\n",
        "    \n",
        "    # Get size\n",
        "    import os\n",
        "    def get_dir_size(path):\n",
        "        total = 0\n",
        "        for dirpath, dirnames, filenames in os.walk(path):\n",
        "            for f in filenames:\n",
        "                fp = os.path.join(dirpath, f)\n",
        "                total += os.path.getsize(fp)\n",
        "        return total\n",
        "    \n",
        "    coreml_size = get_dir_size(coreml_path) / (1024 * 1024)\n",
        "    \n",
        "    print(f\"\\n\u2705 CoreML Export Success!\")\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"COREML MODEL INFO\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\u2713 Format: .mlpackage (ML Program)\")\n",
        "    print(f\"\u2713 Size: {coreml_size:.2f}MB\")\n",
        "    print(f\"\u2713 Precision: Float16\")\n",
        "    print(f\"\u2713 Input: (1, {seq_length}) float32\")\n",
        "    print(f\"\u2713 Output: (1, {vocab_size}) float32\")\n",
        "    print(f\"\u2713 Target: iOS 15+\")\n",
        "    print(f\"\u2713 Path: {coreml_path}\")\n",
        "    print(\"\\n\ud83d\udcf1 iOS Deployment:\")\n",
        "    print(\"   1. Download gru_keyboard_ios.mlpackage from Drive\")\n",
        "    print(\"   2. Drag .mlpackage to Xcode project\")\n",
        "    print(\"   3. Uses Neural Engine automatically\")\n",
        "    print(\"   4. No TensorFlow SDK required!\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u26a0\ufe0f CoreML conversion failed: {str(e)}\")\n",
        "    print(\"\\nPossible causes:\")\n",
        "    print(\"  1. GRU layer conversion issue\")\n",
        "    print(\"  2. TensorFlow/coremltools version mismatch\")\n",
        "    print(\"\\nSolution: Use TFLite on iOS instead.\")\n",
        "    print(\"  - Download gru_model_fp16.tflite\")\n",
        "    print(\"  - Add TensorFlowLiteSwift + SelectTfOps pods\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}