{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Task GRU Keyboard Model\n",
        "\n",
        "Train a **multi-task GRU model** for keyboard suggestions.\n",
        "\n",
        "**Supports 3 Tasks (Smart Detection):**\n",
        "\n",
        "| Input Format | Task | Example |\n",
        "|--------------|------|----------|\n",
        "| `text + space` | Next-word prediction | \"How are \" \u2192 you, they, we |\n",
        "| `partial word` | Word completion | \"Hel\" \u2192 Hello, Help, Hell |\n",
        "| `typo word` | Typo correction | \"Thers\" \u2192 There, These |\n",
        "\n",
        "**Model Specifications:**\n",
        "- Architecture: GRU (Gated Recurrent Unit)\n",
        "- Parameters: ~10M\n",
        "- Model Size: 30-40MB (Keras), 15-20MB (TFLite)\n",
        "- Training Time: 5-10 minutes on GPU\n",
        "- Accuracy: 75-80%\n",
        "- Inference: <10ms on mobile\n",
        "\n",
        "**Why GRU?**\n",
        "- \u2705 30% faster than LSTM\n",
        "- \u2705 75% fewer parameters\n",
        "- \u2705 Works with limited data\n",
        "- \u2705 Mobile-friendly\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime \u2192 Change runtime type \u2192 GPU (T4)\n",
        "2. Set `TESTING_MODE = True` for quick test (2 epochs)\n",
        "3. Set `TESTING_MODE = False` for full training (20 epochs)\n",
        "4. Run all cells in order\n",
        "5. Download TFLite for mobile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup directories\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define directories\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)\n",
        "\n",
        "print(f\"\u2713 Google Drive mounted\")\n",
        "print(f\"\u2713 Project directory: {DRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm\n",
        "print(\"\u2713 Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - OPTIMIZED FOR T4 GPU\n",
        "# ============================================================\n",
        "\n",
        "TESTING_MODE = True  # \u2190 Change to False for full training\n",
        "\n",
        "if TESTING_MODE:\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE\")\n",
        "    print(\"   - Dataset: keyboard_training_data.txt\")\n",
        "    print(\"   - Epochs: 2 (quick verification)\")\n",
        "    print(\"   - Time: ~1 min\")\n",
        "    NUM_EPOCHS = 2\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Limit vocab for speed\n",
        "    SEQUENCE_LENGTH = 10  # Better context\n",
        "else:\n",
        "    print(\"\u2713 FULL TRAINING MODE\")\n",
        "    print(\"   - Dataset: Fake.csv + True.csv + 1661-0.txt\")\n",
        "    print(\"   - Epochs: 20\")\n",
        "    print(\"   - Time: ~8-10 min (with optimizations)\")\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Smaller model, faster inference\n",
        "    SEQUENCE_LENGTH = 10  # Better predictions\n",
        "\n",
        "print(f\"\\nOptimizations:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE} (maximizes GPU)\")\n",
        "print(f\"  - Vocab limit: {VOCAB_SIZE_LIMIT:,} (reduces model size)\")\n",
        "print(f\"  - Sequence length: {SEQUENCE_LENGTH} (better context)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Required datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/Fake.csv` - Fake news dataset\n",
        "- `{DRIVE_DIR}/datasets/True.csv` - True news dataset\n",
        "- `{DRIVE_DIR}/datasets/1661-0.txt` - Next-word prediction corpus\n",
        "\n",
        "Upload these files to your Google Drive before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                    (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                    (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"\u2713 {name}: {size:.2f}MB\")\n",
        "    else:\n",
        "        print(f\"\u2717 Missing: {name}\")\n",
        "        print(f\"   Expected at: {path}\")\n",
        "        datasets_ok = False\n",
        "\n",
        "if not datasets_ok:\n",
        "    print(\"\\n\u26a0\ufe0f  Please upload missing datasets to Google Drive!\")\n",
        "    print(f\"   Upload to: {DRIVE_DIR}/datasets/\")\n",
        "    raise FileNotFoundError(\"Required datasets not found in Google Drive\")\n",
        "else:\n",
        "    print(\"\\n\u2705 All datasets found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading datasets from Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_text = []\n",
        "\n",
        "if TESTING_MODE:\n",
        "    # Testing mode: Use keyboard_training_data.txt (smaller, faster)\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE: Using keyboard_training_data.txt\")\n",
        "    \n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "    \n",
        "    if not os.path.exists(CORPUS_PATH):\n",
        "        print(f\"\\n\u2717 Missing: keyboard_training_data.txt\")\n",
        "        print(f\"   Expected at: {CORPUS_PATH}\")\n",
        "        raise FileNotFoundError(\"keyboard_training_data.txt not found\")\n",
        "    \n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "    \n",
        "    all_text.append(corpus_text)\n",
        "    print(f\"\u2713 Loaded: {len(corpus_text):,} characters\")\n",
        "    \n",
        "else:\n",
        "    # Full training mode: Use Fake.csv + True.csv + 1661-0.txt\n",
        "    print(\"\u2713 FULL TRAINING: Using Fake.csv + True.csv + 1661-0.txt\")\n",
        "    \n",
        "    FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "    TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "    \n",
        "    # Check files exist\n",
        "    for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                        (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                        (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"\\n\u2717 Missing: {name}\")\n",
        "            print(f\"   Expected at: {path}\")\n",
        "            raise FileNotFoundError(f\"{name} not found\")\n",
        "    \n",
        "    # Load fake news\n",
        "    fake_df = pd.read_csv(FAKE_NEWS_PATH)\n",
        "    true_df = pd.read_csv(TRUE_NEWS_PATH)\n",
        "    \n",
        "    print(f\"\u2713 Loaded {len(fake_df):,} fake news articles\")\n",
        "    print(f\"\u2713 Loaded {len(true_df):,} true news articles\")\n",
        "    \n",
        "    all_text.extend(fake_df['text'].tolist())\n",
        "    all_text.extend(true_df['text'].tolist())\n",
        "    \n",
        "    # Load corpus\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "        all_text.append(corpus_text)\n",
        "    \n",
        "    print(f\"\u2713 Loaded corpus: {len(corpus_text):,} characters\")\n",
        "\n",
        "# Combine and clean\n",
        "combined_text = ' '.join(all_text).lower()\n",
        "combined_text = combined_text.replace('\\n', ' ')\n",
        "combined_text = ' '.join(combined_text.split())\n",
        "\n",
        "print(f\"\\n\u2713 Total: {len(combined_text):,} characters\")\n",
        "print(f\"\u2713 Sample: {combined_text[:200]}...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Multi-Task Training Data Generation\n",
        "\n",
        "Generate training data for 3 tasks:\n",
        "1. **Next-word prediction:** \"How are \" \u2192 \"you\"\n",
        "2. **Word completion:** \"Hel\" \u2192 \"Hello\"\n",
        "3. **Typo correction:** \"Thers\" \u2192 \"There\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MULTI-TASK TRAINING DATA GENERATION\n",
        "# ============================================================\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_typos(word):\n",
        "    \"\"\"\n",
        "    Generate synthetic typos for a word\n",
        "    Returns list of common typo patterns\n",
        "    \"\"\"\n",
        "    if len(word) < 3:\n",
        "        return []\n",
        "    \n",
        "    typos = []\n",
        "    \n",
        "    # 1. Swap adjacent characters (teh \u2192 the)\n",
        "    for i in range(len(word)-1):\n",
        "        typo = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "        if typo != word:\n",
        "            typos.append(typo)\n",
        "    \n",
        "    # 2. Delete one character (helo \u2192 hello)\n",
        "    for i in range(len(word)):\n",
        "        typo = word[:i] + word[i+1:]\n",
        "        if len(typo) >= 2:\n",
        "            typos.append(typo)\n",
        "    \n",
        "    # 3. Duplicate one character (helllo \u2192 hello)\n",
        "    for i in range(len(word)):\n",
        "        typo = word[:i+1] + word[i] + word[i+1:]\n",
        "        typos.append(typo)\n",
        "    \n",
        "    # 4. Replace with nearby keyboard key\n",
        "    keyboard_map = {\n",
        "        'a': 'sqwz', 'b': 'vghn', 'c': 'xdfv', 'd': 'sfxce', 'e': 'wsdr',\n",
        "        'f': 'dgcvr', 'g': 'fhvbt', 'h': 'gjbny', 'i': 'ujko', 'j': 'hknmu',\n",
        "        'k': 'jlmio', 'l': 'kop', 'm': 'njk', 'n': 'bhjm', 'o': 'iklp',\n",
        "        'p': 'ol', 'q': 'wa', 'r': 'edft', 's': 'awedxz', 't': 'rfgy',\n",
        "        'u': 'yhji', 'v': 'cfgb', 'w': 'qase', 'x': 'zsdc', 'y': 'tghu',\n",
        "        'z': 'asx'\n",
        "    }\n",
        "    \n",
        "    for i, char in enumerate(word.lower()):\n",
        "        if char in keyboard_map:\n",
        "            for neighbor in keyboard_map[char][:2]:\n",
        "                typo = word[:i] + neighbor + word[i+1:]\n",
        "                typos.append(typo)\n",
        "    \n",
        "    return list(set(typos))[:10]\n",
        "\n",
        "\n",
        "def generate_multitask_training_data(text, tokenizer, max_samples=100000):\n",
        "    \"\"\"\n",
        "    Generate training data for all 3 tasks:\n",
        "    1. Next-word prediction (60%)\n",
        "    2. Word completion (25%)\n",
        "    3. Typo correction (15%)\n",
        "    \"\"\"\n",
        "    training_data = []\n",
        "    vocab_list = list(tokenizer.word_index.keys())[:VOCAB_SIZE_LIMIT]\n",
        "    \n",
        "    print(\"Generating multi-task training data...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Task 1: Next-word prediction from sentences\n",
        "    print(\"1. Generating next-word prediction data...\")\n",
        "    sentences = text.replace('!', '.').replace('?', '.').split('.')\n",
        "    next_word_count = 0\n",
        "    \n",
        "    for sentence in sentences[:max_samples//3]:\n",
        "        words = sentence.strip().split()\n",
        "        if len(words) >= 2:\n",
        "            for i in range(1, min(len(words), 10)):\n",
        "                context = ' '.join(words[:i]) + ' '  # Add space for next-word\n",
        "                target = words[i].lower()\n",
        "                if target in tokenizer.word_index:\n",
        "                    training_data.append((context, target, 'next_word'))\n",
        "                    next_word_count += 1\n",
        "    \n",
        "    print(f\"   \u2713 Generated {next_word_count:,} next-word samples\")\n",
        "    \n",
        "    # Task 2: Word completion from vocabulary\n",
        "    print(\"2. Generating word completion data...\")\n",
        "    completion_count = 0\n",
        "    \n",
        "    for word in vocab_list[:5000]:\n",
        "        if len(word) >= 3:\n",
        "            for i in range(1, len(word)):\n",
        "                prefix = word[:i]\n",
        "                training_data.append((prefix, word, 'completion'))\n",
        "                completion_count += 1\n",
        "    \n",
        "    print(f\"   \u2713 Generated {completion_count:,} completion samples\")\n",
        "    \n",
        "    # Task 3: Typo correction from vocabulary\n",
        "    print(\"3. Generating typo correction data...\")\n",
        "    typo_count = 0\n",
        "    \n",
        "    for word in vocab_list[:3000]:\n",
        "        if len(word) >= 4:\n",
        "            typos = generate_typos(word)\n",
        "            for typo in typos:\n",
        "                training_data.append((typo, word, 'typo'))\n",
        "                typo_count += 1\n",
        "    \n",
        "    print(f\"   \u2713 Generated {typo_count:,} typo samples\")\n",
        "    \n",
        "    # Shuffle all training data\n",
        "    random.shuffle(training_data)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total training samples: {len(training_data):,}\")\n",
        "    print(f\"  - Next-word: {next_word_count:,} ({next_word_count/len(training_data)*100:.1f}%)\")\n",
        "    print(f\"  - Completion: {completion_count:,} ({completion_count/len(training_data)*100:.1f}%)\")\n",
        "    print(f\"  - Typo: {typo_count:,} ({typo_count/len(training_data)*100:.1f}%)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return training_data\n",
        "\n",
        "print(\"\u2713 Multi-task data generation functions ready\")\n",
        "print(\"  Run next cell to generate training data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize and Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"Tokenizing with vocabulary limit...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Step 1: Create tokenizer\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE_LIMIT)\n",
        "tokenizer.fit_on_texts([combined_text])\n",
        "\n",
        "vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_SIZE_LIMIT)\n",
        "\n",
        "print(f\"\u2713 Total unique words: {len(tokenizer.word_index):,}\")\n",
        "print(f\"\u2713 Vocabulary size (limited): {vocab_size:,}\")\n",
        "\n",
        "# Step 2: Generate multi-task training data\n",
        "multitask_data = generate_multitask_training_data(\n",
        "    text=combined_text,\n",
        "    tokenizer=tokenizer,\n",
        "    max_samples=150000\n",
        ")\n",
        "\n",
        "# Step 3: Convert to sequences\n",
        "print(\"\\nConverting multi-task data to sequences...\")\n",
        "\n",
        "X_inputs = []\n",
        "y_targets = []\n",
        "task_types = []\n",
        "\n",
        "for input_text, target_word, task_type in multitask_data:\n",
        "    # Tokenize input\n",
        "    if task_type == 'next_word':\n",
        "        input_seq = tokenizer.texts_to_sequences([input_text.strip()])[0]\n",
        "    else:\n",
        "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    \n",
        "    # Tokenize target\n",
        "    target_seq = tokenizer.texts_to_sequences([target_word])[0]\n",
        "    \n",
        "    if input_seq and target_seq:\n",
        "        input_padded = pad_sequences([input_seq], maxlen=SEQUENCE_LENGTH, padding='pre')[0]\n",
        "        X_inputs.append(input_padded)\n",
        "        y_targets.append(target_seq[0])\n",
        "        task_types.append(task_type)\n",
        "\n",
        "X = np.array(X_inputs)\n",
        "y = np.array(y_targets)\n",
        "\n",
        "print(f\"\u2713 Created {len(X):,} training sequences\")\n",
        "print(f\"\u2713 Input shape: {X.shape}\")\n",
        "print(f\"\u2713 Output shape: {y.shape}\")\n",
        "\n",
        "# Step 4: Create tf.data dataset\n",
        "print(\"\\nCreating optimized tf.data pipeline...\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Split train/val\n",
        "total_steps = len(X) // BATCH_SIZE\n",
        "val_steps = max(1, total_steps // 10)\n",
        "train_steps = total_steps - val_steps\n",
        "\n",
        "train_dataset = dataset.take(train_steps)\n",
        "val_dataset = dataset.skip(train_steps).take(val_steps)\n",
        "\n",
        "print(f\"\u2713 Total steps: {total_steps:,}\")\n",
        "print(f\"\u2713 Train steps: {train_steps:,} (90%)\")\n",
        "print(f\"\u2713 Val steps: {val_steps:,} (10%)\")\n",
        "print(f\"\u2713 Batch size: {BATCH_SIZE}\")\n",
        "print(f\"\u2713 Prefetching: Enabled\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable Mixed Precision for T4 GPU (2x faster training)\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PERFORMANCE OPTIMIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u2713 Mixed Precision enabled (FP16)\")\n",
        "print(\"  - Training speed: ~2x faster\")\n",
        "print(\"  - Memory usage: ~40% less\")\n",
        "print(\"  - Accuracy: Same as FP32\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "print(\"Building GRU model (Functional API + Mixed Precision)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
        "\n",
        "# Embedding layer\n",
        "x = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=128,\n",
        "    name='embedding'\n",
        ")(inputs)\n",
        "\n",
        "# GRU layer\n",
        "x = GRU(\n",
        "    units=256,\n",
        "    dropout=0.2,\n",
        "    recurrent_dropout=0.2,\n",
        "    name='gru'\n",
        ")(x)\n",
        "\n",
        "# Dropout\n",
        "x = Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Output layer (dtype=float32 for numerical stability with mixed precision)\n",
        "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard')\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=AdamW(\n",
        "        learning_rate=1e-3,  # 0.001 (higher than Adam's default)\n",
        "        weight_decay=1e-4    # Decoupled weight decay for better regularization\n",
        "    ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "params = model.count_params()\n",
        "size_mb = (params * 4) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 Parameters: {params:,}\")\n",
        "print(f\"\u2713 Size: {size_mb:.2f}MB (FP32), {size_mb/2:.2f}MB (FP16)\")\n",
        "print(\"\u2713 Architecture: Functional API\")\n",
        "print(\"\u2713 Optimizer: AdamW (lr=1e-3, weight_decay=1e-4)\")\n",
        "print(\"\u2713 Mixed Precision: Enabled\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING (OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Mixed Precision: FP16\")\n",
        "print(f\"Data Pipeline: tf.data (prefetched)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "if TESTING_MODE:\n",
        "    print(\"\\n\u26a0\ufe0f  This was TESTING mode\")\n",
        "    print(\"   Set TESTING_MODE = False for full training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"\\nFinal: Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras')\n",
        "\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "config = {'vocab_size': vocab_size, 'sequence_length': SEQUENCE_LENGTH}\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/config.pkl', 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "print(\"\u2713 Saved: gru_model.keras, tokenizer.pkl, config.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MULTI-TASK PREDICTION FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return edit_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    \n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    \n",
        "    return previous_row[-1]\n",
        "\n",
        "\n",
        "def predict_multitask(input_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Smart prediction that detects task type:\n",
        "    - \"text \" (with space) \u2192 next-word prediction\n",
        "    - \"tex\" (partial) \u2192 word completion\n",
        "    - \"txet\" (typo) \u2192 typo correction\n",
        "    \"\"\"\n",
        "    \n",
        "    # Detect task type\n",
        "    if input_text.endswith(' '):\n",
        "        task = 'next_word'\n",
        "        context = input_text.strip()\n",
        "        partial = None\n",
        "    else:\n",
        "        words = input_text.split()\n",
        "        if len(words) > 1:\n",
        "            context = ' '.join(words[:-1])\n",
        "            partial = words[-1].lower()\n",
        "            task = 'completion_or_typo'\n",
        "        else:\n",
        "            context = \"\"\n",
        "            partial = input_text.lower()\n",
        "            task = 'completion_or_typo'\n",
        "    \n",
        "    # Tokenize and predict\n",
        "    if context:\n",
        "        sequence = tokenizer.texts_to_sequences([context])[0]\n",
        "    else:\n",
        "        sequence = []\n",
        "    \n",
        "    sequence = pad_sequences([sequence], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    predictions = model.predict(sequence, verbose=0)[0]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    if task == 'next_word':\n",
        "        top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
        "        for idx in top_indices:\n",
        "            word = tokenizer.index_word.get(idx, '')\n",
        "            if word:\n",
        "                results.append((word, predictions[idx] * 100, 'next_word'))\n",
        "    else:\n",
        "        candidates = []\n",
        "        for idx, prob in enumerate(predictions):\n",
        "            word = tokenizer.index_word.get(idx, '')\n",
        "            if not word:\n",
        "                continue\n",
        "            \n",
        "            # Completion (starts with partial)\n",
        "            if word.startswith(partial):\n",
        "                candidates.append((word, prob * 100, 'completion', 0))\n",
        "            # Typo correction (close edit distance)\n",
        "            elif len(word) >= len(partial) - 1:\n",
        "                dist = edit_distance(word, partial)\n",
        "                if dist <= 2:\n",
        "                    candidates.append((word, prob * 100, 'typo', dist))\n",
        "        \n",
        "        candidates.sort(key=lambda x: (x[1], -x[3]), reverse=True)\n",
        "        results = [(w, p, t) for w, p, t, d in candidates[:top_k]]\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST MULTI-TASK PREDICTIONS\n",
        "# ============================================================\n",
        "\n",
        "test_cases = [\n",
        "    # Next-word prediction (with space)\n",
        "    (\"How are \", \"Next-word\"),\n",
        "    (\"Thank \", \"Next-word\"),\n",
        "    (\"I want to \", \"Next-word\"),\n",
        "    (\"Good morning \", \"Next-word\"),\n",
        "    \n",
        "    # Word completion (partial word)\n",
        "    (\"Hel\", \"Completion\"),\n",
        "    (\"Tha\", \"Completion\"),\n",
        "    (\"Goo\", \"Completion\"),\n",
        "    (\"Mor\", \"Completion\"),\n",
        "    \n",
        "    # Typo correction (misspelled word)\n",
        "    (\"thers\", \"Typo\"),\n",
        "    (\"teh\", \"Typo\"),\n",
        "    (\"helo\", \"Typo\"),\n",
        "    (\"recieve\", \"Typo\"),\n",
        "    \n",
        "    # Combined (context + partial/typo)\n",
        "    (\"How are thers\", \"Context + Typo\"),\n",
        "    (\"I want to goo\", \"Context + Typo\"),\n",
        "    (\"Thank yo\", \"Context + Completion\"),\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-TASK PREDICTION TESTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for input_text, test_type in test_cases:\n",
        "    print(f\"\\n\ud83d\udcdd Input: '{input_text}' ({test_type})\")\n",
        "    predictions = predict_multitask(input_text, top_k=5)\n",
        "    \n",
        "    if not predictions:\n",
        "        print(\"   (no predictions)\")\n",
        "        continue\n",
        "    \n",
        "    for i, (word, prob, task) in enumerate(predictions, 1):\n",
        "        if prob > 50:\n",
        "            emoji = \"\ud83d\udfe2\"\n",
        "        elif prob > 20:\n",
        "            emoji = \"\ud83d\udfe1\"\n",
        "        else:\n",
        "            emoji = \"\ud83d\udd34\"\n",
        "        \n",
        "        print(f\"  {i}. {word:15s} {emoji} {prob:5.1f}% [{task}]\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Converting to TFLite (GRU-compatible)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# GRU/LSTM requires SELECT_TF_OPS for dynamic tensor lists\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard TFLite ops\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # TensorFlow ops (for GRU)\n",
        "]\n",
        "\n",
        "# Disable tensor list lowering (required for GRU)\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# Optimize for size\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert\n",
        "print(\"Converting model (this may take a minute)...\")\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save\n",
        "tflite_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model.tflite'\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "size_mb = len(tflite_model) / (1024 * 1024)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 TFLite model saved: {size_mb:.2f}MB\")\n",
        "print(f\"\u2713 Path: {tflite_path}\")\n",
        "print(\"\\n\u26a0\ufe0f  Note: Model uses SELECT_TF_OPS for GRU support\")\n",
        "print(\"   This is normal and required for RNN layers\")\n",
        "print(\"\\n\ud83c\udf89 Training complete! Download from Google Drive.\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}