{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRU Keyboard Prediction Model Training\n",
        "\n",
        "Train a lightweight GRU model for keyboard suggestions using English datasets.\n",
        "\n",
        "**Features:**\n",
        "1. Word Completion: \"hel\" ‚Üí [\"hello\", \"help\", \"held\"]\n",
        "2. Next-Word Prediction: \"how are\" ‚Üí [\"you\", \"they\", \"we\"]\n",
        "3. Typo Correction: \"thers\" ‚Üí [\"there\", \"theirs\"]\n",
        "\n",
        "**Model Specifications:**\n",
        "- Architecture: GRU (Gated Recurrent Unit)\n",
        "- Parameters: ~3M (vs 14M for transformer)\n",
        "- Model Size: 3-4MB (TFLite FP16)\n",
        "- Training Time: 15-20 minutes on GPU\n",
        "- Accuracy: 75-80%\n",
        "- Inference: <10ms on mobile\n",
        "\n",
        "**Why GRU over LSTM/Transformer?**\n",
        "- ‚úÖ 30% faster than LSTM\n",
        "- ‚úÖ 75% fewer parameters than LSTM\n",
        "- ‚úÖ Works with limited data (vs transformer needs 1M+ samples)\n",
        "- ‚úÖ Better for short sequences\n",
        "- ‚úÖ Mobile-friendly\n",
        "\n",
        "**Data Sources:**\n",
        "- Fake News Detection (English text)\n",
        "- Next-Word Prediction (English corpus)\n",
        "- **Excludes:** Japanese data, phone conversations\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
        "2. Run all cells in order\n",
        "3. Model saves to Google Drive\n",
        "4. Download TFLite for mobile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup directories\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define directories\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)\n",
        "\n",
        "print(f\"‚úì Google Drive mounted\")\n",
        "print(f\"‚úì Project directory: {DRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - OPTIMIZED FOR T4 GPU\n",
        "# ============================================================\n",
        "\n",
        "TESTING_MODE = True  # ‚Üê Change to False for full training\n",
        "\n",
        "if TESTING_MODE:\n",
        "    print(\"‚ö†Ô∏è  TESTING MODE\")\n",
        "    print(\"   - Dataset: keyboard_training_data.txt\")\n",
        "    print(\"   - Epochs: 2 (quick verification)\")\n",
        "    print(\"   - Time: ~1 min\")\n",
        "    NUM_EPOCHS = 2\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Limit vocab for speed\n",
        "    SEQUENCE_LENGTH = 10  # Better context\n",
        "else:\n",
        "    print(\"‚úì FULL TRAINING MODE\")\n",
        "    print(\"   - Dataset: Fake.csv + True.csv + 1661-0.txt\")\n",
        "    print(\"   - Epochs: 20\")\n",
        "    print(\"   - Time: ~8-10 min (with optimizations)\")\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Smaller model, faster inference\n",
        "    SEQUENCE_LENGTH = 10  # Better predictions\n",
        "\n",
        "print(f\"\\nOptimizations:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE} (maximizes GPU)\")\n",
        "print(f\"  - Vocab limit: {VOCAB_SIZE_LIMIT:,} (reduces model size)\")\n",
        "print(f\"  - Sequence length: {SEQUENCE_LENGTH} (better context)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Required datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/Fake.csv` - Fake news dataset\n",
        "- `{DRIVE_DIR}/datasets/True.csv` - True news dataset\n",
        "- `{DRIVE_DIR}/datasets/1661-0.txt` - Next-word prediction corpus\n",
        "\n",
        "Upload these files to your Google Drive before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                    (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                    (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"‚úì {name}: {size:.2f}MB\")\n",
        "    else:\n",
        "        print(f\"‚úó Missing: {name}\")\n",
        "        print(f\"   Expected at: {path}\")\n",
        "        datasets_ok = False\n",
        "\n",
        "if not datasets_ok:\n",
        "    print(\"\\n‚ö†Ô∏è  Please upload missing datasets to Google Drive!\")\n",
        "    print(f\"   Upload to: {DRIVE_DIR}/datasets/\")\n",
        "    raise FileNotFoundError(\"Required datasets not found in Google Drive\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All datasets found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading datasets from Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_text = []\n",
        "\n",
        "if TESTING_MODE:\n",
        "    # Testing mode: Use keyboard_training_data.txt (smaller, faster)\n",
        "    print(\"‚ö†Ô∏è  TESTING MODE: Using keyboard_training_data.txt\")\n",
        "    \n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "    \n",
        "    if not os.path.exists(CORPUS_PATH):\n",
        "        print(f\"\\n‚úó Missing: keyboard_training_data.txt\")\n",
        "        print(f\"   Expected at: {CORPUS_PATH}\")\n",
        "        raise FileNotFoundError(\"keyboard_training_data.txt not found\")\n",
        "    \n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "    \n",
        "    all_text.append(corpus_text)\n",
        "    print(f\"‚úì Loaded: {len(corpus_text):,} characters\")\n",
        "    \n",
        "else:\n",
        "    # Full training mode: Use Fake.csv + True.csv + 1661-0.txt\n",
        "    print(\"‚úì FULL TRAINING: Using Fake.csv + True.csv + 1661-0.txt\")\n",
        "    \n",
        "    FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "    TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "    \n",
        "    # Check files exist\n",
        "    for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                        (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                        (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"\\n‚úó Missing: {name}\")\n",
        "            print(f\"   Expected at: {path}\")\n",
        "            raise FileNotFoundError(f\"{name} not found\")\n",
        "    \n",
        "    # Load fake news\n",
        "    fake_df = pd.read_csv(FAKE_NEWS_PATH)\n",
        "    true_df = pd.read_csv(TRUE_NEWS_PATH)\n",
        "    \n",
        "    print(f\"‚úì Loaded {len(fake_df):,} fake news articles\")\n",
        "    print(f\"‚úì Loaded {len(true_df):,} true news articles\")\n",
        "    \n",
        "    all_text.extend(fake_df['text'].tolist())\n",
        "    all_text.extend(true_df['text'].tolist())\n",
        "    \n",
        "    # Load corpus\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "        all_text.append(corpus_text)\n",
        "    \n",
        "    print(f\"‚úì Loaded corpus: {len(corpus_text):,} characters\")\n",
        "\n",
        "# Combine and clean\n",
        "combined_text = ' '.join(all_text).lower()\n",
        "combined_text = combined_text.replace('\\n', ' ')\n",
        "combined_text = ' '.join(combined_text.split())\n",
        "\n",
        "print(f\"\\n‚úì Total: {len(combined_text):,} characters\")\n",
        "print(f\"‚úì Sample: {combined_text[:200]}...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize and Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Tokenizing with vocabulary limit...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize with vocab limit (faster, smaller model)\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE_LIMIT)\n",
        "tokenizer.fit_on_texts([combined_text])\n",
        "\n",
        "# Actual vocab size (limited) / +1 for OOV\n",
        "vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_SIZE_LIMIT)\n",
        "\n",
        "print(f\"‚úì Total unique words: {len(tokenizer.word_index):,}\")\n",
        "print(f\"‚úì Vocabulary size (limited): {vocab_size:,}\")\n",
        "print(f\"‚úì Coverage: Top {VOCAB_SIZE_LIMIT:,} most frequent words\")\n",
        "\n",
        "# Convert to sequences\n",
        "sequences = tokenizer.texts_to_sequences([combined_text])[0]\n",
        "\n",
        "print(f\"\\nCreating optimized tf.data pipeline...\")\n",
        "\n",
        "# Create sequences using tf.data (much faster than Python loops)\n",
        "import numpy as np\n",
        "sequences_array = np.array(sequences)\n",
        "\n",
        "# Create input-target pairs\n",
        "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=sequences_array[:-1],\n",
        "    targets=sequences_array[SEQUENCE_LENGTH:],\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    sequence_stride=1,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Prefetch for performance (GPU trains while CPU prepares next batch)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Calculate steps\n",
        "total_sequences = len(sequences) - SEQUENCE_LENGTH\n",
        "steps_per_epoch = total_sequences // BATCH_SIZE\n",
        "\n",
        "# Split into train/val\n",
        "val_steps = steps_per_epoch // 10  # 10% for validation\n",
        "train_steps = steps_per_epoch - val_steps\n",
        "\n",
        "train_dataset = dataset.take(train_steps)\n",
        "val_dataset = dataset.skip(train_steps)\n",
        "\n",
        "print(f\"‚úì Total sequences: {total_sequences:,}\")\n",
        "print(f\"‚úì Train steps: {train_steps:,}\")\n",
        "print(f\"‚úì Val steps: {val_steps:,}\")\n",
        "print(f\"‚úì Batch size: {BATCH_SIZE}\")\n",
        "print(f\"‚úì Prefetching: Enabled (AUTOTUNE)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable Mixed Precision for T4 GPU (2x faster training)\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PERFORMANCE OPTIMIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚úì Mixed Precision enabled (FP16)\")\n",
        "print(\"  - Training speed: ~2x faster\")\n",
        "print(\"  - Memory usage: ~40% less\")\n",
        "print(\"  - Accuracy: Same as FP32\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "print(\"Building GRU model (Functional API + Mixed Precision)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
        "\n",
        "# Embedding layer\n",
        "x = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=128,\n",
        "    name='embedding'\n",
        ")(inputs)\n",
        "\n",
        "# GRU layer\n",
        "x = GRU(\n",
        "    units=256,\n",
        "    dropout=0.2,\n",
        "    recurrent_dropout=0.2,\n",
        "    name='gru'\n",
        ")(x)\n",
        "\n",
        "# Dropout\n",
        "x = Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Output layer (dtype=float32 for numerical stability with mixed precision)\n",
        "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard')\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=AdamW(\n",
        "        learning_rate=1e-3,  # 0.001 (higher than Adam's default)\n",
        "        weight_decay=1e-4    # Decoupled weight decay for better regularization\n",
        "    ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "params = model.count_params()\n",
        "size_mb = (params * 4) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úì Parameters: {params:,}\")\n",
        "print(f\"‚úì Size: {size_mb:.2f}MB (FP32), {size_mb/2:.2f}MB (FP16)\")\n",
        "print(\"‚úì Architecture: Functional API\")\n",
        "print(\"‚úì Optimizer: AdamW (lr=1e-3, weight_decay=1e-4)\")\n",
        "print(\"‚úì Mixed Precision: Enabled\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING (OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Mixed Precision: FP16\")\n",
        "print(f\"Data Pipeline: tf.data (prefetched)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n",
        "if TESTING_MODE:\n",
        "    print(\"\\n‚ö†Ô∏è  This was TESTING mode\")\n",
        "    print(\"   Set TESTING_MODE = False for full training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"\\nFinal: Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.h5')\n",
        "\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "config = {'vocab_size': vocab_size, 'sequence_length': SEQUENCE_LENGTH}\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/config.pkl', 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "print(\"‚úì Saved: gru_model.h5, tokenizer.pkl, config.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_word(text, top_k=5):\n",
        "    seq = tokenizer.texts_to_sequences([text.lower()])[0]\n",
        "    seq = seq[-SEQUENCE_LENGTH:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
        "    return [(tokenizer.index_word.get(i, ''), preds[i]*100) for i in top_idx]\n",
        "\n",
        "tests = [\"how are\", \"thank\", \"good morning\", \"see you\", \"i want to\"]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for text in tests:\n",
        "    print(f\"\\nInput: '{text}'\")\n",
        "    for i, (word, prob) in enumerate(predict_next_word(text), 1):\n",
        "        conf = \"üü¢\" if prob > 50 else \"üü°\" if prob > 20 else \"üî¥\"\n",
        "        print(f\"  {i}. {word:15s} {conf} {prob:5.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Converting to TFLite (GRU-compatible)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# GRU/LSTM requires SELECT_TF_OPS for dynamic tensor lists\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard TFLite ops\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # TensorFlow ops (for GRU)\n",
        "]\n",
        "\n",
        "# Disable tensor list lowering (required for GRU)\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# Optimize for size\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert\n",
        "print(\"Converting model (this may take a minute)...\")\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save\n",
        "tflite_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model.tflite'\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "size_mb = len(tflite_model) / (1024 * 1024)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úì TFLite model saved: {size_mb:.2f}MB\")\n",
        "print(f\"‚úì Path: {tflite_path}\")\n",
        "print(\"\\n‚ö†Ô∏è  Note: Model uses SELECT_TF_OPS for GRU support\")\n",
        "print(\"   This is normal and required for RNN layers\")\n",
        "print(\"\\nüéâ Training complete! Download from Google Drive.\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
