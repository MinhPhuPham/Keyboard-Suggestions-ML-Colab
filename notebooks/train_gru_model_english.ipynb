{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRU Keyboard Prediction Model (Hybrid)\n",
        "\n",
        "Train a GRU model for keyboard suggestions with **hybrid prediction**.\n",
        "\n",
        "**Supports 3 Tasks:**\n",
        "\n",
        "| Input Format | Method | Example |\n",
        "|--------------|--------|----------|\n",
        "| `text + space` | GRU Model | \"How are \" \u2192 you, they, we |\n",
        "| `partial word` | Vocabulary Filter | \"Hel\" \u2192 Hello, Help, Hell |\n",
        "| `typo word` | Edit Distance | \"Thers\" \u2192 There, These |\n",
        "\n",
        "**Why Hybrid?**\n",
        "- \u2705 GRU model: Best for context-aware next-word prediction\n",
        "- \u2705 Vocabulary filter: Instant, accurate word completion\n",
        "- \u2705 Edit distance: Handles any typo without training\n",
        "\n",
        "**Model Specifications:**\n",
        "- Architecture: GRU (Gated Recurrent Unit)\n",
        "- Parameters: ~10M\n",
        "- Model Size: 30-40MB (Keras), 15-20MB (TFLite)\n",
        "- Training Time: 5-10 minutes on GPU\n",
        "- Inference: <10ms on mobile\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions:**\n",
        "1. Runtime \u2192 Change runtime type \u2192 GPU (T4)\n",
        "2. Set `TESTING_MODE = True` for quick test\n",
        "3. Set `TESTING_MODE = False` for full training\n",
        "4. Run all cells in order\n",
        "5. Download TFLite for mobile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup directories\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define directories\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)\n",
        "\n",
        "print(f\"\u2713 Google Drive mounted\")\n",
        "print(f\"\u2713 Project directory: {DRIVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm\n",
        "print(\"\u2713 Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - OPTIMIZED FOR T4 GPU\n",
        "# ============================================================\n",
        "\n",
        "TESTING_MODE = True  # \u2190 Change to False for full training\n",
        "\n",
        "if TESTING_MODE:\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE\")\n",
        "    print(\"   - Dataset: keyboard_training_data.txt\")\n",
        "    print(\"   - Epochs: 2 (quick verification)\")\n",
        "    print(\"   - Time: ~1 min\")\n",
        "    NUM_EPOCHS = 2\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Limit vocab for speed\n",
        "    SEQUENCE_LENGTH = 10  # Better context\n",
        "else:\n",
        "    print(\"\u2713 FULL TRAINING MODE\")\n",
        "    print(\"   - Dataset: Fake.csv + True.csv + 1661-0.txt\")\n",
        "    print(\"   - Epochs: 20\")\n",
        "    print(\"   - Time: ~8-10 min (with optimizations)\")\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 512  # Optimized for T4 GPU\n",
        "    VOCAB_SIZE_LIMIT = 25000  # Smaller model, faster inference\n",
        "    SEQUENCE_LENGTH = 10  # Better predictions\n",
        "\n",
        "print(f\"\\nOptimizations:\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE} (maximizes GPU)\")\n",
        "print(f\"  - Vocab limit: {VOCAB_SIZE_LIMIT:,} (reduces model size)\")\n",
        "print(f\"  - Sequence length: {SEQUENCE_LENGTH} (better context)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Datasets in Google Drive\n",
        "\n",
        "**Required datasets in Google Drive:**\n",
        "- `{DRIVE_DIR}/datasets/Fake.csv` - Fake news dataset\n",
        "- `{DRIVE_DIR}/datasets/True.csv` - True news dataset\n",
        "- `{DRIVE_DIR}/datasets/1661-0.txt` - Next-word prediction corpus\n",
        "\n",
        "Upload these files to your Google Drive before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking datasets in Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define dataset paths\n",
        "FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "\n",
        "# Check each dataset\n",
        "datasets_ok = True\n",
        "\n",
        "for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                    (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                    (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"\u2713 {name}: {size:.2f}MB\")\n",
        "    else:\n",
        "        print(f\"\u2717 Missing: {name}\")\n",
        "        print(f\"   Expected at: {path}\")\n",
        "        datasets_ok = False\n",
        "\n",
        "if not datasets_ok:\n",
        "    print(\"\\n\u26a0\ufe0f  Please upload missing datasets to Google Drive!\")\n",
        "    print(f\"   Upload to: {DRIVE_DIR}/datasets/\")\n",
        "    raise FileNotFoundError(\"Required datasets not found in Google Drive\")\n",
        "else:\n",
        "    print(\"\\n\u2705 All datasets found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading datasets from Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_text = []\n",
        "\n",
        "if TESTING_MODE:\n",
        "    # Testing mode: Use keyboard_training_data.txt (smaller, faster)\n",
        "    print(\"\u26a0\ufe0f  TESTING MODE: Using keyboard_training_data.txt\")\n",
        "    \n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "    \n",
        "    if not os.path.exists(CORPUS_PATH):\n",
        "        print(f\"\\n\u2717 Missing: keyboard_training_data.txt\")\n",
        "        print(f\"   Expected at: {CORPUS_PATH}\")\n",
        "        raise FileNotFoundError(\"keyboard_training_data.txt not found\")\n",
        "    \n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "    \n",
        "    all_text.append(corpus_text)\n",
        "    print(f\"\u2713 Loaded: {len(corpus_text):,} characters\")\n",
        "    \n",
        "else:\n",
        "    # Full training mode: Use Fake.csv + True.csv + 1661-0.txt\n",
        "    print(\"\u2713 FULL TRAINING: Using Fake.csv + True.csv + 1661-0.txt\")\n",
        "    \n",
        "    FAKE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/Fake.csv\"\n",
        "    TRUE_NEWS_PATH = f\"{DRIVE_DIR}/datasets/True.csv\"\n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/1661-0.txt\"\n",
        "    \n",
        "    # Check files exist\n",
        "    for name, path in [(\"Fake.csv\", FAKE_NEWS_PATH), \n",
        "                        (\"True.csv\", TRUE_NEWS_PATH),\n",
        "                        (\"1661-0.txt\", CORPUS_PATH)]:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"\\n\u2717 Missing: {name}\")\n",
        "            print(f\"   Expected at: {path}\")\n",
        "            raise FileNotFoundError(f\"{name} not found\")\n",
        "    \n",
        "    # Load fake news\n",
        "    fake_df = pd.read_csv(FAKE_NEWS_PATH)\n",
        "    true_df = pd.read_csv(TRUE_NEWS_PATH)\n",
        "    \n",
        "    print(f\"\u2713 Loaded {len(fake_df):,} fake news articles\")\n",
        "    print(f\"\u2713 Loaded {len(true_df):,} true news articles\")\n",
        "    \n",
        "    all_text.extend(fake_df['text'].tolist())\n",
        "    all_text.extend(true_df['text'].tolist())\n",
        "    \n",
        "    # Load corpus\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        corpus_text = f.read()\n",
        "        all_text.append(corpus_text)\n",
        "    \n",
        "    print(f\"\u2713 Loaded corpus: {len(corpus_text):,} characters\")\n",
        "\n",
        "# Combine and clean\n",
        "combined_text = ' '.join(all_text).lower()\n",
        "combined_text = combined_text.replace('\\n', ' ')\n",
        "combined_text = ' '.join(combined_text.split())\n",
        "\n",
        "print(f\"\\n\u2713 Total: {len(combined_text):,} characters\")\n",
        "print(f\"\u2713 Sample: {combined_text[:200]}...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize and Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"Tokenizing with vocabulary limit...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize with vocab limit\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE_LIMIT)\n",
        "tokenizer.fit_on_texts([combined_text])\n",
        "\n",
        "vocab_size = min(len(tokenizer.word_index) + 1, VOCAB_SIZE_LIMIT)\n",
        "\n",
        "print(f\"\u2713 Total unique words: {len(tokenizer.word_index):,}\")\n",
        "print(f\"\u2713 Vocabulary size (limited): {vocab_size:,}\")\n",
        "\n",
        "# Convert to sequences\n",
        "sequences = tokenizer.texts_to_sequences([combined_text])[0]\n",
        "\n",
        "print(f\"\\nCreating optimized tf.data pipeline...\")\n",
        "\n",
        "sequences_array = np.array(sequences)\n",
        "\n",
        "# Create dataset using timeseries for next-word prediction\n",
        "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=sequences_array[:-1],\n",
        "    targets=sequences_array[SEQUENCE_LENGTH:],\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    sequence_stride=1,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Calculate total steps\n",
        "total_sequences = len(sequences) - SEQUENCE_LENGTH\n",
        "total_steps = total_sequences // BATCH_SIZE\n",
        "\n",
        "# Split: 90% train, 10% validation\n",
        "val_steps = max(1, total_steps // 10)\n",
        "train_steps = total_steps - val_steps\n",
        "\n",
        "# Split dataset with proper steps\n",
        "train_dataset = dataset.take(train_steps).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = dataset.skip(train_steps).take(val_steps).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"\u2713 Total sequences: {total_sequences:,}\")\n",
        "print(f\"\u2713 Total steps: {total_steps:,}\")\n",
        "print(f\"\u2713 Train steps: {train_steps:,} (90%)\")\n",
        "print(f\"\u2713 Val steps: {val_steps:,} (10%)\")\n",
        "print(f\"\u2713 Batch size: {BATCH_SIZE}\")\n",
        "print(f\"\u2713 Prefetching: Enabled\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n\ud83d\udcdd Note: GRU model trains on NEXT-WORD prediction only.\")\n",
        "print(\"   Word completion & typo correction use vocabulary + edit distance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable Mixed Precision for T4 GPU (2x faster training)\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PERFORMANCE OPTIMIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u2713 Mixed Precision enabled (FP16)\")\n",
        "print(\"  - Training speed: ~2x faster\")\n",
        "print(\"  - Memory usage: ~40% less\")\n",
        "print(\"  - Accuracy: Same as FP32\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "print(\"Building GRU model (Functional API + Mixed Precision)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
        "\n",
        "# Embedding layer\n",
        "x = Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=128,\n",
        "    name='embedding'\n",
        ")(inputs)\n",
        "\n",
        "# GRU layer\n",
        "x = GRU(\n",
        "    units=256,\n",
        "    dropout=0.2,\n",
        "    recurrent_dropout=0.2,\n",
        "    name='gru'\n",
        ")(x)\n",
        "\n",
        "# Dropout\n",
        "x = Dropout(0.3, name='dropout')(x)\n",
        "\n",
        "# Output layer (dtype=float32 for numerical stability with mixed precision)\n",
        "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "# Create model\n",
        "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard')\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=AdamW(\n",
        "        learning_rate=1e-3,  # 0.001 (higher than Adam's default)\n",
        "        weight_decay=1e-4    # Decoupled weight decay for better regularization\n",
        "    ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "params = model.count_params()\n",
        "size_mb = (params * 4) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL INFO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 Parameters: {params:,}\")\n",
        "print(f\"\u2713 Size: {size_mb:.2f}MB (FP32), {size_mb/2:.2f}MB (FP16)\")\n",
        "print(\"\u2713 Architecture: Functional API\")\n",
        "print(\"\u2713 Optimizer: AdamW (lr=1e-3, weight_decay=1e-4)\")\n",
        "print(\"\u2713 Mixed Precision: Enabled\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING (OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Mixed Precision: FP16\")\n",
        "print(f\"Data Pipeline: tf.data (prefetched)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "if TESTING_MODE:\n",
        "    print(\"\\n\u26a0\ufe0f  This was TESTING mode\")\n",
        "    print(\"   Set TESTING_MODE = False for full training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "print(f\"\\nFinal: Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras')\n",
        "\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "config = {'vocab_size': vocab_size, 'sequence_length': SEQUENCE_LENGTH}\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/config.pkl', 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "print(\"\u2713 Saved: gru_model.keras, tokenizer.pkl, config.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HYBRID PREDICTION SYSTEM\n",
        "# ============================================================\n",
        "# - GRU model: next-word prediction (ends with space)\n",
        "# - Vocabulary filter: word completion (partial word)\n",
        "# - Edit distance: typo correction (misspelled word)\n",
        "# ============================================================\n",
        "\n",
        "def edit_distance(s1, s2):\n",
        "    \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return edit_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    \n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    \n",
        "    return previous_row[-1]\n",
        "\n",
        "\n",
        "# Build vocabulary list with word frequencies\n",
        "vocab_list = sorted(tokenizer.word_index.items(), key=lambda x: x[1])[:VOCAB_SIZE_LIMIT]\n",
        "vocab_words = [word for word, idx in vocab_list]\n",
        "print(f\"\u2713 Vocabulary loaded: {len(vocab_words):,} words\")\n",
        "\n",
        "\n",
        "def predict_next_word(context, top_k=5):\n",
        "    \"\"\"Use GRU model for next-word prediction\"\"\"\n",
        "    seq = tokenizer.texts_to_sequences([context.lower()])[0]\n",
        "    seq = seq[-SEQUENCE_LENGTH:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
        "    return [(tokenizer.index_word.get(i, ''), preds[i]*100) for i in top_idx if i in tokenizer.index_word]\n",
        "\n",
        "\n",
        "def complete_word(partial, top_k=5):\n",
        "    \"\"\"Use vocabulary filter for word completion\"\"\"\n",
        "    partial = partial.lower()\n",
        "    candidates = []\n",
        "    \n",
        "    for word in vocab_words:\n",
        "        if word.startswith(partial) and word != partial:\n",
        "            # Score by word frequency (lower index = more common)\n",
        "            idx = tokenizer.word_index.get(word, 999999)\n",
        "            score = 100 / (idx + 1)  # Higher score for common words\n",
        "            candidates.append((word, score))\n",
        "    \n",
        "    # Sort by score (word frequency)\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[:top_k]\n",
        "\n",
        "\n",
        "def correct_typo(typo, top_k=5):\n",
        "    \"\"\"Use edit distance for typo correction\"\"\"\n",
        "    typo = typo.lower()\n",
        "    candidates = []\n",
        "    \n",
        "    # Only check words with similar length\n",
        "    for word in vocab_words[:10000]:  # Top 10k words for speed\n",
        "        if abs(len(word) - len(typo)) <= 2:\n",
        "            dist = edit_distance(word, typo)\n",
        "            if dist <= 2 and word != typo:  # Max 2 character difference\n",
        "                # Score: lower distance = higher score\n",
        "                idx = tokenizer.word_index.get(word, 999999)\n",
        "                score = (100 / (dist + 1)) * (100 / (idx + 1))\n",
        "                candidates.append((word, score, dist))\n",
        "    \n",
        "    # Sort by score\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [(w, s, d) for w, s, d in candidates[:top_k]]\n",
        "\n",
        "\n",
        "def predict_hybrid(input_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Hybrid prediction system:\n",
        "    - \"text \" (ends with space) \u2192 GRU next-word prediction\n",
        "    - \"tex\" (partial word) \u2192 Vocabulary completion\n",
        "    - \"txet\" (typo) \u2192 Edit distance correction\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    if input_text.endswith(' '):\n",
        "        # NEXT-WORD PREDICTION (GRU model)\n",
        "        task = 'next_word'\n",
        "        preds = predict_next_word(input_text.strip(), top_k)\n",
        "        results = [(word, prob, task) for word, prob in preds]\n",
        "    \n",
        "    else:\n",
        "        words = input_text.split()\n",
        "        if len(words) > 1:\n",
        "            # Context + partial/typo: \"How are ther\"\n",
        "            context = ' '.join(words[:-1])\n",
        "            partial = words[-1]\n",
        "        else:\n",
        "            # Just partial: \"Hel\"\n",
        "            context = \"\"\n",
        "            partial = input_text\n",
        "        \n",
        "        # Try COMPLETION first\n",
        "        completions = complete_word(partial, top_k)\n",
        "        \n",
        "        if completions:\n",
        "            # Found completions\n",
        "            task = 'completion'\n",
        "            results = [(word, score, task) for word, score in completions]\n",
        "        else:\n",
        "            # No completions, try TYPO CORRECTION\n",
        "            task = 'typo'\n",
        "            corrections = correct_typo(partial, top_k)\n",
        "            results = [(word, score, task) for word, score, dist in corrections]\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST HYBRID PREDICTIONS\n",
        "# ============================================================\n",
        "\n",
        "test_cases = [\n",
        "    # Next-word prediction (with space) \u2192 GRU model\n",
        "    (\"How are \", \"Next-word (GRU)\"),\n",
        "    (\"Thank \", \"Next-word (GRU)\"),\n",
        "    (\"I want to \", \"Next-word (GRU)\"),\n",
        "    (\"Good morning \", \"Next-word (GRU)\"),\n",
        "    \n",
        "    # Word completion (partial word) \u2192 Vocabulary filter\n",
        "    (\"Hel\", \"Completion (Vocab)\"),\n",
        "    (\"Tha\", \"Completion (Vocab)\"),\n",
        "    (\"Goo\", \"Completion (Vocab)\"),\n",
        "    (\"Mor\", \"Completion (Vocab)\"),\n",
        "    (\"bea\", \"Completion (Vocab)\"),\n",
        "    \n",
        "    # Typo correction (misspelled) \u2192 Edit distance\n",
        "    (\"thers\", \"Typo (Edit Dist)\"),\n",
        "    (\"teh\", \"Typo (Edit Dist)\"),\n",
        "    (\"helo\", \"Typo (Edit Dist)\"),\n",
        "    (\"recieve\", \"Typo (Edit Dist)\"),\n",
        "    \n",
        "    # Combined context + partial/typo\n",
        "    (\"How are yo\", \"Context + Completion\"),\n",
        "    (\"I want to goe\", \"Context + Typo\"),\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HYBRID PREDICTION TESTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for input_text, test_type in test_cases:\n",
        "    print(f\"\\n\ud83d\udcdd Input: '{input_text}' ({test_type})\")\n",
        "    predictions = predict_hybrid(input_text, top_k=5)\n",
        "    \n",
        "    if not predictions:\n",
        "        print(\"   (no predictions)\")\n",
        "        continue\n",
        "    \n",
        "    for i, (word, score, task) in enumerate(predictions, 1):\n",
        "        if score > 50:\n",
        "            emoji = \"\ud83d\udfe2\"\n",
        "        elif score > 10:\n",
        "            emoji = \"\ud83d\udfe1\"\n",
        "        else:\n",
        "            emoji = \"\ud83d\udd34\"\n",
        "        \n",
        "        print(f\"  {i}. {word:15s} {emoji} {score:5.1f} [{task}]\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Next-word: Uses trained GRU model (context-aware)\")\n",
        "print(\"  - Completion: Uses vocabulary filter (instant, accurate)\")\n",
        "print(\"  - Typo: Uses edit distance (handles any typo)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Converting to TFLite (GRU-compatible)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# GRU/LSTM requires SELECT_TF_OPS for dynamic tensor lists\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard TFLite ops\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # TensorFlow ops (for GRU)\n",
        "]\n",
        "\n",
        "# Disable tensor list lowering (required for GRU)\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# Optimize for size\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert\n",
        "print(\"Converting model (this may take a minute)...\")\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save\n",
        "tflite_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model.tflite'\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "size_mb = len(tflite_model) / (1024 * 1024)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2713 TFLite model saved: {size_mb:.2f}MB\")\n",
        "print(f\"\u2713 Path: {tflite_path}\")\n",
        "print(\"\\n\u26a0\ufe0f  Note: Model uses SELECT_TF_OPS for GRU support\")\n",
        "print(\"   This is normal and required for RNN layers\")\n",
        "print(\"\\n\ud83c\udf89 Training complete! Download from Google Drive.\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}