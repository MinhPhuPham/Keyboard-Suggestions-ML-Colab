{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "custom_model_title"
            },
            "source": [
                "# Custom Keyboard Transformer Model Training\n",
                "\n",
                "Train a lightweight custom transformer (3.7M params) optimized for keyboard suggestions.\n",
                "\n",
                "**Features:**\n",
                "1. Word Completion: \"hel\" ‚Üí [\"hello\", \"help\", \"held\"]\n",
                "2. Next-Word Prediction: \"how are\" ‚Üí [\"you\", \"they\", \"we\"]\n",
                "3. Typo Correction: \"thers\" ‚Üí [\"there\", \"theirs\"]\n",
                "\n",
                "**Model Specifications:**\n",
                "- Architecture: Custom Transformer (6 layers, 128 hidden, 4 heads)\n",
                "- Parameters: 3.7M\n",
                "- Vocabulary: 10,000 words (keyboard-optimized)\n",
                "- Model Size: 14MB (FP32), 4MB (INT8)\n",
                "- Expected Accuracy: 80-85%\n",
                "- Training Time: 30-40 minutes on Colab GPU (T4)\n",
                "\n",
                "**Why Custom Model?**\n",
                "- ‚úÖ Small vocabulary (10k vs 50k) = Better learning\n",
                "- ‚úÖ Trained from scratch on keyboard data\n",
                "- ‚úÖ Optimized for mobile deployment\n",
                "\n",
                "---\n",
                "\n",
                "**Instructions:**\n",
                "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
                "2. Run all cells in order\n",
                "3. Model will be saved to Google Drive\n",
                "4. Download CoreML/TFLite for mobile deployment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_section"
            },
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mount_drive"
            },
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Define directories\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "DATA_DIR = f\"{DRIVE_DIR}/data/datasets\"\n",
                "PROCESSED_DIR = f\"{DRIVE_DIR}/data/processed\"\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/custom_keyboard\"\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úì Google Drive mounted\")\n",
                "print(f\"‚úì Data directory: {DATA_DIR}\")\n",
                "print(f\"‚úì Model directory: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q torch transformers tqdm coremltools tensorflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "# Clone repository to get custom model code\n",
                "!git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git /content/repo\n",
                "\n",
                "# Copy custom model scripts\n",
                "import shutil\n",
                "shutil.copytree('/content/repo/scripts/custom-model', '/content/custom_model', dirs_exist_ok=True)\n",
                "\n",
                "print(\"‚úì Repository cloned\")\n",
                "print(\"‚úì Custom model code copied to /content/custom_model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data_section"
            },
            "source": [
                "## 2. Verify Datasets\n",
                "\n",
                "Upload these files to Google Drive at `Keyboard-Suggestions-ML-Colab/data/datasets/`:\n",
                "- `single_word_freq.csv`\n",
                "- `keyboard_training_data.txt`\n",
                "- `misspelled.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify_data"
            },
            "outputs": [],
            "source": [
                "# Verify datasets exist\n",
                "required_files = [\n",
                "    f\"{DATA_DIR}/single_word_freq.csv\",\n",
                "    f\"{DATA_DIR}/keyboard_training_data.txt\",\n",
                "    f\"{DATA_DIR}/misspelled.csv\"\n",
                "]\n",
                "\n",
                "print(\"Checking datasets...\")\n",
                "\n",
                "all_exist = True\n",
                "for file_path in required_files:\n",
                "    exists = os.path.exists(file_path)\n",
                "    status = \"‚úì\" if exists else \"‚ùå\"\n",
                "    print(f\"{status} {os.path.basename(file_path)}: {exists}\")\n",
                "    if not exists:\n",
                "        all_exist = False\n",
                "\n",
                "if not all_exist:\n",
                "    print(\"\\n‚ö†Ô∏è  Please upload missing datasets to Google Drive!\")\n",
                "    print(f\"   Upload to: {DATA_DIR}/\")\n",
                "else:\n",
                "    print(\"\\n‚úÖ All datasets found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "prepare_section"
            },
            "source": [
                "## 3. Prepare Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "prepare_data"
            },
            "outputs": [],
            "source": [
                "# Import data preparation script\n",
                "import sys\n",
                "sys.path.insert(0, '/content/custom_model')\n",
                "\n",
                "# Run data preparation\n",
                "!cd /content/custom_model && python prepare_data.py \\\n",
                "    --data-dir {DATA_DIR} \\\n",
                "    --output-dir {PROCESSED_DIR} \\\n",
                "    --max-completion 50000 \\\n",
                "    --max-nextword 100000 \\\n",
                "    --max-typo 20000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_section"
            },
            "source": [
                "## 4. Train Custom Model\n",
                "\n",
                "Training will take approximately 30-40 minutes on GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_model"
            },
            "outputs": [],
            "source": [
                "# Train model\n",
                "!cd /content/custom_model && python train.py \\\n",
                "    --data-dir {PROCESSED_DIR} \\\n",
                "    --save-dir {MODEL_DIR} \\\n",
                "    --num-epochs 20 \\\n",
                "    --batch-size 64 \\\n",
                "    --device cuda"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "test_section"
            },
            "source": [
                "## 5. Test Model - 10 Test Cases\n",
                "\n",
                "Test the trained model with 10 cases covering all features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_model"
            },
            "outputs": [],
            "source": [
                "# Load trained model for testing\n",
                "import torch\n",
                "from tokenizer import KeyboardTokenizer\n",
                "from model import KeyboardTransformer\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = KeyboardTokenizer.load(f\"{MODEL_DIR}/tokenizer.pkl\")\n",
                "\n",
                "# Load model\n",
                "model = KeyboardTransformer(\n",
                "    vocab_size=len(tokenizer),\n",
                "    hidden_size=256,\n",
                "    num_layers=8,\n",
                "    num_heads=8,\n",
                "    ff_dim=1024,\n",
                "    max_length=16\n",
                ")\n",
                "\n",
                "checkpoint = torch.load(f\"{MODEL_DIR}/best_model.pt\", map_location='cuda')\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "model = model.to('cuda')\n",
                "model.eval()\n",
                "\n",
                "print(\"‚úì Model loaded successfully\")\n",
                "print(f\"‚úì Val Loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
                "print(f\"‚úì Val Accuracy: {checkpoint.get('val_accuracy', 0)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_function"
            },
            "outputs": [],
            "source": [
                "# Test function\n",
                "def test_prediction(input_text, top_k=5):\n",
                "    \"\"\"Test model prediction\"\"\"\n",
                "    # Encode input\n",
                "    input_ids = tokenizer.encode(input_text, max_length=15, padding=False)\n",
                "    input_ids.append(tokenizer.mask_token_id)\n",
                "    \n",
                "    while len(input_ids) < 16:\n",
                "        input_ids.append(tokenizer.pad_token_id)\n",
                "    \n",
                "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to('cuda')\n",
                "    attention_mask = torch.tensor(\n",
                "        [[1 if idx != tokenizer.pad_token_id else 0 for idx in input_ids]],\n",
                "        dtype=torch.long\n",
                "    ).to('cuda')\n",
                "    \n",
                "    # Predict\n",
                "    with torch.no_grad():\n",
                "        top_tokens, top_probs = model.predict(input_tensor, attention_mask, top_k=top_k*2)\n",
                "    \n",
                "    # Decode\n",
                "    predictions = []\n",
                "    for token_id, prob in zip(top_tokens[0], top_probs[0]):\n",
                "        word = tokenizer.idx2word.get(token_id.item(), tokenizer.unk_token)\n",
                "        if word not in [tokenizer.pad_token, tokenizer.unk_token, tokenizer.mask_token]:\n",
                "            predictions.append((word, prob.item() * 100))\n",
                "        if len(predictions) >= top_k:\n",
                "            break\n",
                "    \n",
                "    return predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_tests"
            },
            "outputs": [],
            "source": [
                "# 10 Test Cases\n",
                "test_cases = [\n",
                "    # Word Completion (4 cases)\n",
                "    (\"hel\", \"Word Completion\", \"Should suggest: hello, help, held\"),\n",
                "    (\"prod\", \"Word Completion\", \"Should suggest: product, production, produce\"),\n",
                "    (\"beau\", \"Word Completion\", \"Should suggest: beautiful, beauty, because\"),\n",
                "    (\"comp\", \"Word Completion\", \"Should suggest: complete, computer, company\"),\n",
                "    \n",
                "    # Next-Word Prediction (4 cases)\n",
                "    (\"how are\", \"Next-Word Prediction\", \"Should suggest: you, they, we\"),\n",
                "    (\"thank\", \"Next-Word Prediction\", \"Should suggest: you, for, god\"),\n",
                "    (\"good morning\", \"Next-Word Prediction\", \"Should suggest: to, and, everyone\"),\n",
                "    (\"see you\", \"Next-Word Prediction\", \"Should suggest: later, soon, tomorrow\"),\n",
                "    \n",
                "    # Typo Correction (2 cases)\n",
                "    (\"thers\", \"Typo Correction\", \"Should suggest: there, theirs\"),\n",
                "    (\"recieve\", \"Typo Correction\", \"Should suggest: receive\"),\n",
                "]\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"TESTING CUSTOM KEYBOARD MODEL - 10 TEST CASES\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for i, (input_text, task, expected) in enumerate(test_cases, 1):\n",
                "    print(f\"\\nTest {i}/10: {task}\")\n",
                "    print(f\"Input: '{input_text}'\")\n",
                "    print(f\"Expected: {expected}\")\n",
                "    \n",
                "    predictions = test_prediction(input_text, top_k=3)\n",
                "    \n",
                "    print(\"Predictions:\")\n",
                "    if predictions:\n",
                "        for j, (word, prob) in enumerate(predictions, 1):\n",
                "            confidence = \"üü¢\" if prob > 50 else \"üü°\" if prob > 20 else \"üî¥\"\n",
                "            print(f\"  {j}. {word:15s} {confidence} {prob:5.1f}%\")\n",
                "    else:\n",
                "        print(\"  (no predictions)\")\n",
                "    \n",
                "    print(\"-\" * 80)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ ALL TESTS COMPLETE\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "export_section"
            },
            "source": [
                "## 6. Export to CoreML (iOS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "export_coreml"
            },
            "outputs": [],
            "source": [
                "# Export to CoreML\n",
                "import coremltools as ct\n",
                "import numpy as np\n",
                "\n",
                "print(\"Exporting to CoreML...\")\n",
                "\n",
                "# Prepare model for export\n",
                "model.eval()\n",
                "model = model.to('cpu')\n",
                "\n",
                "# Create dummy input\n",
                "dummy_input = torch.randint(0, len(tokenizer), (1, 16))\n",
                "\n",
                "# Trace model\n",
                "traced_model = torch.jit.trace(model, dummy_input)\n",
                "\n",
                "# Convert to CoreML\n",
                "mlmodel = ct.convert(\n",
                "    traced_model,\n",
                "    inputs=[ct.TensorType(name=\"input_ids\", shape=(1, 16), dtype=np.int32)],\n",
                "    compute_units=ct.ComputeUnit.ALL,\n",
                "    compute_precision=ct.precision.FLOAT16,\n",
                "    minimum_deployment_target=ct.target.iOS14\n",
                ")\n",
                "\n",
                "# Add metadata\n",
                "mlmodel.author = \"MinhPhuPham\"\n",
                "mlmodel.short_description = \"Custom keyboard transformer model\"\n",
                "mlmodel.version = \"1.0\"\n",
                "\n",
                "# Quantize to INT8\n",
                "print(\"Quantizing to INT8...\")\n",
                "import coremltools.optimize.coreml as cto\n",
                "\n",
                "op_config = cto.OpLinearQuantizerConfig(\n",
                "    mode=\"linear_symmetric\",\n",
                "    dtype=\"int8\",\n",
                "    granularity=\"per_channel\"\n",
                ")\n",
                "config = cto.OptimizationConfig(global_config=op_config)\n",
                "mlmodel_int8 = cto.linear_quantize_weights(mlmodel, config=config)\n",
                "\n",
                "# Save\n",
                "coreml_path = f\"{MODEL_DIR}/CustomKeyboard.mlpackage\"\n",
                "mlmodel_int8.save(coreml_path)\n",
                "\n",
                "print(f\"‚úì CoreML model saved: {coreml_path}\")\n",
                "print(f\"‚úì Model size: ~4-6MB (INT8)\")\n",
                "print(f\"‚úì Expected RAM: 12-15MB\")\n",
                "print(f\"‚úì Expected latency: <50ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_vocab_ios"
            },
            "outputs": [],
            "source": [
                "# Save vocabulary for iOS\n",
                "import json\n",
                "\n",
                "vocab_data = {\n",
                "    'word2idx': tokenizer.word2idx,\n",
                "    'idx2word': {str(k): v for k, v in tokenizer.idx2word.items()},\n",
                "    'vocab_size': len(tokenizer),\n",
                "    'pad_token_id': tokenizer.pad_token_id,\n",
                "    'unk_token_id': tokenizer.unk_token_id,\n",
                "    'mask_token_id': tokenizer.mask_token_id\n",
                "}\n",
                "\n",
                "vocab_path = f\"{MODEL_DIR}/vocabulary.json\"\n",
                "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"‚úì Vocabulary saved: {vocab_path}\")\n",
                "print(f\"‚úì Vocab size: {len(tokenizer):,} words\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "export_android_section"
            },
            "source": [
                "## 7. Export to TFLite (Android)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "export_tflite"
            },
            "outputs": [],
            "source": [
                "# Export to TFLite\n",
                "import tensorflow as tf\n",
                "\n",
                "print(\"Exporting to TFLite...\")\n",
                "\n",
                "# Convert traced model to ONNX first\n",
                "onnx_path = f\"{MODEL_DIR}/custom_keyboard.onnx\"\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    dummy_input,\n",
                "    onnx_path,\n",
                "    input_names=['input_ids'],\n",
                "    output_names=['logits'],\n",
                "    dynamic_axes={\n",
                "        'input_ids': {0: 'batch_size'},\n",
                "        'logits': {0: 'batch_size'}\n",
                "    },\n",
                "    opset_version=12\n",
                ")\n",
                "\n",
                "print(f\"‚úì ONNX model saved: {onnx_path}\")\n",
                "\n",
                "# Note: Full ONNX ‚Üí TFLite conversion requires onnx-tf package\n",
                "# For now, we'll save the ONNX model which can be converted separately\n",
                "print(\"\\nüìù Note: Convert ONNX to TFLite using:\")\n",
                "print(\"   pip install onnx-tf\")\n",
                "print(\"   onnx-tf convert -i custom_keyboard.onnx -o custom_keyboard.pb\")\n",
                "print(\"   Then use TensorFlow Lite converter\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summary_section"
            },
            "source": [
                "## 8. Training Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "show_summary"
            },
            "outputs": [],
            "source": [
                "# Display training summary\n",
                "import json\n",
                "\n",
                "history_path = f\"{MODEL_DIR}/training_history.json\"\n",
                "if os.path.exists(history_path):\n",
                "    with open(history_path, 'r') as f:\n",
                "        history = json.load(f)\n",
                "    \n",
                "    print(\"=\"*80)\n",
                "    print(\"TRAINING SUMMARY\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    print(f\"\\nFinal Results:\")\n",
                "    print(f\"  Train Loss: {history['train_loss'][-1]:.4f}\")\n",
                "    print(f\"  Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
                "    print(f\"  Val Accuracy: {history['val_accuracy'][-1]*100:.2f}%\")\n",
                "    \n",
                "    print(f\"\\nBest Results:\")\n",
                "    best_val_loss = min(history['val_loss'])\n",
                "    best_epoch = history['val_loss'].index(best_val_loss) + 1\n",
                "    print(f\"  Best Val Loss: {best_val_loss:.4f} (Epoch {best_epoch})\")\n",
                "    print(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
                "    \n",
                "    print(f\"\\nModel Files:\")\n",
                "    print(f\"  PyTorch Model: {MODEL_DIR}/best_model.pt\")\n",
                "    print(f\"  CoreML Model: {MODEL_DIR}/CustomKeyboard.mlpackage\")\n",
                "    print(f\"  ONNX Model: {MODEL_DIR}/custom_keyboard.onnx\")\n",
                "    print(f\"  Vocabulary: {MODEL_DIR}/vocabulary.json\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
                "    print(\"=\"*80)\n",
                "    print(\"\\nNext Steps:\")\n",
                "    print(\"1. Download models from Google Drive\")\n",
                "    print(\"2. Integrate CoreML model into iOS app\")\n",
                "    print(\"3. Convert ONNX to TFLite for Android\")\n",
                "    print(\"4. Test on actual devices\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Training history not found. Training may not have completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download_section"
            },
            "source": [
                "## 9. Download Models\n",
                "\n",
                "Download these files from Google Drive for mobile deployment:\n",
                "\n",
                "**For iOS:**\n",
                "- `CustomKeyboard.mlpackage` (CoreML model)\n",
                "- `vocabulary.json` (Vocabulary file)\n",
                "\n",
                "**For Android:**\n",
                "- `custom_keyboard.onnx` (ONNX model - convert to TFLite)\n",
                "- `vocabulary.json` (Vocabulary file)\n",
                "\n",
                "**Location:** `Google Drive/Keyboard-Suggestions-ML-Colab/models/custom_keyboard/`"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
