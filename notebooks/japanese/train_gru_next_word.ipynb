{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Next Word Predictor v2 ‚Äî Top-K Suggestions\n",
                "\n",
                "**Supports**: Google Colab & Kaggle (Multi-GPU)\n",
                "\n",
                "**Task**: Predict top-K next words from Japanese word context\n",
                "- Input: `[„ÅÇ„Çä„Åå„Å®„ÅÜ]` ‚Üí Top-K: `„Åî„Åñ„ÅÑ„Åæ„Åô`, `„Åî„Åñ„ÅÑ„Åæ„Åó„Åü`, `„Å≠`\n",
                "- Input: `[‰ªäÊó•, „ÅØ]` ‚Üí Top-K: `Â§©Ê∞ó`, `ËâØ„ÅÑ`, `„Å®„Å¶„ÇÇ`\n",
                "\n",
                "**Same approach as English keyboard model**:\n",
                "- 1 model call ‚Üí top-K next words (instant, no looping)\n",
                "- Fast on mobile: single forward pass\n",
                "\n",
                "**Architecture**: Bi-GRU + Self-Attention + Context GRU\n",
                "\n",
                "**Testing workflow**:\n",
                "1. Set `TESTING_MODE = True` ‚Üí 100K samples, 10 epochs (quality check)\n",
                "2. Train ‚Üí verify loss ‚Üì and accuracy ‚Üë\n",
                "3. Verify with real meaningful sentences from dataset\n",
                "4. Set `TESTING_MODE = False` ‚Üí full training (5M samples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "\n",
                "# Auto-detect platform (Colab check first - Colab also has /kaggle dir!)\n",
                "if 'COLAB_RELEASE_TAG' in os.environ:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Local'\n",
                "    DRIVE_DIR = './output'\n",
                "\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_next_word\"\n",
                "CACHE_DIR = f\"{DRIVE_DIR}/cache/nwp\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Model: {MODEL_DIR}\")\n",
                "print(f\"üíæ Cache: {CACHE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm fugashi unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# ===========================================================\n",
                "# MULTI-GPU + MIXED PRECISION\n",
                "# ===========================================================\n",
                "strategy = tf.distribute.MirroredStrategy()\n",
                "NUM_GPUS = strategy.num_replicas_in_sync\n",
                "print(f\"üî• GPUs available: {NUM_GPUS}\")\n",
                "\n",
                "# Mixed precision: T4 has good FP16 Tensor Cores\n",
                "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
                "print(f\"‚ö° Mixed precision: {tf.keras.mixed_precision.global_policy().name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "# ‚ö†Ô∏è Set True for quality check (100K samples, 10 epochs)\n",
                "#    This is enough data for the model to learn real patterns.\n",
                "# Set False for full production training (5M samples, 15 epochs)\n",
                "TESTING_MODE = True\n",
                "\n",
                "if TESTING_MODE:\n",
                "    MAX_SAMPLES = 100_000\n",
                "    MAX_NWP_PAIRS = 500_000\n",
                "    NUM_EPOCHS = 10\n",
                "    CACHE_SUFFIX = '_test'\n",
                "    print(\"‚ö†Ô∏è TESTING MODE: 100K samples, 10 epochs\")\n",
                "else:\n",
                "    MAX_SAMPLES = 5_000_000\n",
                "    MAX_NWP_PAIRS = 8_000_000\n",
                "    NUM_EPOCHS = 15\n",
                "    CACHE_SUFFIX = ''\n",
                "    print(\"üöÄ FULL TRAINING: 5M samples, 15 epochs\")\n",
                "\n",
                "BATCH_SIZE = 512 * NUM_GPUS  # Scale batch with GPUs (512 per GPU)\n",
                "FORCE_REBUILD_CACHE = False\n",
                "\n",
                "# Word-level model config\n",
                "WORD_VOCAB_SIZE = 6000\n",
                "MAX_WORD_CONTEXT = 10  # Max words in context (left-padded)\n",
                "EMBEDDING_DIM = 96\n",
                "GRU_UNITS = 192\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
                "PAD_IDX = 0\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, batch={BATCH_SIZE} ({BATCH_SIZE//NUM_GPUS}/GPU)\")\n",
                "print(f\"Model: vocab={WORD_VOCAB_SIZE}, embed={EMBEDDING_DIM}, GRU={GRU_UNITS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Shared Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import fugashi\n",
                "\n",
                "tagger = fugashi.Tagger()\n",
                "\n",
                "def tokenize_words(text):\n",
                "    \"\"\"Word-level tokenization using fugashi (MeCab).\"\"\"\n",
                "    if not text:\n",
                "        return []\n",
                "    result = []\n",
                "    for t in tagger(text):\n",
                "        if t.feature.pos1 not in ['Á©∫ÁôΩ']:  # Skip whitespace\n",
                "            result.append(t.surface)\n",
                "    return result\n",
                "\n",
                "def encode_words(words, vocab, pad_id, unk_id, max_len=None):\n",
                "    \"\"\"Encode word list to padded integer IDs (left-padded).\"\"\"\n",
                "    if max_len is None:\n",
                "        max_len = MAX_WORD_CONTEXT\n",
                "    ids = [vocab.get(w, unk_id) for w in words]\n",
                "    if len(ids) < max_len:\n",
                "        ids = [pad_id] * (max_len - len(ids)) + ids  # Left-pad\n",
                "    return ids[-max_len:]  # Keep last N tokens\n",
                "\n",
                "# Quick test\n",
                "test_words = tokenize_words('‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇÊöë„ÅÑ„Åß„Åô„Å≠')\n",
                "print(f\"‚úì Tokenize test: {test_words}\")\n",
                "print(f\"  ({len(test_words)} words)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load or Build Cache\n",
                "\n",
                "Uses `left_context + output` combined for full sentence context.\n",
                "\n",
                "Testing mode uses separate cache files (`_test` suffix)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Cache paths ‚Äî separate for test vs full to avoid overwriting\n",
                "VOCAB_CACHE = f\"{CACHE_DIR}/word_vocab_v2{CACHE_SUFFIX}.json\"\n",
                "NWP_X_CACHE = f\"{CACHE_DIR}/nwp_x_v2{CACHE_SUFFIX}.npy\"\n",
                "NWP_Y_CACHE = f\"{CACHE_DIR}/nwp_y_v2{CACHE_SUFFIX}.npy\"\n",
                "TEST_CASES_CACHE = f\"{CACHE_DIR}/test_cases_nwp{CACHE_SUFFIX}.json\"\n",
                "\n",
                "def cache_exists():\n",
                "    return all(os.path.exists(f) for f in [VOCAB_CACHE, NWP_X_CACHE, NWP_Y_CACHE])\n",
                "\n",
                "if cache_exists() and not FORCE_REBUILD_CACHE:\n",
                "    print(\"üì¶ Loading from cache (memory-mapped)...\")\n",
                "    \n",
                "    with open(VOCAB_CACHE, 'r', encoding='utf-8') as f:\n",
                "        vocab_data = json.load(f)\n",
                "    word_to_idx = vocab_data['word_to_idx']\n",
                "    idx_to_word = {int(k): v for k, v in vocab_data['idx_to_word'].items()}\n",
                "    vocab_size = len(word_to_idx)\n",
                "    \n",
                "    x_mmap = np.load(NWP_X_CACHE, mmap_mode='r')\n",
                "    y_mmap = np.load(NWP_Y_CACHE, mmap_mode='r')\n",
                "    \n",
                "    print(f\"‚úì Vocab: {vocab_size:,} words\")\n",
                "    print(f\"‚úì Pairs: {len(x_mmap):,} (memory-mapped)\")\n",
                "    CACHE_LOADED = True\n",
                "else:\n",
                "    print(\"üî® Building from scratch (will save to drive)...\")\n",
                "    CACHE_LOADED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset + build word vocabulary\n",
                "if not CACHE_LOADED:\n",
                "    from datasets import load_dataset\n",
                "    from collections import Counter\n",
                "    \n",
                "    print(\"üì• Loading zenz dataset...\")\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=\"train\"\n",
                "    )\n",
                "    print(f\"‚úì Raw: {len(dataset):,} items\")\n",
                "    \n",
                "    # Pass 1: Build vocab from left_context + output (combined)\n",
                "    print(f\"\\nüìù Building word vocabulary (limit: {MAX_SAMPLES:,} items)...\")\n",
                "    word_counts = Counter()\n",
                "    # Store tokenized sentences for reuse in Pass 2\n",
                "    all_sentences = []  # [(words_list, sentence_text), ...]\n",
                "    processed = 0\n",
                "    \n",
                "    for item in tqdm(dataset, desc=\"Counting words\"):\n",
                "        left_ctx = item.get('left_context', '') or ''\n",
                "        output = item.get('output', '') or ''\n",
                "        text = left_ctx + output\n",
                "        if not text.strip():\n",
                "            continue\n",
                "        words = tokenize_words(text)\n",
                "        if len(words) < 3:  # Skip very short fragments\n",
                "            continue\n",
                "        word_counts.update(words)\n",
                "        all_sentences.append((words, text))\n",
                "        processed += 1\n",
                "        if MAX_SAMPLES and processed >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    print(f\"\\n‚úì Found {len(word_counts):,} unique words from {processed:,} items\")\n",
                "    print(f\"  Top 15: {[w for w, c in word_counts.most_common(15)]}\")\n",
                "    \n",
                "    # Build vocab: special tokens first, then most common words\n",
                "    word_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "    for word, _ in word_counts.most_common(WORD_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "        word_to_idx[word] = len(word_to_idx)\n",
                "    \n",
                "    idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
                "    vocab_size = len(word_to_idx)\n",
                "    UNK_IDX = word_to_idx['<UNK>']\n",
                "    \n",
                "    # Check vocab coverage\n",
                "    total_tokens = sum(word_counts.values())\n",
                "    covered_tokens = sum(c for w, c in word_counts.items() if w in word_to_idx)\n",
                "    print(f\"‚úì Vocab size: {vocab_size:,}\")\n",
                "    print(f\"  Coverage: {covered_tokens/total_tokens*100:.1f}% of tokens in vocab\")\n",
                "    \n",
                "    # Save vocab\n",
                "    with open(VOCAB_CACHE, 'w', encoding='utf-8') as f:\n",
                "        json.dump({\n",
                "            'word_to_idx': word_to_idx,\n",
                "            'idx_to_word': {str(k): v for k, v in idx_to_word.items()}\n",
                "        }, f, ensure_ascii=False)\n",
                "    print(f\"‚úì Vocab saved to {VOCAB_CACHE}\")\n",
                "    \n",
                "    del word_counts\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create training pairs + save meaningful test cases\n",
                "if not CACHE_LOADED:\n",
                "    print(f\"\\nüî¢ Creating training pairs (limit: {MAX_NWP_PAIRS:,})...\")\n",
                "    \n",
                "    PAD = word_to_idx['<PAD>']\n",
                "    UNK = word_to_idx['<UNK>']\n",
                "    \n",
                "    # Pre-allocate arrays\n",
                "    X = np.zeros((MAX_NWP_PAIRS, MAX_WORD_CONTEXT), dtype=np.int32)\n",
                "    y = np.zeros(MAX_NWP_PAIRS, dtype=np.int32)\n",
                "    pair_idx = 0\n",
                "    \n",
                "    # Save meaningful test cases: complete sentences where ALL words are in vocab\n",
                "    test_cases_to_save = []\n",
                "    \n",
                "    for words, original_text in tqdm(all_sentences, desc=\"Creating pairs\"):\n",
                "        if len(words) < 2:\n",
                "            continue\n",
                "        \n",
                "        # Check if this sentence is \"clean\" (all words in vocab, no UNK)\n",
                "        all_in_vocab = all(w in word_to_idx for w in words)\n",
                "        \n",
                "        # Create sliding window pairs: context ‚Üí next_word\n",
                "        for i in range(1, len(words)):\n",
                "            next_word = words[i]\n",
                "            if next_word not in word_to_idx:\n",
                "                continue\n",
                "            \n",
                "            context = words[max(0, i - MAX_WORD_CONTEXT):i]\n",
                "            X[pair_idx] = encode_words(context, word_to_idx, PAD, UNK)\n",
                "            y[pair_idx] = word_to_idx[next_word]\n",
                "            pair_idx += 1\n",
                "            \n",
                "            if pair_idx >= MAX_NWP_PAIRS:\n",
                "                break\n",
                "        \n",
                "        # Save as test case if: clean sentence, >= 4 words, meaningful\n",
                "        if (all_in_vocab and len(words) >= 4 and \n",
                "            len(test_cases_to_save) < 50):\n",
                "            # Pick a meaningful context‚Üíword pair from middle of sentence\n",
                "            # (not the first word which has little context)\n",
                "            for i in range(2, len(words)):  # Start from 3rd word\n",
                "                next_word = words[i]\n",
                "                # Skip punctuation as target (we want real words)\n",
                "                if next_word in ['„ÄÅ', '„ÄÇ', '„Éª', 'Ôºà', 'Ôºâ', '„Äå', '„Äç', 'ÔºÅ', 'Ôºü']:\n",
                "                    continue\n",
                "                if next_word not in word_to_idx:\n",
                "                    continue\n",
                "                context = words[max(0, i - MAX_WORD_CONTEXT):i]\n",
                "                test_cases_to_save.append({\n",
                "                    'context': context,\n",
                "                    'expected': next_word,\n",
                "                    'sentence': ''.join(words),\n",
                "                })\n",
                "                break  # One test case per sentence\n",
                "        \n",
                "        if pair_idx >= MAX_NWP_PAIRS:\n",
                "            break\n",
                "    \n",
                "    # Trim to actual size\n",
                "    X = X[:pair_idx]\n",
                "    y = y[:pair_idx]\n",
                "    print(f\"\\n‚úì Created {pair_idx:,} training pairs\")\n",
                "    print(f\"  Avg pairs/item: {pair_idx / max(len(all_sentences), 1):.1f}\")\n",
                "    \n",
                "    # Show sample pairs\n",
                "    print(\"\\nüìù Sample training pairs:\")\n",
                "    for i in range(min(10, pair_idx)):\n",
                "        ctx = [idx_to_word.get(int(idx), '?') for idx in X[i] if idx != PAD]\n",
                "        tgt = idx_to_word.get(int(y[i]), '?')\n",
                "        print(f\"  [{', '.join(ctx)}] ‚Üí {tgt}\")\n",
                "    \n",
                "    # üíæ Save test cases (meaningful sentences only)\n",
                "    with open(TEST_CASES_CACHE, 'w', encoding='utf-8') as f:\n",
                "        json.dump(test_cases_to_save, f, ensure_ascii=False, indent=2)\n",
                "    print(f\"\\nüíæ Saved {len(test_cases_to_save)} meaningful test cases ‚Üí {TEST_CASES_CACHE}\")\n",
                "    print(\"\\nüìù Sample test cases:\")\n",
                "    for tc in test_cases_to_save[:5]:\n",
                "        print(f\"  {''.join(tc['context'])} ‚Üí {tc['expected']}  (from: {tc['sentence'][:30]}...)\")\n",
                "    \n",
                "    # Save as .npy and release\n",
                "    np.save(NWP_X_CACHE, X)\n",
                "    np.save(NWP_Y_CACHE, y)\n",
                "    del X, y, test_cases_to_save, all_sentences\n",
                "    gc.collect()\n",
                "    \n",
                "    # Release dataset\n",
                "    del dataset\n",
                "    gc.collect()\n",
                "    print(\"üßπ Saved cache, released memory\")\n",
                "    \n",
                "    # Load as memory-mapped\n",
                "    x_mmap = np.load(NWP_X_CACHE, mmap_mode='r')\n",
                "    y_mmap = np.load(NWP_Y_CACHE, mmap_mode='r')\n",
                "    print(f\"‚úì Loaded as mmap: X={x_mmap.shape}, y={y_mmap.shape}\")\n",
                "\n",
                "print(f\"\\nüìä Total pairs: {len(x_mmap):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_samples = len(x_mmap)\n",
                "split = int(n_samples * 0.9)\n",
                "\n",
                "# Random shuffle indices\n",
                "indices = np.random.permutation(n_samples).astype(np.int32)\n",
                "train_idx = indices[:split]\n",
                "val_idx = indices[split:]\n",
                "\n",
                "def make_generator(x, y_arr, idx_arr):\n",
                "    \"\"\"Generator reads from mmap arrays (zero RAM copy).\"\"\"\n",
                "    def gen():\n",
                "        for i in idx_arr:\n",
                "            yield x[i], y_arr[i]\n",
                "    return gen\n",
                "\n",
                "output_sig = (\n",
                "    tf.TensorSpec(shape=(MAX_WORD_CONTEXT,), dtype=tf.int32),\n",
                "    tf.TensorSpec(shape=(), dtype=tf.int32),\n",
                ")\n",
                "\n",
                "# .repeat() is required: from_generator is one-shot, exhausts after epoch 1\n",
                "train_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(x_mmap, y_mmap, train_idx),\n",
                "    output_signature=output_sig\n",
                ").repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(x_mmap, y_mmap, val_idx),\n",
                "    output_signature=output_sig\n",
                ").repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"Train: {len(train_idx):,}, Val: {len(val_idx):,}\")\n",
                "print(f\"üí° Data loaded via mmap + generator (near-zero RAM)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Model (Bi-GRU + Self-Attention)\n",
                "\n",
                "Model is built inside `strategy.scope()` for multi-GPU training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "with strategy.scope():\n",
                "    inputs = Input(shape=(MAX_WORD_CONTEXT,), name='input')\n",
                "\n",
                "    # Embedding\n",
                "    x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "\n",
                "    # Bidirectional GRU\n",
                "    encoder_out = Bidirectional(\n",
                "        GRU(GRU_UNITS, return_sequences=True, dropout=0.2),\n",
                "        name='bi_gru'\n",
                "    )(x)\n",
                "\n",
                "    # Self-Attention (Luong-style)\n",
                "    attention_out = Attention(use_scale=True, name='attention')(\n",
                "        [encoder_out, encoder_out]\n",
                "    )\n",
                "\n",
                "    # Combine encoder + attention\n",
                "    combined = Concatenate()([encoder_out, attention_out])\n",
                "    combined = LayerNormalization()(combined)\n",
                "\n",
                "    # Context GRU (compress to single vector)\n",
                "    context = GRU(GRU_UNITS, name='context_gru')(combined)\n",
                "    context = Dropout(0.3)(context)\n",
                "\n",
                "    # Output: predict next word\n",
                "    # dtype='float32' ensures output stays FP32 under mixed precision\n",
                "    outputs = Dense(vocab_size, activation='softmax', name='output', dtype='float32')(context)\n",
                "\n",
                "    model = Model(inputs, outputs, name='next_word_lm_v2')\n",
                "\n",
                "    # Gradient clipping for stable training\n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "        loss='sparse_categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "\n",
                "model.summary()\n",
                "params = model.count_params()\n",
                "print(f\"\\nüìä Parameters: {params:,}\")\n",
                "print(f\"   FP32: ~{params * 4 / 1024 / 1024:.1f} MB\")\n",
                "print(f\"   FP16: ~{params * 2 / 1024 / 1024:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "# Calculate steps (from_generator doesn't auto-detect size)\n",
                "steps_per_epoch = len(train_idx) // BATCH_SIZE\n",
                "validation_steps = max(1, len(val_idx) // BATCH_SIZE)\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(\n",
                "        f'{MODEL_DIR}/best_v2.keras',\n",
                "        monitor='val_accuracy',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=5,\n",
                "        restore_best_weights=True\n",
                "    ),\n",
                "    ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=2,\n",
                "        min_lr=1e-6,\n",
                "        verbose=1\n",
                "    )\n",
                "]\n",
                "\n",
                "print(f\"Steps/epoch: {steps_per_epoch}, Val steps: {validation_steps}\")\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    steps_per_epoch=steps_per_epoch,\n",
                "    validation_data=val_ds,\n",
                "    validation_steps=validation_steps,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "\n",
                "plt.savefig(f'{MODEL_DIR}/training_v2.png')\n",
                "plt.show()\n",
                "\n",
                "# ‚úÖ Logic check: loss should decrease, accuracy should increase\n",
                "losses = history.history['loss']\n",
                "accs = history.history['accuracy']\n",
                "print(f\"\\nüìä Training Summary:\")\n",
                "print(f\"  Loss:     {losses[0]:.4f} ‚Üí {losses[-1]:.4f} ({'‚úÖ decreasing' if losses[-1] < losses[0] else '‚ùå NOT decreasing'})\")\n",
                "print(f\"  Accuracy: {accs[0]*100:.2f}% ‚Üí {accs[-1]*100:.2f}% ({'‚úÖ increasing' if accs[-1] > accs[0] else '‚ùå NOT increasing'})\")\n",
                "print(f\"  Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model + vocab + config\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/word_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_word.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_word.items()}, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        'vocab_size': vocab_size,\n",
                "        'max_context_len': MAX_WORD_CONTEXT,\n",
                "        'embedding_dim': EMBEDDING_DIM,\n",
                "        'gru_units': GRU_UNITS,\n",
                "        'architecture': 'BiGRU_SelfAttention_ContextGRU',\n",
                "        'special_tokens': SPECIAL_TOKENS,\n",
                "        'version': 'v2'\n",
                "    }, f, indent=2)\n",
                "\n",
                "keras_size = os.path.getsize(f'{MODEL_DIR}/model.keras')\n",
                "print(f\"‚úì Model saved: {keras_size / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export TFLite\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [\n",
                "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
                "        tf.lite.OpsSet.SELECT_TF_OPS\n",
                "    ]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite)\n",
                "    print(f\"‚úì model.tflite ({len(tflite)/(1024*1024):.2f} MB)\")\n",
                "    \n",
                "    # FP16 version (smaller, same accuracy)\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite16)/(1024*1024):.2f} MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö† TFLite export failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verification ‚Äî Real Meaningful Test Cases\n",
                "\n",
                "Test cases are picked **during data prep** from sentences where:\n",
                "- ‚úÖ ALL words are in vocabulary (no `<UNK>` contamination)\n",
                "- ‚úÖ Sentence has ‚â• 4 words (meaningful context)\n",
                "- ‚úÖ Target is a real word (not punctuation)\n",
                "\n",
                "**What to check**:\n",
                "- Expected word appears in top-5? ‚Üí model learned context\n",
                "- Predictions are diverse? ‚Üí model not collapsed to frequency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Real Test Cases from Dataset\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "PAD = word_to_idx['<PAD>']\n",
                "UNK = word_to_idx['<UNK>']\n",
                "\n",
                "def predict_top_k(context_words, top_k=5):\n",
                "    \"\"\"Predict top-K next words from context. 1 model call = instant.\"\"\"\n",
                "    encoded = np.array([encode_words(context_words, word_to_idx, PAD, UNK)])\n",
                "    probs = model.predict(encoded, verbose=0)[0]\n",
                "    \n",
                "    top_indices = np.argsort(probs)[-top_k*2:][::-1]\n",
                "    results = []\n",
                "    for idx in top_indices:\n",
                "        word = idx_to_word.get(idx, '<UNK>')\n",
                "        if word not in SPECIAL_TOKENS:\n",
                "            results.append((word, float(probs[idx])))\n",
                "        if len(results) >= top_k:\n",
                "            break\n",
                "    return results\n",
                "\n",
                "\n",
                "# ==========================================================\n",
                "# Load test cases saved during data prep\n",
                "# These are clean sentences with NO <UNK> tokens\n",
                "# ==========================================================\n",
                "if os.path.exists(TEST_CASES_CACHE):\n",
                "    with open(TEST_CASES_CACHE, 'r', encoding='utf-8') as f:\n",
                "        test_cases = json.load(f)\n",
                "    print(f\"\\n‚úì Loaded {len(test_cases)} meaningful test cases\")\n",
                "else:\n",
                "    # Fallback: pick from validation data (may have UNK)\n",
                "    print(\"\\n‚ö†Ô∏è No saved test cases, building from validation set...\")\n",
                "    test_cases = []\n",
                "    np.random.seed(42)\n",
                "    sample_indices = np.random.choice(val_idx, size=min(200, len(val_idx)), replace=False)\n",
                "    seen = set()\n",
                "    for i in sample_indices:\n",
                "        x_row = x_mmap[i]\n",
                "        y_val = int(y_mmap[i])\n",
                "        expected = idx_to_word.get(y_val, '<UNK>')\n",
                "        if expected in SPECIAL_TOKENS or expected in seen:\n",
                "            continue\n",
                "        if expected in ['„ÄÅ', '„ÄÇ', '„Éª', 'Ôºà', 'Ôºâ', '„Äå', '„Äç']:\n",
                "            continue\n",
                "        ctx = [idx_to_word.get(int(idx), '<UNK>') for idx in x_row if idx != PAD]\n",
                "        if '<UNK>' in ctx or len(ctx) < 2:\n",
                "            continue\n",
                "        test_cases.append({'context': ctx, 'expected': expected, 'sentence': ''.join(ctx) + expected})\n",
                "        seen.add(expected)\n",
                "        if len(test_cases) >= 15:\n",
                "            break\n",
                "\n",
                "# ==========================================================\n",
                "# Run predictions\n",
                "# ==========================================================\n",
                "# Use at most 20 test cases\n",
                "test_subset = test_cases[:20]\n",
                "\n",
                "print(f\"\\nüìù Testing {len(test_subset)} cases (clean sentences, no UNK):\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "correct_top1 = 0\n",
                "correct_top5 = 0\n",
                "all_predictions = set()  # Track prediction diversity\n",
                "\n",
                "for tc in test_subset:\n",
                "    preds = predict_top_k(tc['context'], top_k=5)\n",
                "    pred_words = [w for w, _ in preds]\n",
                "    all_predictions.update(pred_words)\n",
                "    \n",
                "    in_top1 = pred_words[0] == tc['expected'] if preds else False\n",
                "    in_top5 = tc['expected'] in pred_words\n",
                "    \n",
                "    if in_top1: correct_top1 += 1\n",
                "    if in_top5: correct_top5 += 1\n",
                "    \n",
                "    status = '‚úÖ' if in_top5 else ('üü°' if in_top1 else '‚ùå')\n",
                "    ctx_str = ''.join(tc['context'][-5:])  # Show last 5 context words\n",
                "    pred_str = ', '.join(pred_words[:5])\n",
                "    print(f\"  {status} {ctx_str} ‚Üí expected: {tc['expected']}\")\n",
                "    print(f\"       top5: [{pred_str}]\")\n",
                "\n",
                "n = len(test_subset)\n",
                "print(f\"\\n\" + \"=\"*60)\n",
                "print(f\"üìä Results:\")\n",
                "print(f\"  Top-1 accuracy: {correct_top1}/{n} ({correct_top1/n*100:.1f}%)\")\n",
                "print(f\"  Top-5 accuracy: {correct_top5}/{n} ({correct_top5/n*100:.1f}%)\")\n",
                "print(f\"  Unique predictions across all tests: {len(all_predictions)}\")\n",
                "print(f\"    (should be >> 5. If ‚â§ 5 = model collapsed to frequency only)\")\n",
                "\n",
                "if TESTING_MODE:\n",
                "    print(\"\\n‚ö†Ô∏è TESTING MODE (100K samples).\")\n",
                "    if correct_top5 / n >= 0.1:\n",
                "        print(\"   ‚úÖ Model shows learning signal! Ready for full training.\")\n",
                "        print(\"   ‚Üí Set TESTING_MODE = False, FORCE_REBUILD_CACHE = True\")\n",
                "    else:\n",
                "        print(\"   üü° Low accuracy is expected with 100K samples.\")\n",
                "        print(\"   ‚úÖ Check: loss decreased? accuracy improved? no crashes?\")\n",
                "        print(\"   ‚Üí If yes, set TESTING_MODE = False for production training.\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List exported files\n",
                "print(f\"\\nüì¶ Files ({PLATFORM}):\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    p = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(p):\n",
                "        s = os.path.getsize(p)\n",
                "        if s > 1024*1024:\n",
                "            print(f\"  {f}: {s/(1024*1024):.2f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {s/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}