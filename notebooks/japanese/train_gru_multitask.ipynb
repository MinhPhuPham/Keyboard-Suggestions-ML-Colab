{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Task GRU ‚Äî KKC + NWP\n",
                "\n",
                "Two independent heads in a single model:\n",
                "- **KKC Head**: Kana ‚Üí Kanji (char embedding ‚Üí Bi-GRU ‚Üí GRU decoder + attention)\n",
                "- **NWP Head**: Next word prediction (word embedding ‚Üí Bi-GRU ‚Üí self-attention ‚Üí context GRU)\n",
                "\n",
                "Combined loss: `1.0 √ó KKC + 0.3 √ó NWP`\n",
                "\n",
                "Scripts in `scripts/japanese_enhancement/`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sys, shutil\n",
                "\n",
                "# --- Detect platform ---\n",
                "if os.path.exists('/content/drive'):\n",
                "    PLATFORM = 'colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'kaggle'\n",
                "else:\n",
                "    PLATFORM = 'local'\n",
                "\n",
                "# --- Clone/refresh repo (Colab/Kaggle only) ---\n",
                "REPO_URL = 'https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git'\n",
                "\n",
                "if PLATFORM == 'colab':\n",
                "    REPO_DIR = '/content/KeyboardSuggestionsML'\n",
                "elif PLATFORM == 'kaggle':\n",
                "    REPO_DIR = '/kaggle/working/KeyboardSuggestionsML'\n",
                "else:\n",
                "    REPO_DIR = None\n",
                "\n",
                "if REPO_DIR is not None:\n",
                "    # Always delete & re-clone for latest code\n",
                "    if os.path.exists(REPO_DIR):\n",
                "        shutil.rmtree(REPO_DIR)\n",
                "        print('üóëÔ∏è Removed previous clone')\n",
                "    os.system(f'git clone -q {REPO_URL} {REPO_DIR}')\n",
                "    PROJECT_ROOT = REPO_DIR\n",
                "    print(f'‚úì Cloned latest code to {REPO_DIR}')\n",
                "else:\n",
                "    # Local: notebook is in notebooks/japanese/, project root is 2 levels up\n",
                "    PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f'Platform: {PLATFORM}')\n",
                "print(f'Project:  {PROJECT_ROOT}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow==2.20.0 keras==3.13.1 datasets numpy tqdm fugashi unidic-lite matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Environment versions (compare local vs Colab) ---\n",
                "import sys\n",
                "print(f'Python:     {sys.version}')\n",
                "import tensorflow as tf\n",
                "print(f'TensorFlow: {tf.__version__}')\n",
                "import keras\n",
                "print(f'Keras:      {keras.__version__}')\n",
                "import numpy as np\n",
                "print(f'NumPy:      {np.__version__}')\n",
                "print(f'GPU:        {tf.config.list_physical_devices(\"GPU\")}')\n",
                "print(f'Platform:   {sys.platform}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "NUM_GPUS = len(gpus) if gpus else 1\n",
                "if NUM_GPUS > 1:\n",
                "    strategy = tf.distribute.MirroredStrategy()\n",
                "elif gpus:\n",
                "    strategy = tf.distribute.OneDeviceStrategy('/gpu:0')\n",
                "else:\n",
                "    strategy = tf.distribute.OneDeviceStrategy('/cpu:0')\n",
                "\n",
                "print(f'GPUs: {NUM_GPUS}, Strategy: {strategy.__class__.__name__}')\n",
                "\n",
                "if gpus:\n",
                "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
                "    print(f'Mixed precision: {tf.keras.mixed_precision.global_policy().name}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration\n",
                "\n",
                "Override any config values here before importing modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement import config\n",
                "\n",
                "# ============================================\n",
                "# ‚ö†Ô∏è OVERRIDE CONFIG HERE\n",
                "# ============================================\n",
                "config.TESTING_MODE = True      # True = 100K, False = 8M\n",
                "config.FORCE_REBUILD_CACHE = False\n",
                "config.BATCH_SIZE = 512 * NUM_GPUS\n",
                "\n",
                "if config.TESTING_MODE:\n",
                "    config.MAX_SAMPLES = 100_000\n",
                "    config.MAX_NWP_PAIRS = 500_000\n",
                "    config.NUM_EPOCHS = 10\n",
                "    config.CACHE_SUFFIX = '_test'\n",
                "else:\n",
                "    config.MAX_SAMPLES = 8_000_000\n",
                "    config.MAX_NWP_PAIRS = 8_000_000\n",
                "    config.NUM_EPOCHS = 10\n",
                "    config.CACHE_SUFFIX = ''\n",
                "\n",
                "config.ensure_dirs()\n",
                "config.print_config()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load or Build Cache\n",
                "\n",
                "Loads zenz dataset once, builds both KKC and NWP caches.\n",
                "Uses memory-mapped .npy files for near-zero RAM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement import data_loader\n",
                "import gc\n",
                "\n",
                "cache_paths = config.get_cache_paths(config.CACHE_DIR, config.CACHE_SUFFIX)\n",
                "kkc_ready, nwp_ready = data_loader.check_cache(cache_paths)\n",
                "\n",
                "if (kkc_ready and nwp_ready) and not config.FORCE_REBUILD_CACHE:\n",
                "    print('‚úì All caches found, loading...')\n",
                "else:\n",
                "    print('üî® Building caches from scratch...')\n",
                "    training_data = data_loader.load_raw_dataset()\n",
                "    \n",
                "    if not kkc_ready or config.FORCE_REBUILD_CACHE:\n",
                "        data_loader.build_kkc_cache(training_data, cache_paths)\n",
                "    \n",
                "    if not nwp_ready or config.FORCE_REBUILD_CACHE:\n",
                "        data_loader.build_nwp_cache(training_data, cache_paths)\n",
                "    \n",
                "    del training_data\n",
                "    gc.collect()\n",
                "\n",
                "char_to_idx, idx_to_char, enc_mmap, dec_in_mmap, dec_tgt_mmap = \\\n",
                "    data_loader.load_kkc_cache(cache_paths)\n",
                "\n",
                "word_to_idx, idx_to_word, nwp_x_mmap, nwp_y_mmap = \\\n",
                "    data_loader.load_nwp_cache(cache_paths)\n",
                "\n",
                "char_vocab_size = len(char_to_idx)\n",
                "word_vocab_size = len(word_to_idx)\n",
                "print(f'\\nüìä Char vocab: {char_vocab_size:,}, Word vocab: {word_vocab_size:,}')\n",
                "print(f'   KKC: {len(enc_mmap):,} samples')\n",
                "print(f'   NWP: {len(nwp_x_mmap):,} pairs')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.training import create_datasets\n",
                "\n",
                "kkc_data = (enc_mmap, dec_in_mmap, dec_tgt_mmap)\n",
                "nwp_data = (nwp_x_mmap, nwp_y_mmap, word_to_idx)\n",
                "\n",
                "datasets, info = create_datasets(kkc_data, nwp_data, config.BATCH_SIZE)\n",
                "print('‚úì Datasets ready')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Model\n",
                "\n",
                "3+input model with independent paths:\n",
                "- KKC: `encoder_input` + `decoder_input` ‚Üí char embedding ‚Üí Bi-GRU ‚Üí decoder + attention\n",
                "- NWP: `nwp_input` ‚Üí word embedding ‚Üí Bi-GRU ‚Üí self-attention ‚Üí context GRU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.model import build_multitask_model\n",
                "\n",
                "model = build_multitask_model(char_vocab_size, word_vocab_size, strategy)\n",
                "model.summary()\n",
                "\n",
                "params = model.count_params()\n",
                "print(f'\\nüìä Parameters: {params:,}')\n",
                "print(f'   FP32: ~{params * 4 / 1024 / 1024:.1f} MB')\n",
                "print(f'   FP16: ~{params * 2 / 1024 / 1024:.1f} MB')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.training import train_multitask\n",
                "\n",
                "with strategy.scope():\n",
                "    optimizer = tf.keras.optimizers.Adam(\n",
                "        learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
                "    )\n",
                "    model.optimizer = optimizer\n",
                "    model.compile()\n",
                "\n",
                "history = train_multitask(model, datasets, info)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.1 Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.plotting import plot_training_history\n",
                "plot_training_history(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.export import save_model, export_tflite, list_saved_files\n",
                "\n",
                "save_model(model, char_to_idx, word_to_idx)\n",
                "export_tflite(model)\n",
                "list_saved_files()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Verification\n",
                "\n",
                "Test both KKC and NWP heads with real test cases from training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scripts.japanese_enhancement.verify import verify_all\n",
                "\n",
                "verify_all(\n",
                "    model, char_to_idx, idx_to_char,\n",
                "    word_to_idx, idx_to_word, cache_paths\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}