{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Multi-Task GRU ‚Äî Shared Encoder (v2)\n",
    "\n",
    "**Kana‚ÜíKanji Conversion (KKC) + Next Word Prediction (NWP)**\n",
    "\n",
    "## Architecture\n",
    "Both heads share one character-level encoder:\n",
    "```\n",
    "Input (char IDs) ‚Üí Shared BiGRU Encoder ‚Üí encoder_output\n",
    "                      ‚Üì                         ‚Üì\n",
    "               KKC Decoder (seq2seq)    NWP Head (attention+GRU)\n",
    "                      ‚Üì                         ‚Üì\n",
    "               kanji output              next_word prediction\n",
    "```\n",
    "\n",
    "- **KKC input**: hiragana chars (e.g., \"„Åç„Çá„ÅÜ„ÅØ„Å¶„Çì„Åç„Åå„ÅÑ„ÅÑ\")\n",
    "- **NWP input**: context chars with `<SEP>` markers (e.g., \"‰ªäÊó•<SEP>„ÅØ<SEP>Â§©Ê∞ó<SEP>„Åå\")\n",
    "- Both use same encoder ‚Äî encoder learns from BOTH tasks!\n",
    "\n",
    "## Training: Two Forward Passes\n",
    "Each step passes KKC data then NWP data through the **same encoder**.\n",
    "The shared encoder gets gradients from both tasks.\n",
    "\n",
    "## How to Run\n",
    "1. Set `TESTING_MODE = True` for quick validation (100K samples, 10 epochs)\n",
    "2. Set `TESTING_MODE = False` for full production training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pin versions for reproducibility\n",
    "!pip install tensorflow==2.20.0 keras==3.13.1 fugashi[unidic-lite] -q\n",
    "!pip install tqdm -q\n",
    "\n",
    "import os, sys, gc\n",
    "import numpy as np\n",
    "\n",
    "# Clone/pull repo\n",
    "REPO_DIR = '/content/KeyboardSuggestionsML'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"TF: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement import config\n",
    "\n",
    "# ‚ö†Ô∏è TESTING MODE: Set False for full production training\n",
    "config.TESTING_MODE = True\n",
    "\n",
    "if config.TESTING_MODE:\n",
    "    config.MAX_SAMPLES = 100_000\n",
    "    config.MAX_NWP_PAIRS = 500_000\n",
    "    config.NUM_EPOCHS = 10\n",
    "    config.CACHE_SUFFIX = '_test'\n",
    "    print(\"‚ö†Ô∏è TESTING MODE: 100K samples, 10 epochs\")\n",
    "else:\n",
    "    config.MAX_SAMPLES = 8_000_000\n",
    "    config.MAX_NWP_PAIRS = 8_000_000\n",
    "    config.NUM_EPOCHS = 10\n",
    "    config.CACHE_SUFFIX = ''\n",
    "    print(\"üöÄ FULL TRAINING: 8M samples, 10 epochs\")\n",
    "\n",
    "# GPU scaling\n",
    "NUM_GPUS = max(1, len(tf.config.list_physical_devices('GPU')))\n",
    "# Reduced for shared encoder (2 forward passes per step = 2x VRAM)\n",
    "config.BATCH_SIZE = 256 * NUM_GPUS\n",
    "\n",
    "# Override paths for v2\n",
    "config.MODEL_DIR = f'{config.DRIVE_DIR}/models/multitask_v2'\n",
    "config.CACHE_DIR = f'{config.DRIVE_DIR}/cache/multitask_v2'\n",
    "config.ensure_dirs()\n",
    "config.print_config()\n",
    "\n",
    "# Cache paths\n",
    "cache_paths = config.get_cache_paths(config.CACHE_DIR, config.CACHE_SUFFIX)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detect GPU strategy\n",
    "if NUM_GPUS > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"‚úÖ MirroredStrategy: {NUM_GPUS} GPUs\")\n",
    "elif NUM_GPUS == 1:\n",
    "    strategy = None  # Single GPU, no strategy needed\n",
    "    print(\"‚úÖ Single GPU mode\")\n",
    "else:\n",
    "    strategy = None\n",
    "    print(\"‚ö†Ô∏è CPU mode (no GPU)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Cache Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.data_loader import (\n",
    "    check_cache, load_raw_dataset,\n",
    "    build_kkc_cache, build_nwp_char_cache,\n",
    "    load_kkc_cache, load_nwp_char_cache,\n",
    ")\n",
    "\n",
    "# Check existing cache\n",
    "kkc_ready, _ = check_cache(cache_paths)\n",
    "nwp_char_ready = os.path.exists(cache_paths.get('nwp_char_x', ''))\n",
    "\n",
    "if not kkc_ready or not nwp_char_ready or config.FORCE_REBUILD_CACHE:\n",
    "    print(\"\\nüì• Loading raw data...\")\n",
    "    training_data = load_raw_dataset(config.MAX_SAMPLES)\n",
    "\n",
    "    if not kkc_ready:\n",
    "        c2i, _ = build_kkc_cache(training_data, cache_paths)\n",
    "    else:\n",
    "        c2i, _, _, _, _ = load_kkc_cache(cache_paths)\n",
    "\n",
    "    if not nwp_char_ready:\n",
    "        build_nwp_char_cache(training_data, cache_paths, c2i)\n",
    "\n",
    "    del training_data\n",
    "    gc.collect()\n",
    "    print(\"\\n‚úÖ Cache ready!\")\n",
    "else:\n",
    "    print(\"‚úÖ Cache already exists, skipping build\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Cached Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load KKC cache\n",
    "char_to_idx, idx_to_char, enc_mmap, dec_in_mmap, dec_tgt_mmap = \\\n",
    "    load_kkc_cache(cache_paths)\n",
    "kkc_data = (enc_mmap, dec_in_mmap, dec_tgt_mmap)\n",
    "\n",
    "# Load NWP char cache (shared encoder format)\n",
    "word_to_idx, idx_to_word, nwp_char_x_mmap, nwp_y_mmap = \\\n",
    "    load_nwp_char_cache(cache_paths)\n",
    "nwp_char_data = (nwp_char_x_mmap, nwp_y_mmap, word_to_idx)\n",
    "\n",
    "actual_char_vocab = len(char_to_idx)\n",
    "actual_word_vocab = len(word_to_idx)\n",
    "print(f\"\\nüìä Char vocab: {actual_char_vocab:,}\")\n",
    "print(f\"üìä Word vocab: {actual_word_vocab:,}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.training import create_shared_datasets\n",
    "\n",
    "datasets, info = create_shared_datasets(kkc_data, nwp_char_data, config.BATCH_SIZE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Shared Encoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.model import build_shared_multitask_model\n",
    "\n",
    "model = build_shared_multitask_model(\n",
    "    actual_char_vocab, actual_word_vocab, strategy=strategy\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Compile (optimizer only ‚Äî custom training loop manages losses)\n",
    "if strategy:\n",
    "    with strategy.scope():\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
    "        ))\n",
    "else:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
    "    ))\n",
    "\n",
    "print(f\"\\n‚úÖ Shared encoder model ready\")\n",
    "print(f\"   Inputs:  {len(model.inputs)} (encoder_input + decoder_input)\")\n",
    "print(f\"   Outputs: {len(model.outputs)} (kkc_output + nwp_output)\")\n",
    "print(f\"   Params:  {model.count_params():,}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.training import train_shared_multitask\n",
    "\n",
    "history = train_shared_multitask(model, datasets, info)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.plotting import plot_training_history\n",
    "\n",
    "plot_training_history(history)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scripts.japanese_enhancement.export import save_model, export_tflite, list_saved_files\n",
    "\n",
    "save_model(model, char_to_idx, word_to_idx)\n",
    "export_tflite(model)\n",
    "list_saved_files()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}