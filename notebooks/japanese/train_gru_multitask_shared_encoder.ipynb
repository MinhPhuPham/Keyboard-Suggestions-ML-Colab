{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Multi-Task GRU ‚Äî Shared Encoder (v2)\n",
                "\n",
                "**Kana‚ÜíKanji Conversion (KKC) + Next Word Prediction (NWP)**\n",
                "\n",
                "## Architecture\n",
                "Both heads share one character-level encoder:\n",
                "```\n",
                "Input (char IDs) ‚Üí Shared BiGRU Encoder ‚Üí encoder_output\n",
                "                      ‚Üì                         ‚Üì\n",
                "               KKC Decoder (seq2seq)    NWP Head (attention+GRU)\n",
                "                      ‚Üì                         ‚Üì\n",
                "               kanji output              next_word prediction\n",
                "```\n",
                "\n",
                "- **KKC input**: hiragana chars (e.g., \"„Åç„Çá„ÅÜ„ÅØ„Å¶„Çì„Åç„Åå„ÅÑ„ÅÑ\")\n",
                "- **NWP input**: context chars with `<SEP>` markers (e.g., \"‰ªäÊó•<SEP>„ÅØ<SEP>Â§©Ê∞ó<SEP>„Åå\")\n",
                "- Both use same encoder ‚Äî encoder learns from BOTH tasks!\n",
                "\n",
                "## Training: Two Forward Passes\n",
                "Each step passes KKC data then NWP data through the **same encoder**.\n",
                "The shared encoder gets gradients from both tasks.\n",
                "\n",
                "## How to Run\n",
                "1. Set `TESTING_MODE = True` for quick validation (100K samples, 10 epochs)\n",
                "2. Set `TESTING_MODE = False` for full production training\n",
                "\n",
                "## Platform Support\n",
                "- **Colab**: Mounts Google Drive, clones repo\n",
                "- **Kaggle**: Clones repo, downloads dataset via `gdown`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Pin versions for reproducibility\n",
                "!pip install tensorflow==2.20.0 keras==3.13.1 fugashi[unidic-lite] -q\n",
                "!pip install tqdm gdown -q\n",
                "\n",
                "import os, sys, gc\n",
                "import numpy as np\n",
                "\n",
                "# ===========================================================\n",
                "# PLATFORM DETECTION\n",
                "# ===========================================================\n",
                "if os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'kaggle'\n",
                "elif os.path.exists('/content'):\n",
                "    PLATFORM = 'colab'\n",
                "else:\n",
                "    PLATFORM = 'local'\n",
                "\n",
                "print(f\"üñ•Ô∏è Platform: {PLATFORM}\")\n",
                "\n",
                "# ===========================================================\n",
                "# MOUNT / CLONE\n",
                "# ===========================================================\n",
                "if PLATFORM == 'colab':\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    REPO_DIR = '/content/KeyboardSuggestionsML'\n",
                "elif PLATFORM == 'kaggle':\n",
                "    REPO_DIR = '/kaggle/working/KeyboardSuggestionsML'\n",
                "else:\n",
                "    REPO_DIR = os.path.expanduser('~/KeyboardSuggestionsML')\n",
                "\n",
                "# Clone/pull repo\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git {REPO_DIR}\n",
                "else:\n",
                "    !cd {REPO_DIR} && git pull\n",
                "\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "\n",
                "import tensorflow as tf\n",
                "print(f\"TF: {tf.__version__}\")\n",
                "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement import config\n",
                "\n",
                "# ‚ö†Ô∏è TESTING MODE: Set False for full production training\n",
                "config.TESTING_MODE = True\n",
                "\n",
                "if config.TESTING_MODE:\n",
                "    config.MAX_SAMPLES = 100_000\n",
                "    config.MAX_NWP_PAIRS = 500_000\n",
                "    config.NUM_EPOCHS = 10\n",
                "    config.CACHE_SUFFIX = '_test'\n",
                "    print(\"‚ö†Ô∏è TESTING MODE: 100K samples, 10 epochs\")\n",
                "else:\n",
                "    config.MAX_SAMPLES = 8_000_000\n",
                "    config.MAX_NWP_PAIRS = 8_000_000\n",
                "    config.NUM_EPOCHS = 10\n",
                "    config.CACHE_SUFFIX = ''\n",
                "    print(\"üöÄ FULL TRAINING: 8M samples, 10 epochs\")\n",
                "\n",
                "# GPU scaling\n",
                "NUM_GPUS = max(1, len(tf.config.list_physical_devices('GPU')))\n",
                "# Reduced for shared encoder (2 forward passes per step = 2x VRAM)\n",
                "config.BATCH_SIZE = 256 * NUM_GPUS\n",
                "\n",
                "# Override paths for v2\n",
                "config.MODEL_DIR = f'{config.DRIVE_DIR}/models/multitask_v2'\n",
                "config.CACHE_DIR = f'{config.DRIVE_DIR}/cache/multitask_v2'\n",
                "config.ensure_dirs()\n",
                "\n",
                "# ===========================================================\n",
                "# DOWNLOAD DATASET (if not exists)\n",
                "# ===========================================================\n",
                "dataset_path = f'{config.DATASET_DIR}/ime_dataset_10m.jsonl'\n",
                "if not os.path.exists(dataset_path):\n",
                "    print(f\"üì• Downloading dataset to {dataset_path}...\")\n",
                "    os.makedirs(config.DATASET_DIR, exist_ok=True)\n",
                "    !gdown \"1b5YgqVUEU2HGlkPBcSUIjL1XyJ7n8Cgg\" -O {dataset_path}\n",
                "    print(f\"‚úÖ Dataset downloaded! Size: {os.path.getsize(dataset_path) / 1e6:.1f} MB\")\n",
                "else:\n",
                "    print(f\"‚úÖ Dataset exists: {dataset_path} ({os.path.getsize(dataset_path) / 1e6:.1f} MB)\")\n",
                "\n",
                "config.print_config()\n",
                "\n",
                "# Cache paths\n",
                "cache_paths = config.get_cache_paths(config.CACHE_DIR, config.CACHE_SUFFIX)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. GPU Strategy\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Detect GPU strategy\n",
                "if NUM_GPUS > 1:\n",
                "    strategy = tf.distribute.MirroredStrategy()\n",
                "    print(f\"‚úÖ MirroredStrategy: {NUM_GPUS} GPUs\")\n",
                "elif NUM_GPUS == 1:\n",
                "    strategy = None  # Single GPU, no strategy needed\n",
                "    print(\"‚úÖ Single GPU mode\")\n",
                "else:\n",
                "    strategy = None\n",
                "    print(\"‚ö†Ô∏è CPU mode (no GPU)\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load & Cache Data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.data_loader import (\n",
                "    check_cache, load_raw_dataset,\n",
                "    build_kkc_cache, build_nwp_char_cache,\n",
                "    load_kkc_cache, load_nwp_char_cache,\n",
                ")\n",
                "\n",
                "# Check existing cache\n",
                "kkc_ready, _ = check_cache(cache_paths)\n",
                "nwp_char_ready = os.path.exists(cache_paths.get('nwp_char_x', ''))\n",
                "\n",
                "if not kkc_ready or not nwp_char_ready or config.FORCE_REBUILD_CACHE:\n",
                "    print(\"\\nüì• Loading raw data...\")\n",
                "    training_data = load_raw_dataset(config.MAX_SAMPLES)\n",
                "\n",
                "    if not kkc_ready:\n",
                "        c2i, _ = build_kkc_cache(training_data, cache_paths)\n",
                "    else:\n",
                "        c2i, _, _, _, _ = load_kkc_cache(cache_paths)\n",
                "\n",
                "    if not nwp_char_ready:\n",
                "        build_nwp_char_cache(training_data, cache_paths, c2i)\n",
                "\n",
                "    del training_data\n",
                "    gc.collect()\n",
                "    print(\"\\n‚úÖ Cache ready!\")\n",
                "else:\n",
                "    print(\"‚úÖ Cache already exists, skipping build\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Cached Data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Load KKC cache\n",
                "char_to_idx, idx_to_char, enc_mmap, dec_in_mmap, dec_tgt_mmap = \\\n",
                "    load_kkc_cache(cache_paths)\n",
                "kkc_data = (enc_mmap, dec_in_mmap, dec_tgt_mmap)\n",
                "\n",
                "# Load NWP char cache (shared encoder format)\n",
                "word_to_idx, idx_to_word, nwp_char_x_mmap, nwp_y_mmap = \\\n",
                "    load_nwp_char_cache(cache_paths)\n",
                "nwp_char_data = (nwp_char_x_mmap, nwp_y_mmap, word_to_idx)\n",
                "\n",
                "actual_char_vocab = len(char_to_idx)\n",
                "actual_word_vocab = len(word_to_idx)\n",
                "print(f\"\\nüìä Char vocab: {actual_char_vocab:,}\")\n",
                "print(f\"üìä Word vocab: {actual_word_vocab:,}\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5b. Preview Test Cases\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# ============================================================\n",
                "# üìã Log 100 Training Items (for testing)\n",
                "# Copy these to use as test cases in test_prediction.py\n",
                "# ============================================================\n",
                "\n",
                "import json as _json\n",
                "\n",
                "# Load test cases from cache\n",
                "_kkc_test_path = cache_paths.get('kkc_test_cases', '')\n",
                "_nwp_test_path = cache_paths.get('nwp_test_cases', '')\n",
                "\n",
                "# --- KKC Test Cases ---\n",
                "print('=' * 60)\n",
                "print('üìã KKC TEST CASES (Kana ‚Üí Kanji)')\n",
                "print('=' * 60)\n",
                "if os.path.exists(_kkc_test_path):\n",
                "    with open(_kkc_test_path, 'r', encoding='utf-8') as f:\n",
                "        _kkc_tests = _json.load(f)\n",
                "    for j, t in enumerate(_kkc_tests[:50]):\n",
                "        ctx = t.get('context', '')\n",
                "        ctx_str = f' [ctx: {ctx[:15]}]' if ctx else ''\n",
                "        print(f\"  {j+1:2d}. {t['kana']} ‚Üí {t['expected']}{ctx_str}\")\n",
                "    print(f'  Total: {len(_kkc_tests)} test cases')\n",
                "else:\n",
                "    print('  ‚ö† No KKC test cases found')\n",
                "\n",
                "# --- NWP Test Cases ---\n",
                "print()\n",
                "print('=' * 60)\n",
                "print('üìã NWP TEST CASES (Next Word Prediction)')\n",
                "print('=' * 60)\n",
                "if os.path.exists(_nwp_test_path):\n",
                "    with open(_nwp_test_path, 'r', encoding='utf-8') as f:\n",
                "        _nwp_tests = _json.load(f)\n",
                "    for j, t in enumerate(_nwp_tests[:50]):\n",
                "        ctx = ' '.join(t['context'])\n",
                "        print(f\"  {j+1:2d}. {ctx} ‚Üí {t['expected']}  [{t.get('sentence', '')[:20]}]\")\n",
                "    print(f'  Total: {len(_nwp_tests)} test cases')\n",
                "else:\n",
                "    print('  ‚ö† No NWP test cases found')\n",
                "\n",
                "# --- Sample Raw Data (encoder/decoder) ---\n",
                "print()\n",
                "print('=' * 60)\n",
                "print('üìã SAMPLE ENCODED DATA (first 10)')\n",
                "print('=' * 60)\n",
                "idx_to_char_local = {v: k for k, v in char_to_idx.items()}\n",
                "for j in range(min(10, len(enc_mmap))):\n",
                "    enc_chars = [idx_to_char_local.get(int(c), '?') for c in enc_mmap[j] if c != 0]\n",
                "    dec_chars = [idx_to_char_local.get(int(c), '?') for c in dec_tgt_mmap[j] if c != 0]\n",
                "    print(f\"  {j+1:2d}. {''.join(enc_chars)[:30]} ‚Üí {''.join(dec_chars)[:20]}\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Create Datasets\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.training import create_shared_datasets\n",
                "\n",
                "datasets, info = create_shared_datasets(kkc_data, nwp_char_data, config.BATCH_SIZE)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Build Shared Encoder Model\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.model import build_shared_multitask_model\n",
                "\n",
                "model = build_shared_multitask_model(\n",
                "    actual_char_vocab, actual_word_vocab, strategy=strategy\n",
                ")\n",
                "model.summary()\n",
                "\n",
                "# Compile (optimizer only ‚Äî custom training loop manages losses)\n",
                "if strategy:\n",
                "    with strategy.scope():\n",
                "        model.compile(optimizer=tf.keras.optimizers.Adam(\n",
                "            learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
                "        ))\n",
                "else:\n",
                "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
                "        learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
                "    ))\n",
                "\n",
                "print(f\"\\n‚úÖ Shared encoder model ready\")\n",
                "print(f\"   Inputs:  {len(model.inputs)} (encoder_input + decoder_input)\")\n",
                "print(f\"   Outputs: {len(model.outputs)} (kkc_output + nwp_output)\")\n",
                "print(f\"   Params:  {model.count_params():,}\")\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Train\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.training import train_shared_multitask\n",
                "\n",
                "history = train_shared_multitask(model, datasets, info)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Training Plots\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.plotting import plot_training_history\n",
                "\n",
                "plot_training_history(history)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save & Export\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from scripts.japanese_enhancement.export import save_model, export_tflite, list_saved_files\n",
                "\n",
                "save_model(model, char_to_idx, word_to_idx)\n",
                "export_tflite(model)\n",
                "list_saved_files()\n"
            ],
            "outputs": [],
            "execution_count": null
        }
    ]
}