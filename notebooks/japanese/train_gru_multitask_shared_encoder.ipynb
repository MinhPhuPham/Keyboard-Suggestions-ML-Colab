{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Multi-Task GRU ‚Äî Shared Encoder (v2)\n",
    "\n",
    "**Kana‚ÜíKanji Conversion (KKC) + Next Word Prediction (NWP)**\n",
    "\n",
    "## Architecture\n",
    "Both heads share one character-level encoder:\n",
    "```\n",
    "Input (char IDs) ‚Üí Shared BiGRU Encoder ‚Üí encoder_output\n",
    "                      ‚Üì                         ‚Üì\n",
    "               KKC Decoder (seq2seq)    NWP Head (attention+GRU)\n",
    "                      ‚Üì                         ‚Üì\n",
    "               kanji output              next_word prediction\n",
    "```\n",
    "\n",
    "- **KKC input**: hiragana chars (e.g., \"„Åç„Çá„ÅÜ„ÅØ„Å¶„Çì„Åç„Åå„ÅÑ„ÅÑ\")\n",
    "- **NWP input**: context chars with `<SEP>` markers (e.g., \"‰ªäÊó•<SEP>„ÅØ<SEP>Â§©Ê∞ó<SEP>„Åå\")\n",
    "- Both use same encoder ‚Äî encoder learns from BOTH tasks!\n",
    "\n",
    "## Training: Two Forward Passes\n",
    "Each step passes KKC data then NWP data through the **same encoder**.\n",
    "The shared encoder gets gradients from both tasks.\n",
    "\n",
    "## How to Run\n",
    "1. Set `TESTING_MODE = True` for quick validation (100K samples, 10 epochs)\n",
    "2. Set `TESTING_MODE = False` for full production training\n",
    "\n",
    "## Platform Support\n",
    "- **Colab**: Mounts Google Drive, clones repo\n",
    "- **Kaggle**: Clones repo, downloads dataset via `gdown`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pin versions for reproducibility\n",
    "!pip install tensorflow==2.20.0 keras==3.13.1 fugashi[unidic-lite] -q\n",
    "!pip install tqdm gdown -q\n",
    "\n",
    "import os, sys, gc\n",
    "import numpy as np\n",
    "\n",
    "# ===========================================================\n",
    "# PLATFORM DETECTION\n",
    "# ===========================================================\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    PLATFORM = 'kaggle'\n",
    "elif os.path.exists('/content'):\n",
    "    PLATFORM = 'colab'\n",
    "else:\n",
    "    PLATFORM = 'local'\n",
    "\n",
    "print(f\"üñ•Ô∏è Platform: {PLATFORM}\")\n",
    "\n",
    "# ===========================================================\n",
    "# MOUNT / CLONE\n",
    "# ===========================================================\n",
    "if PLATFORM == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    REPO_DIR = '/content/KeyboardSuggestionsML'\n",
    "elif PLATFORM == 'kaggle':\n",
    "    REPO_DIR = '/kaggle/working/KeyboardSuggestionsML'\n",
    "else:\n",
    "    REPO_DIR = os.path.expanduser('~/KeyboardSuggestionsML')\n",
    "\n",
    "# Clone/pull repo\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"TF: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.japanese_enhancement import config\n",
    "\n",
    "# ‚ö†Ô∏è TESTING MODE: Set False for full production training\n",
    "config.TESTING_MODE = True\n",
    "\n",
    "if config.TESTING_MODE:\n",
    "    config.MAX_SAMPLES = 2_000_000\n",
    "    config.MAX_NWP_PAIRS = 5_000_000\n",
    "    config.NUM_EPOCHS = 4\n",
    "    config.FORCE_REBUILD_CACHE = True\n",
    "    config.CACHE_SUFFIX = '_test'\n",
    "    print(f\"‚ö†Ô∏è TESTING MODE: ${config.MAX_SAMPLES} samples, ${config.NUM_EPOCHS} epochs\")\n",
    "else:\n",
    "    config.MAX_SAMPLES = 6_000_000\n",
    "    config.MAX_NWP_PAIRS = 10_000_000\n",
    "    config.NUM_EPOCHS = 10\n",
    "    config.CACHE_SUFFIX = ''\n",
    "    print(f\"üöÄ FULL TRAINING: ${config.MAX_SAMPLES} samples, ${config.NUM_EPOCHS} epochs\")\n",
    "\n",
    "# Single GPU ‚Äî batch size tuned for T4 (16GB)\n",
    "# 2 forward passes per step ‚Üí keep batch small to avoid OOM\n",
    "config.BATCH_SIZE = 512\n",
    "\n",
    "# Override paths for v2\n",
    "config.MODEL_DIR = f'{config.DRIVE_DIR}/models/multitask_v2'\n",
    "config.CACHE_DIR = f'{config.DRIVE_DIR}/cache/multitask_v2'\n",
    "config.ensure_dirs()\n",
    "\n",
    "# ===========================================================\n",
    "# DOWNLOAD DATASET (if not exists)\n",
    "# ===========================================================\n",
    "dataset_path = f'{config.DATASET_DIR}/ime_dataset_10m.jsonl'\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"üì• Downloading dataset to {dataset_path}...\")\n",
    "    os.makedirs(config.DATASET_DIR, exist_ok=True)\n",
    "    !gdown \"1b5YgqVUEU2HGlkPBcSUIjL1XyJ7n8Cgg\" -O {dataset_path}\n",
    "    print(f\"‚úÖ Dataset downloaded! Size: {os.path.getsize(dataset_path) / 1e6:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset exists: {dataset_path} ({os.path.getsize(dataset_path) / 1e6:.1f} MB)\")\n",
    "\n",
    "config.print_config()\n",
    "\n",
    "# Cache paths\n",
    "cache_paths = config.get_cache_paths(config.CACHE_DIR, config.CACHE_SUFFIX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify single GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úÖ GPU: {gpus[0].name}\")\n",
    "    # Enable memory growth to avoid pre-allocating all VRAM\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"   Memory growth enabled\")\n",
    "    except RuntimeError:\n",
    "        pass  # Already set\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU ‚Äî training will be slow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Cache Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload data_loader & tokenizer to pick up latest code after git pull\n",
    "# NOTE: Do NOT reload config ‚Äî that would reset user overrides from Cell 2!\n",
    "import importlib\n",
    "import scripts.japanese_enhancement.tokenizer as _tok_mod\n",
    "import scripts.japanese_enhancement.data_loader as _dl_mod\n",
    "importlib.reload(_tok_mod)\n",
    "importlib.reload(_dl_mod)\n",
    "\n",
    "from scripts.japanese_enhancement.data_loader import (\n",
    "    check_cache, load_raw_dataset,\n",
    "    build_kkc_cache, build_nwp_char_cache,\n",
    "    load_kkc_cache, load_nwp_char_cache,\n",
    ")\n",
    "\n",
    "# Check existing cache\n",
    "kkc_ready, _ = check_cache(cache_paths)\n",
    "nwp_char_ready = os.path.exists(cache_paths.get('nwp_char_x', ''))\n",
    "\n",
    "force = config.FORCE_REBUILD_CACHE\n",
    "need_build = not kkc_ready or not nwp_char_ready or force\n",
    "print(f'FORCE_REBUILD={force}, kkc_ready={kkc_ready}, nwp_ready={nwp_char_ready}, need_build={need_build}')\n",
    "\n",
    "if need_build:\n",
    "    if force:\n",
    "        print('\\U0001f504 Force rebuild enabled \\u2014 rebuilding all caches...')\n",
    "    print('\\n\\U0001f4e5 Loading raw data...')\n",
    "    training_data = load_raw_dataset(config.MAX_SAMPLES)\n",
    "\n",
    "    if not kkc_ready or force:\n",
    "        c2i, _ = build_kkc_cache(training_data, cache_paths)\n",
    "    else:\n",
    "        c2i, _, _, _, _ = load_kkc_cache(cache_paths)\n",
    "\n",
    "    if not nwp_char_ready or force:\n",
    "        build_nwp_char_cache(training_data, cache_paths, c2i)\n",
    "\n",
    "    del training_data\n",
    "    gc.collect()\n",
    "    print('\\n\\u2705 Cache ready!')\n",
    "else:\n",
    "    print('\\u2705 Cache already exists, skipping build')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Cached Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KKC cache\n",
    "char_to_idx, idx_to_char, enc_mmap, dec_in_mmap, dec_tgt_mmap = \\\n",
    "    load_kkc_cache(cache_paths)\n",
    "kkc_data = (enc_mmap, dec_in_mmap, dec_tgt_mmap)\n",
    "\n",
    "# Load NWP char cache (shared encoder format)\n",
    "word_to_idx, idx_to_word, nwp_char_x_mmap, nwp_y_mmap = \\\n",
    "    load_nwp_char_cache(cache_paths)\n",
    "nwp_char_data = (nwp_char_x_mmap, nwp_y_mmap, word_to_idx)\n",
    "\n",
    "actual_char_vocab = len(char_to_idx)\n",
    "actual_word_vocab = len(word_to_idx)\n",
    "print(f\"\\nüìä Char vocab: {actual_char_vocab:,}\")\n",
    "print(f\"üìä Word vocab: {actual_word_vocab:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Preview Test Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìã Log 100 Training Items (for testing)\n",
    "# Copy these to use as test cases in test_prediction.py\n",
    "# ============================================================\n",
    "\n",
    "import json as _json\n",
    "\n",
    "# Load test cases from cache\n",
    "_kkc_test_path = cache_paths.get('kkc_test_cases', '')\n",
    "_nwp_test_path = cache_paths.get('nwp_test_cases', '')\n",
    "\n",
    "# --- KKC Test Cases ---\n",
    "print('=' * 60)\n",
    "print('üìã KKC TEST CASES (Kana ‚Üí Kanji)')\n",
    "print('=' * 60)\n",
    "if os.path.exists(_kkc_test_path):\n",
    "    with open(_kkc_test_path, 'r', encoding='utf-8') as f:\n",
    "        _kkc_tests = _json.load(f)\n",
    "    for j, t in enumerate(_kkc_tests[:50]):\n",
    "        ctx = t.get('context', '')\n",
    "        ctx_str = f' [ctx: {ctx[:15]}]' if ctx else ''\n",
    "        print(f\"  {j+1:2d}. {t['kana']} ‚Üí {t['expected']}{ctx_str}\")\n",
    "    print(f'  Total: {len(_kkc_tests)} test cases')\n",
    "else:\n",
    "    print('  ‚ö† No KKC test cases found')\n",
    "\n",
    "# --- NWP Test Cases ---\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('üìã NWP TEST CASES (Next Word Prediction)')\n",
    "print('=' * 60)\n",
    "if os.path.exists(_nwp_test_path):\n",
    "    with open(_nwp_test_path, 'r', encoding='utf-8') as f:\n",
    "        _nwp_tests = _json.load(f)\n",
    "    for j, t in enumerate(_nwp_tests[:50]):\n",
    "        ctx = ' '.join(t['context'])\n",
    "        print(f\"  {j+1:2d}. {ctx} ‚Üí {t['expected']}  [{t.get('sentence', '')[:20]}]\")\n",
    "    print(f'  Total: {len(_nwp_tests)} test cases')\n",
    "else:\n",
    "    print('  ‚ö† No NWP test cases found')\n",
    "\n",
    "# --- Sample Raw Data (encoder/decoder) ---\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('üìã SAMPLE ENCODED DATA (first 10)')\n",
    "print('=' * 60)\n",
    "idx_to_char_local = {v: k for k, v in char_to_idx.items()}\n",
    "for j in range(min(10, len(enc_mmap))):\n",
    "    enc_chars = [idx_to_char_local.get(int(c), '?') for c in enc_mmap[j] if c != 0]\n",
    "    dec_chars = [idx_to_char_local.get(int(c), '?') for c in dec_tgt_mmap[j] if c != 0]\n",
    "    print(f\"  {j+1:2d}. {''.join(enc_chars)[:30]} ‚Üí {''.join(dec_chars)[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import scripts.japanese_enhancement.training as _train_mod\n",
    "importlib.reload(_train_mod)\n",
    "from scripts.japanese_enhancement.training import create_shared_datasets\n",
    "\n",
    "datasets, info = create_shared_datasets(kkc_data, nwp_char_data, config.BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Shared Encoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import scripts.japanese_enhancement.model as _model_mod\n",
    "importlib.reload(_model_mod)\n",
    "from scripts.japanese_enhancement.model import build_shared_multitask_model\n",
    "\n",
    "model = build_shared_multitask_model(\n",
    "    actual_char_vocab, actual_word_vocab, strategy=None\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=config.LEARNING_RATE, clipnorm=1.0\n",
    "))\n",
    "\n",
    "print(f\"\\n\\u2705 Shared encoder model ready\")\n",
    "print(f\"   Inputs:  {len(model.inputs)} (encoder_input + decoder_input)\")\n",
    "print(f\"   Outputs: {len(model.outputs)} (kkc_output + nwp_output)\")\n",
    "print(f\"   Params:  {model.count_params():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import scripts.japanese_enhancement.training as _train_mod\n",
    "importlib.reload(_train_mod)\n",
    "from scripts.japanese_enhancement.training import train_shared_multitask\n",
    "\n",
    "history = train_shared_multitask(model, datasets, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.japanese_enhancement.plotting import plot_training_history\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save & Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.japanese_enhancement.export import save_model, export_tflite, list_saved_files\n",
    "\n",
    "save_model(model, char_to_idx, word_to_idx)\n",
    "export_tflite(model)\n",
    "list_saved_files()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
