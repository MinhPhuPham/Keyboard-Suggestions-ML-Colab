{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Dataset Quality Filter for Japanese Keyboard IME\n",
                "\n",
                "Filter the zenz dataset to keep high-quality, daily conversation text.\n",
                "\n",
                "**3 Filter Layers:**\n",
                "1. **Kill List** ‚Äî Remove toxic/useless patterns (forum artifacts, URLs in kana, Cyrillic, etc.)\n",
                "2. **Fix List** ‚Äî Clean/normalize data (bad left_context, cut-off sentences)\n",
                "3. **Quality Score** ‚Äî Score remaining items for daily conversation relevance\n",
                "\n",
                "**Output:** Filtered high-quality JSONL in Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect platform\n",
                "if 'COLAB_RELEASE_TAG' in os.environ:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Local'\n",
                "    DRIVE_DIR = './output'\n",
                "\n",
                "OUTPUT_DIR = f\"{DRIVE_DIR}/filtered_data\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Output: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q datasets tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "\n",
                "# Processing limits\n",
                "MAX_ITEMS = None  # None = ALL items, or e.g. 100_000 for testing\n",
                "\n",
                "# Quality threshold (0-10 score)\n",
                "MIN_QUALITY_SCORE = 4  # Keep items scoring >= this\n",
                "\n",
                "print(f\"Min quality score: {MIN_QUALITY_SCORE}\")\n",
                "print(f\"Items limit: {'ALL' if MAX_ITEMS is None else f'{MAX_ITEMS:,}'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Kill List ‚Äî Must Remove\n",
                "\n",
                "These patterns destroy keyboard quality. Instant reject, no scoring needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "# ==========================================================\n",
                "# KILL LIST: Patterns that MUST be removed\n",
                "# ==========================================================\n",
                "\n",
                "# A. Forum / Scraping Artifacts\n",
                "FORUM_PATTERNS = re.compile(\n",
                "    r'(ID:|„Ç¢„Ç§„Éá„Ç£:|ÊäïÁ®øÊó•|ÊäïÁ®ø:|ÂêçÂâç:|ÂêçÁÑ°„Åó|'\n",
                "    r'\\d{2}:\\d{2}:\\d{2}|'           # Timestamps 23:06:08\n",
                "    r'\\d{4}/\\d{1,2}/\\d{1,2}|'       # Dates 2010/12/15\n",
                "    r'\\(\\w\\)\\s*\\d{2}:\\d{2}|'        # Forum date format („Ç´) 10:55\n",
                "    r'„Éä„Éû„Ç®„Éä„Ç§„É®|'                  # Anonymous username\n",
                "    r'„É¨„ÇπÊï∞|„Çπ„É¨„ÉÉ„Éâ|'               # Thread/reply terms\n",
                "    r'^##\\s|^‚ñ†|^‚óè|^‚óÜ)',             # Markdown/structured headers\n",
                "    re.MULTILINE\n",
                ")\n",
                "\n",
                "# B. URLs spelled in Kana (useless for IME)\n",
                "KANA_URL_PATTERNS = re.compile(\n",
                "    r'(„Ç®„Ç§„ÉÅ„ÉÜ„Ç£„Éº„ÉÜ„Ç£„Éº„Éî„Éº|'       # http\n",
                "    r'„ÉÄ„Éñ„É™„É•„ÉÄ„Éñ„É™„É•|'              # www\n",
                "    r'„Éâ„ÉÉ„Éà„Ç≥„É†|„Éâ„ÉÉ„Éà„Ç∏„Çß„Éº„Éî„Éº|'    # .com, .jp\n",
                "    r'„É¶„Éº„Ç¢„Éº„É´„Ç®„É´|'                # URL\n",
                "    r'http[s]?://|'                   # Actual URLs in output\n",
                "    r'www\\.)',\n",
                "    re.IGNORECASE\n",
                ")\n",
                "\n",
                "# C. Foreign Script Garbage (Cyrillic, extended Latin, etc.)\n",
                "#    Keep basic ASCII (English is popular in Japanese!)\n",
                "GARBAGE_CHARS = re.compile(\n",
                "    r'[\\u0400-\\u04FF'     # Cyrillic\n",
                "    r'\\u0370-\\u03FF'      # Greek (except common math)\n",
                "    r'\\u0100-\\u024F'      # Extended Latin (√Ö, √∂, etc.)\n",
                "    r'\\u0250-\\u02AF'      # IPA extensions\n",
                "    r'\\u2600-\\u26FF'      # Misc symbols (but NOT emoji)\n",
                "    r']'\n",
                ")\n",
                "\n",
                "# D. ASCII art / broken encoding patterns\n",
                "ASCII_ART_RE = re.compile(\n",
                "    r'([‚îÄ‚îÅ‚îÇ‚îÉ‚îå‚îê‚îî‚îò‚îú‚î§‚î¨‚î¥‚îº]{3,}|'   # Box drawing\n",
                "    r'[=\\-]{5,}|'                 # Long separators\n",
                "    r'[\\*]{3,}|'                  # Stars\n",
                "    r'[_]{5,})',                   # Underscores\n",
                ")\n",
                "\n",
                "# E. Punctuation-only output (zero learning value)\n",
                "PUNCT_ONLY_RE = re.compile(r'^[„ÄÅ„ÄÇÔºüÔºÅ?!\\s\\d.,;:()ÔºàÔºâ„Äå„Äç„Äé„Äè„Äê„Äë„Éª]+$')\n",
                "\n",
                "\n",
                "def is_killed(item):\n",
                "    \"\"\"Check if item should be killed (removed). Returns (True, reason) or (False, None).\"\"\"\n",
                "    inp = item.get('input', '') or ''\n",
                "    out = item.get('output', '') or ''\n",
                "    ctx = item.get('left_context', '') or ''\n",
                "    full = ctx + out\n",
                "    \n",
                "    # Empty\n",
                "    if not inp or not out:\n",
                "        return True, 'empty'\n",
                "    \n",
                "    # Forum artifacts\n",
                "    if FORUM_PATTERNS.search(full):\n",
                "        return True, 'forum_artifact'\n",
                "    \n",
                "    # Kana URLs\n",
                "    if KANA_URL_PATTERNS.search(inp) or KANA_URL_PATTERNS.search(out):\n",
                "        return True, 'kana_url'\n",
                "    \n",
                "    # Foreign script garbage (NOT basic ASCII ‚Äî English is OK!)\n",
                "    if GARBAGE_CHARS.search(out):\n",
                "        return True, 'garbage_chars'\n",
                "    \n",
                "    # ASCII art / broken encoding\n",
                "    if ASCII_ART_RE.search(out):\n",
                "        return True, 'ascii_art'\n",
                "    \n",
                "    # Punctuation-only output\n",
                "    if PUNCT_ONLY_RE.match(out):\n",
                "        return True, 'punct_only'\n",
                "    \n",
                "    return False, None\n",
                "\n",
                "\n",
                "# Quick test\n",
                "print(\"üß™ Kill List test:\")\n",
                "kill_tests = [\n",
                "    {'input': '„Éä„Éû„Ç®', 'output': 'ÂêçÂâç: „ÉÜ„Çπ„Éà ID:123', 'left_context': ''},\n",
                "    {'input': '„Ç®„Ç§„ÉÅ„ÉÜ„Ç£„Éº„ÉÜ„Ç£„Éº„Éî„Éº„Ç®„Çπ', 'output': 'https://example.com', 'left_context': ''},\n",
                "    {'input': '„ÉÜ„Çπ„Éà', 'output': '–Ö–ú–ê–Ø', 'left_context': ''},\n",
                "    {'input': '„ÉÜ„É≥', 'output': '„ÄÅ', 'left_context': ''},\n",
                "    {'input': '„Åç„Çá„ÅÜ', 'output': '‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠', 'left_context': ''},\n",
                "    {'input': '„Ç¢„Éó„É™', 'output': 'app„Çí‰Ωø„Å£„Å¶', 'left_context': ''},  # English OK!\n",
                "]\n",
                "for t in kill_tests:\n",
                "    killed, reason = is_killed(t)\n",
                "    status = f'‚ùå KILL ({reason})' if killed else '‚úÖ KEEP'\n",
                "    print(f\"  {status}: {t['input']} ‚Üí {t['output'][:30]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Fix List ‚Äî Clean & Normalize\n",
                "\n",
                "Don't delete these ‚Äî fix them to make the IME smarter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================================\n",
                "# FIX LIST: Clean/normalize data\n",
                "# ==========================================================\n",
                "\n",
                "# Bad left_context patterns (set to empty)\n",
                "BAD_CONTEXT_PATTERNS = re.compile(\n",
                "    r'(^ID:|^„Ç¢„Ç§„Éá„Ç£:|^ÊäïÁ®øÊó•|^ÂêçÂâç:|'\n",
                "    r'^\\d{4}[/\\-]\\d{1,2}[/\\-]\\d{1,2}|'   # Date-only context (2023/10/01)\n",
                "    r'^\\d{2}:\\d{2}:\\d{2}|'                  # Time-only context\n",
                "    r'http[s]?://|'                          # URL context\n",
                "    r'^[A-Z]{5,}$)',                         # All-caps codes\n",
                "    re.IGNORECASE\n",
                ")\n",
                "\n",
                "# Particles that can't start a sentence without context\n",
                "STARTING_PARTICLES = {'„Å¶', '„Å´', '„Çí', '„ÅØ', '„Åå', '„Åß', '„Å®', '„ÇÇ',\n",
                "                       '„Å∏', '„Åã„Çâ', '„Åæ„Åß', '„Çà„Çä', '„ÅÆ„Åß', '„ÅÆ„Å´',\n",
                "                       '„Åë„Å©', '„Åë„Çå„Å©', '„Å™„Åå„Çâ'}\n",
                "\n",
                "\n",
                "def fix_item(item):\n",
                "    \"\"\"Clean and normalize an item. Returns fixed item.\"\"\"\n",
                "    result = {\n",
                "        'input': item.get('input', '') or '',\n",
                "        'output': item.get('output', '') or '',\n",
                "        'left_context': item.get('left_context', '') or '',\n",
                "    }\n",
                "    \n",
                "    ctx = result['left_context']\n",
                "    out = result['output']\n",
                "    \n",
                "    # Fix 1: Clean bad left_context ‚Üí set to empty\n",
                "    if ctx and BAD_CONTEXT_PATTERNS.search(ctx):\n",
                "        result['left_context'] = ''\n",
                "        result['_fixed'] = 'bad_context_cleared'\n",
                "    \n",
                "    # Fix 2: Cut-off sentences ‚Äî output starts with particle but no context\n",
                "    #         \"„Å¶Á∂ôÁ∂ö„Åó„Å¶\" with no context ‚Üí meaningless fragment\n",
                "    if not result['left_context']:\n",
                "        first_chars = out[:3]\n",
                "        if any(first_chars.startswith(p) for p in STARTING_PARTICLES):\n",
                "            # This is a cut-off fragment without context\n",
                "            result['_cutoff'] = True\n",
                "    \n",
                "    # Fix 3: Strip whitespace\n",
                "    result['left_context'] = result['left_context'].strip()\n",
                "    result['output'] = result['output'].strip()\n",
                "    result['input'] = result['input'].strip()\n",
                "    \n",
                "    return result\n",
                "\n",
                "\n",
                "# Quick test\n",
                "print(\"üß™ Fix List test:\")\n",
                "fix_tests = [\n",
                "    {'input': '„Ç¢„É°', 'output': 'Èõ®„Å†„Å£„Åü', 'left_context': '2023/10/01'},\n",
                "    {'input': '„Ç±„Ç§„Çæ„ÇØ', 'output': '„Å¶Á∂ôÁ∂ö„Åó„Å¶', 'left_context': ''},\n",
                "    {'input': '„Ç¢„É°', 'output': 'Èõ®„Å†„Å£„Åü', 'left_context': 'Êò®Êó•„ÅØ'},\n",
                "]\n",
                "for t in fix_tests:\n",
                "    fixed = fix_item(t)\n",
                "    ctx_before = t['left_context'] or '(none)'\n",
                "    ctx_after = fixed['left_context'] or '(none)'\n",
                "    flags = []\n",
                "    if fixed.get('_fixed'): flags.append(f\"üîß {fixed['_fixed']}\")\n",
                "    if fixed.get('_cutoff'): flags.append('‚ö†Ô∏è cutoff')\n",
                "    flag_str = ' '.join(flags) if flags else '‚úÖ OK'\n",
                "    print(f\"  {flag_str}: ctx [{ctx_before}] ‚Üí [{ctx_after}] | {fixed['output'][:20]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Quality Score ‚Äî Daily Conversation Relevance\n",
                "\n",
                "Score 0-10 based on how useful this item is for keyboard IME."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "# ==========================================================\n",
                "# Daily vocabulary (for positive scoring)\n",
                "# ==========================================================\n",
                "\n",
                "# Common daily verbs (stem forms to match conjugations)\n",
                "DAILY_VERBS = {\n",
                "    '„Åô„Çã', '„Åó', '„Åï', '„ÅÑ„Çã', '„ÅÑ', '„ÅÇ„Çã', '„ÅÇ„Å£', '„Å™„Çã', '„Å™„Å£',\n",
                "    '„Åß„Åç„Çã', '„Åß„Åç', '„ÇÑ„Çã', '„ÇÑ„Å£',\n",
                "    'Ë°å„Åè', 'Ë°å„Åç', 'Ë°å„Å£', 'Êù•„Çã', 'Êù•„Åü', 'Â∏∞„Çã', 'Â∏∞„Å£',\n",
                "    'Ë¶ã„Çã', 'Ë¶ã„Åü', 'Ë¶ã„Åà', 'ËÅû„Åè', 'ËÅû„ÅÑ', 'Ë™≠„ÇÄ', 'Ë™≠„Çì',\n",
                "    'È£ü„Åπ', 'È£≤„ÇÄ', 'È£≤„Çì', 'Ë≤∑„ÅÜ', 'Ë≤∑„Å£', '‰Ωú„Çã', '‰Ωú„Å£',\n",
                "    '‰Ωø„ÅÜ', '‰Ωø„Å£', 'ÊåÅ„Å§', 'ÊåÅ„Å£', 'ÂÖ•„Çã', 'ÂÖ•„Å£', 'Âá∫„Çã', 'Âá∫„Åó',\n",
                "    'ÊÄù„ÅÜ', 'ÊÄù„Å£', 'ËÄÉ„Åà', 'Áü•„Çã', 'Áü•„Å£', 'ÂàÜ„Åã„Çã', 'ÂàÜ„Åã„Å£',\n",
                "    'Ë®Ä„ÅÜ', 'Ë®Ä„Å£', 'Ë©±„Åô', 'Ë©±„Åó', 'Êõ∏„Åè', 'Êõ∏„ÅÑ', 'Êïô„Åà',\n",
                "    '‰Ωè„ÇÄ', '‰Ωè„Çì', 'ÂÉç„Åè', 'ÂÉç„ÅÑ', 'Âßã„ÇÅ', 'ÁµÇ„Çè',\n",
                "    'ÂæÖ„Å§', 'ÂæÖ„Å£', 'ÈÄÅ„Çã', 'ÈÄÅ„Å£', 'Âèó„Åë', 'Âèñ„Çã', 'Âèñ„Å£',\n",
                "    'ÈÅä„Å∂', 'ÈÅä„Çì', 'Ëµ∞„Çã', 'Ëµ∞„Å£', 'Ê≠©„Åè', 'Ê≠©„ÅÑ',\n",
                "    'Ëµ∑„Åç', 'ÂØù„Çã', 'ÂØù„Åü', 'Èñã„Åè', 'Èñã„ÅÑ', 'Èñâ„ÇÅ',\n",
                "    'Â§â„Çè„Çã', 'Â§â„Çè„Å£', 'Ê±∫„ÇÅ', 'ÈÅ∏„Å∂', 'ÈÅ∏„Çì',\n",
                "    '‰ºö„ÅÜ', '‰ºö„Å£', 'Âëº„Å∂', 'Âëº„Çì', 'Êâï„ÅÜ', 'Êâï„Å£',\n",
                "    'Á´ã„Å§', 'Á´ã„Å£', 'Â∫ß„Çã', 'Â∫ß„Å£', 'ÁΩÆ„Åè', 'ÁΩÆ„ÅÑ',\n",
                "    'Â•Ω„Åç', 'Â´å„ÅÑ', 'Ê¨≤„Åó„ÅÑ', 'Ê•Ω„Åó„ÅÑ', 'Â¨â„Åó„ÅÑ',\n",
                "    'ÂøÖË¶Å', 'Â§ßÂàá', 'Â§ß‰∫ã', 'Á∞°Âçò', 'Èõ£„Åó„ÅÑ',\n",
                "}\n",
                "\n",
                "# Common daily nouns\n",
                "DAILY_NOUNS = {\n",
                "    '‰∫∫', '‰∫ã', 'Áâ©', 'ÊâÄ', 'Êñπ', 'ÊôÇ', 'Êó•', 'Âπ¥', 'Êúà', 'Ââç',\n",
                "    'Âæå', '‰∏≠', '‰∏ä', '‰∏ã', 'Èñì', 'Ê∞ó', 'ÁõÆ', 'Êâã', 'È†≠', 'ÂøÉ',\n",
                "    'ÂêçÂâç', '‰ªï‰∫ã', 'Â≠¶Ê†°', '‰ºöÁ§æ', 'ÂÆ∂', 'ÈÉ®Â±ã', 'ÈõªËªä', 'ÈßÖ',\n",
                "    'ÂèãÈÅî', 'ÂÆ∂Êóè', 'Â≠ê‰æõ', 'ÂÖàÁîü', 'Â≠¶Áîü', 'ÂΩº', 'ÂΩºÂ•≥', 'Ëá™ÂàÜ',\n",
                "    'Êúù', 'Êòº', 'Â§ú', '‰ªäÊó•', 'ÊòéÊó•', 'Êò®Êó•', '‰ªä', 'ÈÄ±Êú´',\n",
                "    'Â§©Ê∞ó', 'Èõ®', 'Èõ™', 'Êò•', 'Â§è', 'Áßã', 'ÂÜ¨',\n",
                "    'È£ü‰∫ã', 'ÊñôÁêÜ', 'Ê∞¥', '„ÅäËå∂', '„Ç≥„Éº„Éí„Éº', '„ÅîÈ£Ø',\n",
                "    'ÈõªË©±', '„É°„Éº„É´', 'ÂÜôÁúü', 'Êò†Áîª', 'Èü≥Ê•Ω', 'Êú¨', '„Ç≤„Éº„É†',\n",
                "    'Â∫ó', 'ÁóÖÈô¢', 'ÂÖ¨Âúí', 'ÈÅì', 'Áî∫', 'ÂõΩ', 'Â†¥ÊâÄ',\n",
                "    'ÂïèÈ°å', 'Ë≥™Âïè', 'Á≠î„Åà', 'ÊÑèÂë≥', 'ÁêÜÁî±', 'ÁµêÊûú',\n",
                "    'Ë©±', 'Ë®ÄËëâ', 'Â£∞', '‰Ωì', 'È°î', 'Âè£',\n",
                "    'Ëªä', 'Ëä±', 'Áå´', 'Áä¨', 'Á©∫', 'Êµ∑', 'Â±±',\n",
                "    '„ÅäÈáë', 'ÊôÇÈñì', 'Ë∂£Âë≥', 'ÁµåÈ®ì', 'Ê∞óÊåÅ„Å°', 'ÁîüÊ¥ª',\n",
                "    '„Çπ„Éû„Éõ', '„Ç¢„Éó„É™', '„Éó„É≠„Ç∞„É©„É†', '„Éá„Éº„Çø', '„Çµ„Ç§„Éà',\n",
                "}\n",
                "\n",
                "# Polite/conversational markers\n",
                "POLITE_MARKERS = {\n",
                "    '„Åß„Åô', '„Åæ„Åô', '„Åß„Åó„Åü', '„Åæ„Åó„Åü', '„Åæ„Åõ„Çì', '„Åè„Å†„Åï„ÅÑ',\n",
                "    '„Åß„Åó„Çá„ÅÜ', '„Åæ„Åó„Çá„ÅÜ', '„Åß„Åô„Åã', '„Åæ„Åô„Åã',\n",
                "    '„Å†', '„Å†„Å£„Åü', '„Å†„Çç„ÅÜ', '„Åã„Å™', '„Çà„Å≠', '„Å≠', '„Çà',\n",
                "    '„Åë„Å©', '„Åë„Çå„Å©', '„ÅÆ„Åß', '„Åã„Çâ', '„Åü„Çâ', '„Å¶„ÇÇ',\n",
                "    '„Å™„Çä„Åæ„Åó„Åü', '„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åô„Åø„Åæ„Åõ„Çì', '„ÅäÈ°ò„ÅÑ',\n",
                "}\n",
                "\n",
                "# Encyclopedia patterns (reduce score)\n",
                "ENCYCLOPEDIA_PATTERNS = re.compile(\n",
                "    r'(„Å´‰ΩçÁΩÆ„Åô„Çã|„Å´ÊâÄÂú®„Åô„Çã|'          # Location descriptions\n",
                "    r'ÂåóÁ∑Ø\\d+|ÂçóÁ∑Ø\\d+|Êù±Áµå\\d+|Ë•øÁµå\\d+|'  # Coordinates\n",
                "    r'ISBN|ISSN|'                        # Book codes\n",
                "    r'Á¥ÄÂÖÉÂâç\\d|'                         # BC dates\n",
                "    r'‰∫∫Âè£„ÅØÁ¥Ñ?\\d|Èù¢Á©ç„ÅØÁ¥Ñ?\\d|'          # Population/area stats\n",
                "    r'Ê®ôÈ´ò\\d|Êµ∑Êäú\\d|'                    # Elevation\n",
                "    r'Â≠¶Âêç|ÂàÜÈ°ûÂ≠¶|'                      # Scientific taxonomy\n",
                "    r'Á¨¨\\d+‰ª£|Á¨¨\\d+Âõû|'                  # Ordinal titles\n",
                "    r'Êù°Á¥Ñ|ÂãÖ‰ª§|Ê≥ï‰ª§)',                   # Legal terms\n",
                ")\n",
                "\n",
                "# Number density pattern\n",
                "NUMBER_RE = re.compile(r'\\d+')\n",
                "\n",
                "\n",
                "def score_quality(item):\n",
                "    \"\"\"Score an item 0-10 for daily conversation relevance.\n",
                "    \n",
                "    Note: item should already be passed through fix_item().\n",
                "    Returns: (score, reasons_list)\n",
                "    \"\"\"\n",
                "    ctx = item.get('left_context', '') or ''\n",
                "    inp = item.get('input', '') or ''\n",
                "    out = item.get('output', '') or ''\n",
                "    full_text = ctx + out\n",
                "    \n",
                "    score = 5  # Start neutral\n",
                "    reasons = []\n",
                "    \n",
                "    # --- NEGATIVE SIGNALS ---\n",
                "    \n",
                "    # Cut-off fragment (particle start without context)\n",
                "    if item.get('_cutoff'):\n",
                "        score -= 2\n",
                "        reasons.append('cutoff_fragment')\n",
                "    \n",
                "    # Very long text (likely encyclopedia paragraphs)\n",
                "    if len(full_text) > 140:\n",
                "        score -= 1\n",
                "        reasons.append('very_long')\n",
                "    \n",
                "    # Encyclopedia patterns\n",
                "    if ENCYCLOPEDIA_PATTERNS.search(full_text):\n",
                "        score -= 2\n",
                "        reasons.append('encyclopedia')\n",
                "    \n",
                "    # High number density (3+ number groups = statistics)\n",
                "    numbers = NUMBER_RE.findall(full_text)\n",
                "    if len(numbers) >= 3:\n",
                "        score -= 2\n",
                "        reasons.append(f'many_numbers({len(numbers)})')\n",
                "    elif len(numbers) >= 2:\n",
                "        score -= 1\n",
                "        reasons.append(f'numbers({len(numbers)})')\n",
                "    \n",
                "    # Long consecutive numbers (serial codes, etc.)\n",
                "    if re.search(r'\\d{5,}', full_text):\n",
                "        score -= 2\n",
                "        reasons.append('long_number')\n",
                "    \n",
                "    # Many brackets (technical notation)\n",
                "    brackets = sum(full_text.count(c) for c in '(Ôºà[„Äê')\n",
                "    if brackets >= 3:\n",
                "        score -= 1\n",
                "        reasons.append(f'brackets({brackets})')\n",
                "    \n",
                "    # --- POSITIVE SIGNALS ---\n",
                "    \n",
                "    # Contains daily verbs\n",
                "    daily_verb_count = sum(1 for v in DAILY_VERBS if v in full_text)\n",
                "    if daily_verb_count >= 2:\n",
                "        score += 2\n",
                "        reasons.append(f'daily_verbs({daily_verb_count})')\n",
                "    elif daily_verb_count >= 1:\n",
                "        score += 1\n",
                "        reasons.append('daily_verb')\n",
                "    \n",
                "    # Contains daily nouns\n",
                "    daily_noun_count = sum(1 for n in DAILY_NOUNS if n in full_text)\n",
                "    if daily_noun_count >= 2:\n",
                "        score += 1\n",
                "        reasons.append(f'daily_nouns({daily_noun_count})')\n",
                "    \n",
                "    # Polite/conversational markers\n",
                "    polite_count = sum(1 for p in POLITE_MARKERS if p in full_text)\n",
                "    if polite_count >= 1:\n",
                "        score += 1\n",
                "        reasons.append(f'polite({polite_count})')\n",
                "    \n",
                "    # Natural sentence ending\n",
                "    if full_text and full_text[-1] in '„ÄÇÔºÅÔºü„Å≠„ÄÅ„Çà„Åã':\n",
                "        score += 1\n",
                "        reasons.append('natural_end')\n",
                "    \n",
                "    # Good output length (natural phrase)\n",
                "    if 3 <= len(out) <= 50:\n",
                "        score += 1\n",
                "        reasons.append('good_out_len')\n",
                "    \n",
                "    # Has meaningful left_context (provides training signal)\n",
                "    if len(ctx) >= 2:\n",
                "        score += 1\n",
                "        reasons.append('has_context')\n",
                "    \n",
                "    # Mixed script (natural Japanese + katakana/English)\n",
                "    has_hiragana = bool(re.search(r'[\\u3040-\\u309F]', full_text))\n",
                "    has_katakana = bool(re.search(r'[\\u30A0-\\u30FF]', full_text))\n",
                "    has_kanji = bool(re.search(r'[\\u4E00-\\u9FFF]', full_text))\n",
                "    if has_hiragana and has_kanji:\n",
                "        score += 1\n",
                "        reasons.append('mixed_script')\n",
                "    \n",
                "    # Clamp to 0-10\n",
                "    score = max(0, min(10, score))\n",
                "    \n",
                "    return score, reasons\n",
                "\n",
                "\n",
                "# Quick test\n",
                "print(\"üß™ Quality Score test:\")\n",
                "score_tests = [\n",
                "    {'input': '„Åç„Çá„ÅÜ', 'output': '‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠', 'left_context': ''},\n",
                "    {'input': '„Åª„Åè„ÅÑ', 'output': 'ÂåóÁ∑Ø35Â∫¶12ÂàÜ„Å´‰ΩçÁΩÆ„Åô„Çã', 'left_context': ''},\n",
                "    {'input': '„ÅÑ„Å£„Åó„Çá', 'output': '‰∏ÄÁ∑í„Å´Êò†Áîª„ÇíË¶ã„Å´Ë°å„Åç„Åæ„Åó„Åü', 'left_context': 'ÂèãÈÅî„Å®'},\n",
                "    {'input': '„Åó', 'output': '„Å¶Á∂ôÁ∂ö„Åó„Å¶', 'left_context': '', '_cutoff': True},\n",
                "    {'input': '„Ç¢„Éó„É™', 'output': 'app„Çí‰Ωø„Å£„Å¶Ë¶ã„Çã', 'left_context': '„Çπ„Éû„Éõ„ÅÆ'},\n",
                "    {'input': '„Å†„ÅÑ', 'output': 'Á¨¨35‰ª£Â§ßÁµ±È†ò„ÅÆ', 'left_context': ''},\n",
                "    {'input': '„ÅÇ„Å§', 'output': 'Êöë„ÅÑ', 'left_context': '‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇ'},  # Short output OK!\n",
                "]\n",
                "for t in score_tests:\n",
                "    s, r = score_quality(t)\n",
                "    status = '‚úÖ' if s >= MIN_QUALITY_SCORE else '‚ùå'\n",
                "    ctx = t.get('left_context', '')[:10] or ''\n",
                "    print(f\"  {status} Score {s:2d}/10 | {ctx}<SEP>{t['input']} ‚Üí {t['output'][:25]}\")\n",
                "    print(f\"              Reasons: {', '.join(r)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Dataset & Process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from tqdm.auto import tqdm\n",
                "import json\n",
                "\n",
                "print(\"üì• Loading zenz dataset...\")\n",
                "dataset = load_dataset(\n",
                "    \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "    data_files=\"train_wikipedia.jsonl\",\n",
                "    split=\"train\"\n",
                ")\n",
                "total = len(dataset)\n",
                "print(f\"‚úì Loaded {total:,} items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Full pipeline: Kill ‚Üí Fix ‚Üí Score ‚Üí Save\n",
                "# ============================================================\n",
                "\n",
                "FILTERED_FILE = f\"{OUTPUT_DIR}/filtered_high_quality.jsonl\"\n",
                "KILLED_FILE = f\"{OUTPUT_DIR}/killed_samples.jsonl\"  # Sample of removed items\n",
                "PROGRESS_FILE = f\"{OUTPUT_DIR}/filter_progress.json\"\n",
                "\n",
                "# Stats\n",
                "stats = {\n",
                "    'total': 0,\n",
                "    'killed': Counter(),       # Reason ‚Üí count\n",
                "    'score_dist': Counter(),   # Score ‚Üí count\n",
                "    'kept': 0,\n",
                "    'fixed': Counter(),        # Fix type ‚Üí count\n",
                "}\n",
                "\n",
                "# Resume support\n",
                "start_from = 0\n",
                "if os.path.exists(PROGRESS_FILE):\n",
                "    with open(PROGRESS_FILE, 'r') as f:\n",
                "        progress = json.load(f)\n",
                "    start_from = progress.get('processed', 0)\n",
                "    stats['kept'] = progress.get('kept', 0)\n",
                "    stats['killed'] = Counter(progress.get('killed', {}))\n",
                "    stats['score_dist'] = Counter({int(k): v for k, v in progress.get('score_dist', {}).items()})\n",
                "    stats['fixed'] = Counter(progress.get('fixed', {}))\n",
                "    print(f\"üìÇ Resuming from item {start_from:,}\")\n",
                "\n",
                "limit = min(total, MAX_ITEMS) if MAX_ITEMS else total\n",
                "\n",
                "# Keep some killed samples for inspection\n",
                "killed_samples = []\n",
                "MAX_KILLED_SAMPLES = 100\n",
                "\n",
                "# Process\n",
                "mode = 'a' if start_from > 0 else 'w'\n",
                "with open(FILTERED_FILE, mode, encoding='utf-8') as out_f:\n",
                "    for idx in tqdm(range(start_from, limit), desc=\"Filtering\", total=limit - start_from):\n",
                "        item = dataset[idx]\n",
                "        stats['total'] += 1\n",
                "        \n",
                "        # Step 1: KILL LIST ‚Äî instant reject\n",
                "        killed, kill_reason = is_killed(item)\n",
                "        if killed:\n",
                "            stats['killed'][kill_reason] += 1\n",
                "            if len(killed_samples) < MAX_KILLED_SAMPLES:\n",
                "                killed_samples.append({\n",
                "                    'reason': kill_reason,\n",
                "                    'input': item.get('input', '')[:30],\n",
                "                    'output': (item.get('output', '') or '')[:50],\n",
                "                })\n",
                "            continue\n",
                "        \n",
                "        # Step 2: FIX LIST ‚Äî clean/normalize\n",
                "        fixed = fix_item(item)\n",
                "        if fixed.get('_fixed'):\n",
                "            stats['fixed'][fixed['_fixed']] += 1\n",
                "        if fixed.get('_cutoff'):\n",
                "            stats['fixed']['cutoff'] += 1\n",
                "        \n",
                "        # Step 3: QUALITY SCORE\n",
                "        score, reasons = score_quality(fixed)\n",
                "        stats['score_dist'][score] += 1\n",
                "        \n",
                "        if score >= MIN_QUALITY_SCORE:\n",
                "            out_item = {\n",
                "                'left_context': fixed['left_context'],\n",
                "                'input': fixed['input'],\n",
                "                'output': fixed['output'],\n",
                "                'score': score,\n",
                "            }\n",
                "            out_f.write(json.dumps(out_item, ensure_ascii=False) + '\\n')\n",
                "            stats['kept'] += 1\n",
                "        \n",
                "        # Save progress every 100K items\n",
                "        if stats['total'] % 100_000 == 0:\n",
                "            with open(PROGRESS_FILE, 'w') as pf:\n",
                "                json.dump({\n",
                "                    'processed': start_from + stats['total'],\n",
                "                    'kept': stats['kept'],\n",
                "                    'killed': dict(stats['killed']),\n",
                "                    'score_dist': dict(stats['score_dist']),\n",
                "                    'fixed': dict(stats['fixed']),\n",
                "                }, pf)\n",
                "            out_f.flush()\n",
                "\n",
                "# Save final progress\n",
                "with open(PROGRESS_FILE, 'w') as pf:\n",
                "    json.dump({\n",
                "        'processed': start_from + stats['total'],\n",
                "        'kept': stats['kept'],\n",
                "        'killed': dict(stats['killed']),\n",
                "        'score_dist': dict(stats['score_dist']),\n",
                "        'fixed': dict(stats['fixed']),\n",
                "    }, pf)\n",
                "\n",
                "# Save killed samples\n",
                "with open(KILLED_FILE, 'w', encoding='utf-8') as f:\n",
                "    for s in killed_samples:\n",
                "        f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n",
                "\n",
                "total_processed = start_from + stats['total']\n",
                "total_killed = sum(stats['killed'].values())\n",
                "\n",
                "print(f\"\\n‚úì Processed: {total_processed:,}\")\n",
                "print(f\"üóëÔ∏è Killed: {total_killed:,}\")\n",
                "print(f\"‚úÖ Kept: {stats['kept']:,} ({stats['kept']/total_processed*100:.1f}%)\")\n",
                "print(f\"üíæ Saved: {FILTERED_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results & Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_processed = start_from + stats['total']\n",
                "total_killed = sum(stats['killed'].values())\n",
                "total_scored = total_processed - total_killed\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üìä FILTERING REPORT\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Kill List breakdown\n",
                "print(f\"\\nüóëÔ∏è KILL LIST ({total_killed:,} removed):\")\n",
                "print(\"-\" * 40)\n",
                "for reason, count in stats['killed'].most_common():\n",
                "    pct = count / total_processed * 100\n",
                "    print(f\"  {reason:<20} {count:>8,} ({pct:.1f}%)\")\n",
                "\n",
                "# Fix List breakdown\n",
                "total_fixed = sum(stats['fixed'].values())\n",
                "print(f\"\\nüîß FIX LIST ({total_fixed:,} items cleaned):\")\n",
                "print(\"-\" * 40)\n",
                "for fix_type, count in stats['fixed'].most_common():\n",
                "    print(f\"  {fix_type:<20} {count:>8,}\")\n",
                "\n",
                "# Score distribution\n",
                "print(f\"\\nüìà SCORE DISTRIBUTION (of {total_scored:,} non-killed items):\")\n",
                "print(\"-\" * 55)\n",
                "for score in sorted(stats['score_dist'].keys()):\n",
                "    count = stats['score_dist'][score]\n",
                "    pct = count / max(total_scored, 1) * 100\n",
                "    bar = '‚ñà' * int(pct / 2)\n",
                "    status = '‚úÖ KEEP' if score >= MIN_QUALITY_SCORE else '‚ùå DROP'\n",
                "    print(f\"  Score {score:2d}: {count:>8,} ({pct:5.1f}%) {bar} {status}\")\n",
                "\n",
                "# Summary\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üìã SUMMARY\")\n",
                "print(f\"  Original:     {total_processed:>10,}\")\n",
                "print(f\"  Killed:       {total_killed:>10,} ({total_killed/total_processed*100:.1f}%)\")\n",
                "print(f\"  Score < {MIN_QUALITY_SCORE}:    {total_scored - stats['kept']:>10,}\")\n",
                "print(f\"  ‚úÖ Kept:      {stats['kept']:>10,} ({stats['kept']/total_processed*100:.1f}%)\")\n",
                "\n",
                "file_size = os.path.getsize(FILTERED_FILE)\n",
                "print(f\"\\n  File: {FILTERED_FILE}\")\n",
                "print(f\"  Size: {file_size / (1024*1024):.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show killed samples for verification\n",
                "print(\"\\nüóëÔ∏è Sample KILLED items (verify these should be removed):\")\n",
                "print(\"-\" * 60)\n",
                "# Group by reason\n",
                "by_reason = {}\n",
                "for s in killed_samples:\n",
                "    r = s['reason']\n",
                "    if r not in by_reason:\n",
                "        by_reason[r] = []\n",
                "    if len(by_reason[r]) < 3:  # 3 samples per kill reason\n",
                "        by_reason[r].append(s)\n",
                "\n",
                "for reason, samples in by_reason.items():\n",
                "    print(f\"\\n  [{reason}]\")\n",
                "    for s in samples:\n",
                "        print(f\"    {s['input']} ‚Üí {s['output'][:40]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show kept samples for verification\n",
                "print(\"\\n‚úÖ Sample KEPT items (verify these are good quality):\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "kept_samples = []\n",
                "with open(FILTERED_FILE, 'r', encoding='utf-8') as f:\n",
                "    for i, line in enumerate(f):\n",
                "        if i >= 30:\n",
                "            break\n",
                "        kept_samples.append(json.loads(line))\n",
                "\n",
                "for item in kept_samples[:15]:\n",
                "    ctx = item['left_context'][:12] or ''\n",
                "    print(f\"  s={item['score']} | {ctx}<SEP>{item['input'][:10]} ‚Üí {item['output'][:30]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# How to use in training notebooks\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üìã HOW TO USE IN TRAINING NOTEBOOKS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\"\"\n",
                "Replace dataset loading with:\n",
                "\n",
                "  import json\n",
                "  \n",
                "  FILTERED_FILE = \"{FILTERED_FILE}\"\n",
                "  \n",
                "  dataset = []\n",
                "  with open(FILTERED_FILE, 'r', encoding='utf-8') as f:\n",
                "      for line in f:\n",
                "          dataset.append(json.loads(line))\n",
                "  \n",
                "  print(f\"Loaded {{len(dataset):,}} filtered items\")\n",
                "  \n",
                "  # Same fields as before:\n",
                "  for item in dataset:\n",
                "      left_ctx = item['left_context']\n",
                "      kana = item['input']\n",
                "      kanji = item['output']\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}