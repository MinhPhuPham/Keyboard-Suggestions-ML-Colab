{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model 2: Next Word Predictor (Language Model)\n",
                "\n",
                "**Task:** Predict next words word-by-word\n",
                "- Input: `[„Åä‰∏ñË©±]`\n",
                "- Output: `„Å´ ‚Üí „Å™„Å£„Å¶ ‚Üí „Åä„Çä„Åæ„Åô` (word-by-word)\n",
                "\n",
                "**Architecture:** Bi-GRU + Luong Attention (Word-Level)\n",
                "\n",
                "**Target:** ~2.5MB, 85%+ accuracy, <5ms inference"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_next_word\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"‚úì Model: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm fugashi unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 4\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 300000\n",
                "else:\n",
                "    NUM_EPOCHS = 25\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 500000\n",
                "\n",
                "# Model specs\n",
                "WORD_VOCAB_SIZE = 6000\n",
                "MAX_CONTEXT_LEN = 10  # Max words in context\n",
                "EMBEDDING_DIM = 96\n",
                "GRU_UNITS = 192\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, samples={MAX_SAMPLES:,}\")\n",
                "print(f\"Model: Embed={EMBEDDING_DIM}, GRU={GRU_UNITS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "except:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "\n",
                "print(f\"‚úì Loaded {len(dataset):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tokenize Words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import fugashi\n",
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "\n",
                "tagger = fugashi.Tagger()\n",
                "\n",
                "def tokenize_words(text):\n",
                "    \"\"\"Word-level tokenization with emoji support.\"\"\"\n",
                "    result = []\n",
                "    for t in tagger(text):\n",
                "        # Keep words and emojis, filter only whitespace\n",
                "        if t.feature.pos1 not in ['Á©∫ÁôΩ']:\n",
                "            result.append(t.surface)\n",
                "    return result\n",
                "\n",
                "# Test\n",
                "print(f\"Words: {tokenize_words('ÊúâÈõ£„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åôüòä')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Word Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Building word vocabulary...\")\n",
                "\n",
                "word_counts = Counter()\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Counting words\"):\n",
                "    kanji = item.get('output', '')\n",
                "    words = tokenize_words(kanji)\n",
                "    word_counts.update(words)\n",
                "\n",
                "print(f\"‚úì Found {len(word_counts):,} unique words\")\n",
                "print(f\"Top 15: {[w for w, c in word_counts.most_common(15)]}\")\n",
                "\n",
                "# Build vocab\n",
                "word_to_idx = {}\n",
                "for i, tok in enumerate(SPECIAL_TOKENS):\n",
                "    word_to_idx[tok] = i\n",
                "\n",
                "for word, _ in word_counts.most_common(WORD_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "    word_to_idx[word] = len(word_to_idx)\n",
                "\n",
                "idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
                "vocab_size = len(word_to_idx)\n",
                "\n",
                "print(f\"‚úì Vocab size: {vocab_size:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create Training Data (Word-by-Word)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "print(\"Creating word-by-word training data...\")\n",
                "\n",
                "def encode_words(words, max_len=MAX_CONTEXT_LEN):\n",
                "    ids = [word_to_idx.get(w, word_to_idx['<UNK>']) for w in words]\n",
                "    if len(ids) < max_len:\n",
                "        ids = [word_to_idx['<PAD>']] * (max_len - len(ids)) + ids  # Left-pad\n",
                "    return ids[-max_len:]  # Keep last N tokens\n",
                "\n",
                "X_data = []\n",
                "y_data = []\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Processing\"):\n",
                "    kanji = item.get('output', '').strip()\n",
                "    if not kanji:\n",
                "        continue\n",
                "    \n",
                "    words = tokenize_words(kanji)\n",
                "    if len(words) < 2:\n",
                "        continue\n",
                "    \n",
                "    # Create training pairs: context ‚Üí next_word\n",
                "    for i in range(1, len(words)):\n",
                "        context = words[max(0, i-MAX_CONTEXT_LEN):i]\n",
                "        next_word = words[i]\n",
                "        \n",
                "        # Only train on words in vocabulary\n",
                "        if next_word not in word_to_idx:\n",
                "            continue\n",
                "        \n",
                "        X_data.append(encode_words(context))\n",
                "        y_data.append(word_to_idx[next_word])\n",
                "\n",
                "X_data = np.array(X_data)\n",
                "y_data = np.array(y_data)\n",
                "\n",
                "print(f\"\\n‚úì {len(X_data):,} training samples\")\n",
                "print(f\"‚úì Shape: {X_data.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Shuffle and split\n",
                "indices = np.random.permutation(len(X_data))\n",
                "X_data = X_data[indices]\n",
                "y_data = y_data[indices]\n",
                "\n",
                "split = int(len(X_data) * 0.9)\n",
                "X_train, X_val = X_data[:split], X_data[split:]\n",
                "y_train, y_val = y_data[:split], y_data[split:]\n",
                "\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"‚úì Train: {len(X_train):,}, Val: {len(X_val):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Build Next Word Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import mixed_precision\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "print(\"Building Next Word Language Model...\")\n",
                "\n",
                "inputs = Input(shape=(MAX_CONTEXT_LEN,), name='input')\n",
                "\n",
                "# Embedding\n",
                "x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "\n",
                "# Bi-directional GRU\n",
                "encoder_out = Bidirectional(\n",
                "    GRU(GRU_UNITS, return_sequences=True, dropout=0.2),\n",
                "    name='bi_gru'\n",
                ")(x)\n",
                "\n",
                "# Luong Attention (self-attention)\n",
                "attention_out = Attention(use_scale=True, name='attention')(\n",
                "    [encoder_out, encoder_out]\n",
                ")\n",
                "\n",
                "# Combine\n",
                "combined = Concatenate()([encoder_out, attention_out])\n",
                "combined = LayerNormalization()(combined)\n",
                "\n",
                "# Context vector (last state)\n",
                "context = GRU(GRU_UNITS, name='context_gru')(combined)\n",
                "context = Dropout(0.3)(context)\n",
                "\n",
                "# Output: predict next word\n",
                "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(context)\n",
                "\n",
                "model = Model(inputs, outputs, name='next_word_lm')\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()\n",
                "print(f\"\\n‚úì Parameters: {model.count_params():,}\")\n",
                "print(f\"‚úì Estimated size: {model.count_params() * 4 / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n‚úì Final Val Accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/word_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_word.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_word.items()}, f, ensure_ascii=False)\n",
                "\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'max_context_len': MAX_CONTEXT_LEN,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'gru_units': GRU_UNITS,\n",
                "    'architecture': 'BiGRU_LuongAttention_LM',\n",
                "    'task': 'next_word_prediction',\n",
                "    'special_tokens': SPECIAL_TOKENS\n",
                "}\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"‚úì Saved model and config\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export TFLite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Exporting TFLite...\")\n",
                "\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"‚úì model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # FP16\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite_fp16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite_fp16)/(1024*1024):.2f}MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö† Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Next Word Prediction\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def predict_next_word(context_words, top_k=5):\n",
                "    \"\"\"Predict next word given context.\"\"\"\n",
                "    encoded = np.array([encode_words(context_words)])\n",
                "    probs = model.predict(encoded, verbose=0)[0]\n",
                "    \n",
                "    top_indices = np.argsort(probs)[-top_k*2:][::-1]\n",
                "    predictions = []\n",
                "    for idx in top_indices:\n",
                "        word = idx_to_word.get(idx, '<UNK>')\n",
                "        if word not in SPECIAL_TOKENS:\n",
                "            predictions.append(word)\n",
                "        if len(predictions) >= top_k:\n",
                "            break\n",
                "    return predictions\n",
                "\n",
                "def generate_sequence(start_words, num_words=5):\n",
                "    \"\"\"Generate word-by-word sequence.\"\"\"\n",
                "    context = list(start_words)\n",
                "    generated = []\n",
                "    \n",
                "    for _ in range(num_words):\n",
                "        predictions = predict_next_word(context, top_k=1)\n",
                "        if not predictions or predictions[0] == '<EOS>':\n",
                "            break\n",
                "        next_word = predictions[0]\n",
                "        generated.append(next_word)\n",
                "        context.append(next_word)\n",
                "    \n",
                "    return generated\n",
                "\n",
                "# Test: Top-K predictions\n",
                "print(\"\\nüìù Top-5 Next Word Predictions:\")\n",
                "print(\"-\" * 50)\n",
                "tests = [\n",
                "    ['„ÅÇ„Çä„Åå„Å®„ÅÜ'],           # ‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô, „Åî„Åñ„ÅÑ„Åæ„Åó„Åü, „Å≠\n",
                "    ['„Åä‰∏ñË©±'],               # ‚Üí „Å´, „Å´„Å™„Å£„Å¶\n",
                "    ['Ë°å„Åç'],                 # ‚Üí „Åæ„Åô, „Åü„ÅÑ\n",
                "    ['Áî≥„ÅóË®≥'],               # ‚Üí „ÅÇ„Çä„Åæ„Åõ„Çì, „Åî„Åñ„ÅÑ„Åæ„Åõ„Çì\n",
                "    ['„Åù„ÅÜ'],                 # ‚Üí „Åß„Åô, „Å†\n",
                "    ['‰ªäÊó•'],                 # ‚Üí „ÅØ, „ÅÆ\n",
                "    ['Êó•Êú¨'],                 # ‚Üí „ÅÆ, „ÅØ\n",
                "]\n",
                "for ctx in tests:\n",
                "    result = predict_next_word(ctx)\n",
                "    print(f\"  {''.join(ctx)} ‚Üí {result}\")\n",
                "\n",
                "# Test: Word-by-word generation\n",
                "print(\"\\nüìù Word-by-Word Generation:\")\n",
                "print(\"-\" * 50)\n",
                "generations = [\n",
                "    ['„Åä‰∏ñË©±'],      # Should generate: „Å´ „Å™„Å£„Å¶ „Åä„Çä„Åæ„Åô\n",
                "    ['„ÅÇ„Çä„Åå„Å®„ÅÜ'],  # Should generate: „Åî„Åñ„ÅÑ„Åæ„Åô/„Åî„Åñ„ÅÑ„Åæ„Åó„Åü\n",
                "    ['‰ªäÊó•', '„ÅØ'],  # Should continue\n",
                "]\n",
                "for start in generations:\n",
                "    gen = generate_sequence(start, num_words=4)\n",
                "    print(f\"  {''.join(start)} ‚Üí {''.join(gen)}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List exports\n",
                "print(\"\\nExported files:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    path = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(path):\n",
                "        size = os.path.getsize(path)\n",
                "        if size > 1024*1024:\n",
                "            print(f\"  {f}: {size/(1024*1024):.1f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {size/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
