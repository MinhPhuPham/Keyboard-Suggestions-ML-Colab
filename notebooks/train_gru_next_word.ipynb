{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Next Word Predictor v2 (Word-Level Language Model)\n",
                "\n",
                "**Supports**: Google Colab & Kaggle\n",
                "\n",
                "**Task**: Predict next word from kanji context\n",
                "- Input: `[‰ªäÊó•, „ÅØ]` ‚Üí Output: `Â§©Ê∞ó` / `Êöë„ÅÑ` / `ËâØ„ÅÑ`\n",
                "\n",
                "**Architecture**: Bi-GRU + Self-Attention + Context GRU\n",
                "\n",
                "**v2 Improvements over v1**:\n",
                "- Uses `left_context + output` combined (5√ó more training pairs)\n",
                "- Memory-safe: mmap + generator = ~0 GB data in RAM\n",
                "- Removed mixed_precision (hurt accuracy on small model)\n",
                "- More data: 2M items ‚Üí ~5M+ training pairs\n",
                "- Platform detection fix (Colab/Kaggle/Local)\n",
                "- Cache to drive (.npy for mmap support)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "\n",
                "# Auto-detect platform (Colab check first - Colab also has /kaggle dir!)\n",
                "if 'COLAB_RELEASE_TAG' in os.environ:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Local'\n",
                "    DRIVE_DIR = './output'\n",
                "\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_next_word\"\n",
                "CACHE_DIR = f\"{DRIVE_DIR}/cache/nwp\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Model: {MODEL_DIR}\")\n",
                "print(f\"üíæ Cache: {CACHE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm fugashi unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "TESTING_MODE = False\n",
                "MAX_SAMPLES = 5_000_000    # Dataset items to process\n",
                "MAX_NWP_PAIRS = 8_000_000  # Max training pairs to create\n",
                "BATCH_SIZE = 512\n",
                "FORCE_REBUILD_CACHE = False\n",
                "\n",
                "NUM_EPOCHS = 3 if TESTING_MODE else 15\n",
                "\n",
                "# Word-level model config\n",
                "WORD_VOCAB_SIZE = 6000\n",
                "MAX_WORD_CONTEXT = 10  # Max words in context (left-padded)\n",
                "EMBEDDING_DIM = 96\n",
                "GRU_UNITS = 192\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
                "PAD_IDX = 0\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, max_items={MAX_SAMPLES:,}, max_pairs={MAX_NWP_PAIRS:,}\")\n",
                "print(f\"Model: vocab={WORD_VOCAB_SIZE}, embed={EMBEDDING_DIM}, GRU={GRU_UNITS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Shared Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import fugashi\n",
                "\n",
                "tagger = fugashi.Tagger()\n",
                "\n",
                "def tokenize_words(text):\n",
                "    \"\"\"Word-level tokenization using fugashi (MeCab).\"\"\"\n",
                "    if not text:\n",
                "        return []\n",
                "    result = []\n",
                "    for t in tagger(text):\n",
                "        if t.feature.pos1 not in ['Á©∫ÁôΩ']:  # Skip whitespace\n",
                "            result.append(t.surface)\n",
                "    return result\n",
                "\n",
                "def encode_words(words, vocab, pad_id, unk_id, max_len=None):\n",
                "    \"\"\"Encode word list to padded integer IDs (left-padded).\"\"\"\n",
                "    if max_len is None:\n",
                "        max_len = MAX_WORD_CONTEXT\n",
                "    ids = [vocab.get(w, unk_id) for w in words]\n",
                "    if len(ids) < max_len:\n",
                "        ids = [pad_id] * (max_len - len(ids)) + ids  # Left-pad\n",
                "    return ids[-max_len:]  # Keep last N tokens\n",
                "\n",
                "# Quick test\n",
                "test_words = tokenize_words('‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇÊöë„ÅÑ„Åß„Åô„Å≠')\n",
                "print(f\"‚úì Tokenize test: {test_words}\")\n",
                "print(f\"  ({len(test_words)} words)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load or Build Cache\n",
                "\n",
                "**Key improvement**: Uses `left_context + output` combined for full sentence context.\n",
                "\n",
                "Before (v1): only `output` ‚Üí 2-3 words ‚Üí ~1 pair/item\n",
                "\n",
                "After (v2): `left_context + output` ‚Üí 5-10 words ‚Üí ~5 pairs/item"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Cache paths (.npy for mmap support)\n",
                "VOCAB_CACHE = f\"{CACHE_DIR}/word_vocab_v2.json\"\n",
                "NWP_X_CACHE = f\"{CACHE_DIR}/nwp_x_v2.npy\"\n",
                "NWP_Y_CACHE = f\"{CACHE_DIR}/nwp_y_v2.npy\"\n",
                "\n",
                "def cache_exists():\n",
                "    return all(os.path.exists(f) for f in [VOCAB_CACHE, NWP_X_CACHE, NWP_Y_CACHE])\n",
                "\n",
                "if cache_exists() and not FORCE_REBUILD_CACHE:\n",
                "    print(\"üì¶ Loading from cache (memory-mapped)...\")\n",
                "    \n",
                "    with open(VOCAB_CACHE, 'r', encoding='utf-8') as f:\n",
                "        vocab_data = json.load(f)\n",
                "    word_to_idx = vocab_data['word_to_idx']\n",
                "    idx_to_word = {int(k): v for k, v in vocab_data['idx_to_word'].items()}\n",
                "    vocab_size = len(word_to_idx)\n",
                "    \n",
                "    x_mmap = np.load(NWP_X_CACHE, mmap_mode='r')\n",
                "    y_mmap = np.load(NWP_Y_CACHE, mmap_mode='r')\n",
                "    \n",
                "    print(f\"‚úì Vocab: {vocab_size:,} words\")\n",
                "    print(f\"‚úì Pairs: {len(x_mmap):,} (memory-mapped)\")\n",
                "    CACHE_LOADED = True\n",
                "else:\n",
                "    print(\"üî® Building from scratch (will save to drive)...\")\n",
                "    CACHE_LOADED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset + build word vocabulary\n",
                "if not CACHE_LOADED:\n",
                "    from datasets import load_dataset\n",
                "    from collections import Counter\n",
                "    \n",
                "    print(\"üì• Loading zenz dataset...\")\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=\"train\"\n",
                "    )\n",
                "    print(f\"‚úì Raw: {len(dataset):,} items\")\n",
                "    \n",
                "    # Pass 1: Build vocab from left_context + output (combined)\n",
                "    print(\"\\nüìù Building word vocabulary (left_context + output)...\")\n",
                "    word_counts = Counter()\n",
                "    processed = 0\n",
                "    \n",
                "    for item in tqdm(dataset, desc=\"Counting words\"):\n",
                "        left_ctx = item.get('left_context', '') or ''\n",
                "        output = item.get('output', '') or ''\n",
                "        text = left_ctx + output\n",
                "        if not text.strip():\n",
                "            continue\n",
                "        words = tokenize_words(text)\n",
                "        word_counts.update(words)\n",
                "        processed += 1\n",
                "        if MAX_SAMPLES and processed >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    print(f\"\\n‚úì Found {len(word_counts):,} unique words from {processed:,} items\")\n",
                "    print(f\"  Top 15: {[w for w, c in word_counts.most_common(15)]}\")\n",
                "    \n",
                "    # Build vocab: special tokens first, then most common words\n",
                "    word_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "    for word, _ in word_counts.most_common(WORD_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "        word_to_idx[word] = len(word_to_idx)\n",
                "    \n",
                "    idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
                "    vocab_size = len(word_to_idx)\n",
                "    print(f\"‚úì Vocab size: {vocab_size:,}\")\n",
                "    \n",
                "    # Save vocab\n",
                "    with open(VOCAB_CACHE, 'w', encoding='utf-8') as f:\n",
                "        json.dump({\n",
                "            'word_to_idx': word_to_idx,\n",
                "            'idx_to_word': {str(k): v for k, v in idx_to_word.items()}\n",
                "        }, f, ensure_ascii=False)\n",
                "    print(f\"‚úì Vocab saved to {VOCAB_CACHE}\")\n",
                "    \n",
                "    del word_counts\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create training pairs from left_context + output\n",
                "# Example: \"Â§©Ê∞ó„ÅåËâØ„ÅÑ‰ªäÊó•„ÅØÊöë„ÅÑ\" ‚Üí [\"Â§©Ê∞ó\",\"„Åå\",\"ËâØ„ÅÑ\",\"‰ªäÊó•\",\"„ÅØ\",\"Êöë„ÅÑ\"]\n",
                "#   Pairs: [Â§©Ê∞ó]‚Üí„Åå, [Â§©Ê∞ó,„Åå]‚ÜíËâØ„ÅÑ, [Â§©Ê∞ó,„Åå,ËâØ„ÅÑ]‚Üí‰ªäÊó•, ...\n",
                "if not CACHE_LOADED:\n",
                "    print(\"\\nüî¢ Creating training pairs (left_context + output)...\")\n",
                "    \n",
                "    PAD = word_to_idx['<PAD>']\n",
                "    UNK = word_to_idx['<UNK>']\n",
                "    \n",
                "    # Pre-allocate arrays (fill up to MAX_NWP_PAIRS)\n",
                "    X = np.zeros((MAX_NWP_PAIRS, MAX_WORD_CONTEXT), dtype=np.int32)\n",
                "    y = np.zeros(MAX_NWP_PAIRS, dtype=np.int32)\n",
                "    pair_idx = 0\n",
                "    processed = 0\n",
                "    \n",
                "    for item in tqdm(dataset, desc=\"Creating pairs\"):\n",
                "        left_ctx = item.get('left_context', '') or ''\n",
                "        output = item.get('output', '') or ''\n",
                "        text = left_ctx + output\n",
                "        if not text.strip():\n",
                "            continue\n",
                "        \n",
                "        words = tokenize_words(text)\n",
                "        if len(words) < 2:\n",
                "            continue\n",
                "        \n",
                "        # Create sliding window pairs: context ‚Üí next_word\n",
                "        for i in range(1, len(words)):\n",
                "            next_word = words[i]\n",
                "            if next_word not in word_to_idx:\n",
                "                continue\n",
                "            \n",
                "            context = words[max(0, i - MAX_WORD_CONTEXT):i]\n",
                "            X[pair_idx] = encode_words(context, word_to_idx, PAD, UNK)\n",
                "            y[pair_idx] = word_to_idx[next_word]\n",
                "            pair_idx += 1\n",
                "            \n",
                "            if pair_idx >= MAX_NWP_PAIRS:\n",
                "                break\n",
                "        \n",
                "        if pair_idx >= MAX_NWP_PAIRS:\n",
                "            break\n",
                "        \n",
                "        processed += 1\n",
                "        if MAX_SAMPLES and processed >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    # Trim to actual size\n",
                "    X = X[:pair_idx]\n",
                "    y = y[:pair_idx]\n",
                "    print(f\"\\n‚úì Created {pair_idx:,} training pairs from {processed:,} items\")\n",
                "    print(f\"  Avg pairs/item: {pair_idx / max(processed, 1):.1f}\")\n",
                "    \n",
                "    # Save as .npy and release\n",
                "    np.save(NWP_X_CACHE, X)\n",
                "    np.save(NWP_Y_CACHE, y)\n",
                "    del X, y\n",
                "    gc.collect()\n",
                "    \n",
                "    # Release dataset\n",
                "    del dataset\n",
                "    gc.collect()\n",
                "    print(\"üßπ Saved cache, released memory\")\n",
                "    \n",
                "    # Load as memory-mapped\n",
                "    x_mmap = np.load(NWP_X_CACHE, mmap_mode='r')\n",
                "    y_mmap = np.load(NWP_Y_CACHE, mmap_mode='r')\n",
                "    print(f\"‚úì Loaded as mmap: X={x_mmap.shape}, y={y_mmap.shape}\")\n",
                "\n",
                "print(f\"\\nüìä Total pairs: {len(x_mmap):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "n_samples = len(x_mmap)\n",
                "split = int(n_samples * 0.9)\n",
                "\n",
                "# Random shuffle indices\n",
                "indices = np.random.permutation(n_samples).astype(np.int32)\n",
                "train_idx = indices[:split]\n",
                "val_idx = indices[split:]\n",
                "\n",
                "def make_generator(x, y_arr, idx_arr):\n",
                "    \"\"\"Generator reads from mmap arrays (zero RAM copy).\"\"\"\n",
                "    def gen():\n",
                "        for i in idx_arr:\n",
                "            yield x[i], y_arr[i]\n",
                "    return gen\n",
                "\n",
                "output_sig = (\n",
                "    tf.TensorSpec(shape=(MAX_WORD_CONTEXT,), dtype=tf.int32),\n",
                "    tf.TensorSpec(shape=(), dtype=tf.int32),\n",
                ")\n",
                "\n",
                "train_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(x_mmap, y_mmap, train_idx),\n",
                "    output_signature=output_sig\n",
                ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(x_mmap, y_mmap, val_idx),\n",
                "    output_signature=output_sig\n",
                ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"Train: {len(train_idx):,}, Val: {len(val_idx):,}\")\n",
                "print(f\"üí° Data loaded via mmap + generator (near-zero RAM)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Model (Bi-GRU + Self-Attention)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "inputs = Input(shape=(MAX_WORD_CONTEXT,), name='input')\n",
                "\n",
                "# Embedding\n",
                "x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "\n",
                "# Bidirectional GRU\n",
                "encoder_out = Bidirectional(\n",
                "    GRU(GRU_UNITS, return_sequences=True, dropout=0.2),\n",
                "    name='bi_gru'\n",
                ")(x)\n",
                "\n",
                "# Self-Attention (Luong-style)\n",
                "attention_out = Attention(use_scale=True, name='attention')(\n",
                "    [encoder_out, encoder_out]\n",
                ")\n",
                "\n",
                "# Combine encoder + attention\n",
                "combined = Concatenate()([encoder_out, attention_out])\n",
                "combined = LayerNormalization()(combined)\n",
                "\n",
                "# Context GRU (compress to single vector)\n",
                "context = GRU(GRU_UNITS, name='context_gru')(combined)\n",
                "context = Dropout(0.3)(context)\n",
                "\n",
                "# Output: predict next word\n",
                "outputs = Dense(vocab_size, activation='softmax', name='output')(context)\n",
                "\n",
                "model = Model(inputs, outputs, name='next_word_lm_v2')\n",
                "\n",
                "# Gradient clipping for stable training\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()\n",
                "params = model.count_params()\n",
                "print(f\"\\nüìä Parameters: {params:,}\")\n",
                "print(f\"   FP32: ~{params * 4 / 1024 / 1024:.1f} MB\")\n",
                "print(f\"   FP16: ~{params * 2 / 1024 / 1024:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "# Calculate steps (from_generator doesn't auto-detect size)\n",
                "steps_per_epoch = len(train_idx) // BATCH_SIZE\n",
                "validation_steps = len(val_idx) // BATCH_SIZE\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(\n",
                "        f'{MODEL_DIR}/best_v2.keras',\n",
                "        monitor='val_accuracy',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=5,\n",
                "        restore_best_weights=True\n",
                "    ),\n",
                "    ReduceLROnPlateau(\n",
                "        monitor='val_loss',\n",
                "        factor=0.5,\n",
                "        patience=2,\n",
                "        min_lr=1e-6,\n",
                "        verbose=1\n",
                "    )\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    steps_per_epoch=steps_per_epoch,\n",
                "    validation_data=val_ds,\n",
                "    validation_steps=validation_steps,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "\n",
                "plt.savefig(f'{MODEL_DIR}/training_v2.png')\n",
                "plt.show()\n",
                "print(f\"Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model + vocab + config\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/word_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_word.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_word.items()}, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        'vocab_size': vocab_size,\n",
                "        'max_context_len': MAX_WORD_CONTEXT,\n",
                "        'embedding_dim': EMBEDDING_DIM,\n",
                "        'gru_units': GRU_UNITS,\n",
                "        'architecture': 'BiGRU_SelfAttention_ContextGRU',\n",
                "        'special_tokens': SPECIAL_TOKENS,\n",
                "        'version': 'v2'\n",
                "    }, f, indent=2)\n",
                "\n",
                "keras_size = os.path.getsize(f'{MODEL_DIR}/model.keras')\n",
                "print(f\"‚úì Model saved: {keras_size / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export TFLite\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [\n",
                "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
                "        tf.lite.OpsSet.SELECT_TF_OPS\n",
                "    ]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite)\n",
                "    print(f\"‚úì model.tflite ({len(tflite)/(1024*1024):.2f} MB)\")\n",
                "    \n",
                "    # FP16 version (smaller, same accuracy)\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite16)/(1024*1024):.2f} MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö† TFLite export failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Next Word Prediction v2\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "PAD = word_to_idx['<PAD>']\n",
                "UNK = word_to_idx['<UNK>']\n",
                "\n",
                "def predict_next_word(context_words, top_k=5):\n",
                "    \"\"\"Predict next word given context words.\"\"\"\n",
                "    encoded = np.array([encode_words(context_words, word_to_idx, PAD, UNK)])\n",
                "    probs = model.predict(encoded, verbose=0)[0]\n",
                "    \n",
                "    top_indices = np.argsort(probs)[-top_k*2:][::-1]\n",
                "    predictions = []\n",
                "    for idx in top_indices:\n",
                "        word = idx_to_word.get(idx, '<UNK>')\n",
                "        if word not in SPECIAL_TOKENS:\n",
                "            predictions.append((word, float(probs[idx])))\n",
                "        if len(predictions) >= top_k:\n",
                "            break\n",
                "    return predictions\n",
                "\n",
                "def generate_sequence(start_words, num_words=5):\n",
                "    \"\"\"Generate word-by-word sequence.\"\"\"\n",
                "    context = list(start_words)\n",
                "    generated = []\n",
                "    for _ in range(num_words):\n",
                "        preds = predict_next_word(context, top_k=1)\n",
                "        if not preds or preds[0][0] == '<EOS>':\n",
                "            break\n",
                "        next_word = preds[0][0]\n",
                "        generated.append(next_word)\n",
                "        context.append(next_word)\n",
                "    return generated\n",
                "\n",
                "# ==========================================================\n",
                "# Test: Top-K predictions\n",
                "# ==========================================================\n",
                "print(\"\\nüìù Top-5 Next Word Predictions:\")\n",
                "print(\"-\" * 50)\n",
                "tests = [\n",
                "    (['„ÅÇ„Çä„Åå„Å®„ÅÜ'],           '‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô'),\n",
                "    (['„Åä', '‰∏ñË©±'],           '‚Üí „Å´'),\n",
                "    (['‰ªäÊó•', '„ÅØ'],           '‚Üí particle/topic'),\n",
                "    (['Êó•Êú¨', '„ÅÆ'],           '‚Üí contextual'),\n",
                "    (['Áî≥„ÅóË®≥'],               '‚Üí „Åî„Åñ„ÅÑ„Åæ„Åõ„Çì'),\n",
                "    (['Êù±‰∫¨', '„Å´'],           '‚Üí location'),\n",
                "    (['„Åù„Çå', '„ÅØ'],           '‚Üí contextual'),\n",
                "    (['Ë°å„Åç', '„Åü„ÅÑ'],         '‚Üí „Å®/„Åß„Åô'),\n",
                "    (['Â§ß', 'Â≠¶'],             '‚Üí „ÅÆ/„Å´/„Åß'),\n",
                "    (['ÂïèÈ°å', '„Åå'],           '‚Üí „ÅÇ„Çã/„Å™„ÅÑ'),\n",
                "]\n",
                "for ctx, hint in tests:\n",
                "    result = predict_next_word(ctx)\n",
                "    words = [f\"{w}({p:.2f})\" for w, p in result[:5]]\n",
                "    print(f\"  {''.join(ctx)} {hint}\")\n",
                "    print(f\"    ‚Üí {', '.join(words)}\")\n",
                "\n",
                "# ==========================================================\n",
                "# Test: Word-by-word generation\n",
                "# ==========================================================\n",
                "print(\"\\nüìù Word-by-Word Generation:\")\n",
                "print(\"-\" * 50)\n",
                "generations = [\n",
                "    ['„ÅÇ„Çä„Åå„Å®„ÅÜ'],\n",
                "    ['‰ªäÊó•', '„ÅØ'],\n",
                "    ['Êó•Êú¨', '„ÅÆ'],\n",
                "    ['„Åä', '‰∏ñË©±'],\n",
                "]\n",
                "for start in generations:\n",
                "    gen = generate_sequence(start, num_words=4)\n",
                "    print(f\"  {''.join(start)} ‚Üí {''.join(gen)}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List exported files\n",
                "print(f\"\\nüì¶ Files ({PLATFORM}):\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    p = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(p):\n",
                "        s = os.path.getsize(p)\n",
                "        if s > 1024*1024:\n",
                "            print(f\"  {f}: {s/(1024*1024):.2f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {s/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
