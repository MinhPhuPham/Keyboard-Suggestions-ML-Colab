{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owXaugdVe2bo"
   },
   "source": [
    "# GRU Keyboard Model (2026 Optimized)\n",
    "\n",
    "Train a GRU model with **Zipf-sorted vocabulary** for optimal mobile chunking.\n",
    "\n",
    "**Workflow:**\n",
    "1. Setup & Config\n",
    "2. Load Data\n",
    "3. Create Zipf-Sorted Vocabulary (NEW)\n",
    "4. Tokenize & Create Sequences\n",
    "5. Build & Train Model\n",
    "6. Visualize Training\n",
    "7. Save Model\n",
    "8. Export TFLite (Android)\n",
    "9. Export CoreML (iOS)\n",
    "10. Export Mobile Resources (SymSpell + Compact Trie)\n",
    "11. Verification Test\n",
    "\n",
    "**Key Feature:** Token indices match Zipf frequency order - high-frequency words have low indices.\n",
    "\n",
    "---\n",
    "**Instructions:**\n",
    "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. Set `TESTING_MODE = True` for quick test\n",
    "3. Set `TESTING_MODE = False` for full training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup directories\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
    "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "TESTING_MODE = True  # â† Change to False for full training\n",
    "\n",
    "if TESTING_MODE:\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 512\n",
    "    VOCAB_SIZE_LIMIT = 25000\n",
    "    SEQUENCE_LENGTH = 10\n",
    "else:\n",
    "    NUM_EPOCHS = 20\n",
    "    BATCH_SIZE = 512\n",
    "    VOCAB_SIZE_LIMIT = 25000\n",
    "    SEQUENCE_LENGTH = 10\n",
    "\n",
    "# Zipf thresholds for chunking\n",
    "HIGH_THRESHOLD = 4.0\n",
    "MEDIUM_THRESHOLD = 2.5\n",
    "\n",
    "print(f\"Config: vocab={VOCAB_SIZE_LIMIT:,}, seq={SEQUENCE_LENGTH}, epochs={NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "all_text = []\n",
    "\n",
    "if TESTING_MODE:\n",
    "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
    "    if not os.path.exists(CORPUS_PATH):\n",
    "        raise FileNotFoundError(f\"Missing: {CORPUS_PATH}\")\n",
    "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "        all_text.append(f.read())\n",
    "    print(f\"âœ“ Loaded test corpus: {len(all_text[0]):,} chars\")\n",
    "else:\n",
    "    # Full training: Fake.csv + True.csv + 1661-0.txt\n",
    "    fake_df = pd.read_csv(f\"{DRIVE_DIR}/datasets/Fake.csv\")\n",
    "    true_df = pd.read_csv(f\"{DRIVE_DIR}/datasets/True.csv\")\n",
    "    all_text.extend(fake_df['text'].tolist())\n",
    "    all_text.extend(true_df['text'].tolist())\n",
    "    with open(f\"{DRIVE_DIR}/datasets/1661-0.txt\", 'r', encoding='utf-8') as f:\n",
    "        all_text.append(f.read())\n",
    "    print(f\"âœ“ Loaded {len(all_text):,} text segments\")\n",
    "\n",
    "# Combine and clean\n",
    "combined_text = ' '.join(all_text).lower()\n",
    "combined_text = ' '.join(combined_text.split())\n",
    "print(f\"âœ“ Total: {len(combined_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Zipf-Sorted Vocabulary (KEY STEP)\n",
    "\n",
    "This creates a vocabulary where **token indices match Zipf frequency order**:\n",
    "- Index 0: Most frequent word (\"the\")\n",
    "- Index N: Least frequent word\n",
    "\n",
    "This ensures model's output indices directly correspond to frequency-sorted vocab for mobile chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordfreq import zipf_frequency\n",
    "import re\n",
    "\n",
    "print(\"Creating Zipf-sorted vocabulary...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Count words in corpus\n",
    "words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
    "word_counts = Counter(words)\n",
    "print(f\"âœ“ Found {len(word_counts):,} unique words in corpus\")\n",
    "\n",
    "# 2. Get Zipf frequency for each word and sort\n",
    "word_zipf_list = []\n",
    "for word, count in word_counts.items():\n",
    "    if len(word) >= 1:  # Include all words\n",
    "        zipf = zipf_frequency(word, 'en')\n",
    "        word_zipf_list.append((word, zipf, count))\n",
    "\n",
    "# Sort by Zipf (descending) - most frequent first\n",
    "word_zipf_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 3. Limit to vocab size\n",
    "word_zipf_list = word_zipf_list[:VOCAB_SIZE_LIMIT]\n",
    "\n",
    "# 4. Create word_to_index mapping (Zipf order = index order)\n",
    "# Index 1 = most frequent (reserve 0 for padding)\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "for idx, (word, zipf, count) in enumerate(word_zipf_list, start=1):\n",
    "    word_to_index[word] = idx\n",
    "    index_to_word[idx] = word\n",
    "\n",
    "vocab_size = len(word_to_index) + 1  # +1 for padding index 0\n",
    "\n",
    "# 5. Classify into chunks\n",
    "high_words = [(w, z) for w, z, c in word_zipf_list if z >= HIGH_THRESHOLD]\n",
    "medium_words = [(w, z) for w, z, c in word_zipf_list if MEDIUM_THRESHOLD <= z < HIGH_THRESHOLD]\n",
    "low_words = [(w, z) for w, z, c in word_zipf_list if z < MEDIUM_THRESHOLD]\n",
    "\n",
    "print(f\"\\nâœ“ Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  High (Zipf â‰¥ {HIGH_THRESHOLD}):   {len(high_words):,} words\")\n",
    "print(f\"  Medium ({MEDIUM_THRESHOLD} â‰¤ Zipf < {HIGH_THRESHOLD}): {len(medium_words):,} words\")\n",
    "print(f\"  Low (Zipf < {MEDIUM_THRESHOLD}):    {len(low_words):,} words\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nExamples (word: index â†’ Zipf):\")\n",
    "for word in ['the', 'is', 'have', 'important', 'keyboard']:\n",
    "    if word in word_to_index:\n",
    "        idx = word_to_index[word]\n",
    "        zipf = zipf_frequency(word, 'en')\n",
    "        print(f\"  '{word}': {idx} â†’ Zipf {zipf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenize & Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Tokenizing with Zipf-sorted indices...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenize using our Zipf-sorted word_to_index\n",
    "words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
    "sequences = [word_to_index.get(w, 0) for w in words]  # 0 for OOV\n",
    "\n",
    "# Remove zeros (OOV words)\n",
    "sequences = [idx for idx in sequences if idx > 0]\n",
    "sequences_array = np.array(sequences)\n",
    "\n",
    "print(f\"âœ“ Tokenized {len(sequences_array):,} tokens\")\n",
    "\n",
    "# Create tf.data dataset\n",
    "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    data=sequences_array[:-1],\n",
    "    targets=sequences_array[SEQUENCE_LENGTH:],\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    sequence_stride=1,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Split 90/10\n",
    "total_steps = (len(sequences) - SEQUENCE_LENGTH) // BATCH_SIZE\n",
    "val_steps = max(1, total_steps // 10)\n",
    "train_steps = total_steps - val_steps\n",
    "\n",
    "train_dataset = dataset.take(train_steps).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = dataset.skip(train_steps).take(val_steps).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"âœ“ Train: {train_steps} batches, Val: {val_steps} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build & Train GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# Enable Mixed Precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Build model\n",
    "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
    "x = Embedding(vocab_size, 128, name='embedding')(inputs)\n",
    "x = GRU(256, dropout=0.2, recurrent_dropout=0.2, name='gru')(x)\n",
    "x = Dropout(0.3, name='dropout')(x)\n",
    "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard_zipf')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Train')\n",
    "ax1.plot(history.history['val_loss'], label='Val')\n",
    "ax1.set_title('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Train')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal: Val Acc={history.history['val_accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save Keras model\n",
    "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras')\n",
    "\n",
    "# Save word_to_index (Zipf-sorted)\n",
    "with open(f'{DRIVE_DIR}/models/gru_keyboard/word_to_index.json', 'w') as f:\n",
    "    json.dump(word_to_index, f, separators=(',', ':'))\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'embedding_dim': 128,\n",
    "    'gru_units': 256,\n",
    "    'zipf_thresholds': {'high': HIGH_THRESHOLD, 'medium': MEDIUM_THRESHOLD}\n",
    "}\n",
    "with open(f'{DRIVE_DIR}/models/gru_keyboard/model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Saved: gru_model.keras, word_to_index.json, model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export TFLite (Android)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Exporting TFLite models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# IMPORTANT: GRU with recurrent_dropout requires Flex ops (SELECT_TF_OPS)\n",
    "\n",
    "# Option 1: Standard TFLite with Flex ops (recommended)\n",
    "print(\"\\n[1] Creating TFLite with Flex ops...\")\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard ops\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS      # Flex ops for GRU\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    tflite_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model.tflite'\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    size_mb = len(tflite_model) / (1024 * 1024)\n",
    "    print(f\"   âœ“ gru_model.tflite ({size_mb:.2f}MB)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âœ— Error: {str(e)[:100]}\")\n",
    "    tflite_path = None\n",
    "\n",
    "# Option 2: FP16 quantized (smaller)\n",
    "print(\"\\n[2] Creating FP16 quantized TFLite...\")\n",
    "try:\n",
    "    converter_fp16 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter_fp16.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter_fp16._experimental_lower_tensor_list_ops = False\n",
    "    converter_fp16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter_fp16.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    tflite_fp16 = converter_fp16.convert()\n",
    "    fp16_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model_fp16.tflite'\n",
    "    with open(fp16_path, 'wb') as f:\n",
    "        f.write(tflite_fp16)\n",
    "    \n",
    "    size_mb = len(tflite_fp16) / (1024 * 1024)\n",
    "    print(f\"   âœ“ gru_model_fp16.tflite ({size_mb:.2f}MB)\")\n",
    "    tflite_path = fp16_path\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âœ— FP16 error: {str(e)[:100]}\")\n",
    "\n",
    "# Benchmark\n",
    "print(\"\\n[3] Running latency benchmark...\")\n",
    "if tflite_path:\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    \n",
    "    for _ in range(10):\n",
    "        test_input = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
    "        interpreter.set_tensor(input_details['index'], test_input)\n",
    "        interpreter.invoke()\n",
    "    \n",
    "    latencies = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        interpreter.set_tensor(input_details['index'], test_input)\n",
    "        interpreter.invoke()\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "    \n",
    "    print(f\"   âœ“ Latency: avg={np.mean(latencies):.2f}ms, min={np.min(latencies):.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTE: Android needs TensorFlow Lite Flex delegate.\")\n",
    "print(\"Add to build.gradle:\")\n",
    "print(\"  implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.14.0'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export CoreML Weights (iOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(\"Exporting weights for CoreML conversion...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export weights\n",
    "weights_list = model.get_weights()\n",
    "weights_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_weights.npz'\n",
    "np.savez(weights_path, *weights_list)\n",
    "\n",
    "print(f\"âœ“ gru_weights.npz ({len(weights_list)} arrays)\")\n",
    "for i, w in enumerate(weights_list):\n",
    "    print(f\"   Weight {i}: {w.shape}\")\n",
    "\n",
    "print(f\"\\nâ†’ Run on Mac: python scripts/convert_to_coreml.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Mobile Resources\n",
    "\n",
    "Export optimized data structures for iOS/Android:\n",
    "- `vocab_high.json` - High-frequency words (always loaded)\n",
    "- `vocab_medium.json` - Medium-frequency words (lazy load)\n",
    "- `vocab_low.json` - Low-frequency words (lazy load)\n",
    "- `compact_trie.json` - **NEW** Trie structure for prefix completion\n",
    "- `symspell_index.json` - **NEW** SymSpell delete index for typo correction\n",
    "- `word_to_index.json` - Word to index mapping\n",
    "- `keyboard_adjacent.json` - Keyboard proximity map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Exporting mobile resources (BK-Tree + Compact Trie)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get vocab list in Zipf order (matches index order)\n",
    "vocab_list = sorted(word_to_index.items(), key=lambda x: x[1])\n",
    "vocab_words = [w for w, idx in vocab_list]\n",
    "index_to_word = {idx: w for w, idx in word_to_index.items()}\n",
    "\n",
    "# Chunk boundaries (based on Zipf classification)\n",
    "high_end = len(high_words)\n",
    "medium_end = high_end + len(medium_words)\n",
    "low_end = len(vocab_words)\n",
    "\n",
    "chunks = {\n",
    "    'high': (0, high_end),\n",
    "    'medium': (high_end, medium_end),\n",
    "    'low': (medium_end, low_end)\n",
    "}\n",
    "\n",
    "# 1. Export vocab chunks\n",
    "print(\"\\n[1/5] Exporting vocab chunks...\")\n",
    "for name, (start, end) in chunks.items():\n",
    "    chunk_words = vocab_words[start:end]\n",
    "    if not chunk_words:\n",
    "        continue\n",
    "    \n",
    "    data = {\n",
    "        \"chunk\": name,\n",
    "        \"startIndex\": start + 1,\n",
    "        \"endIndex\": end + 1,\n",
    "        \"words\": chunk_words\n",
    "    }\n",
    "    \n",
    "    path = f'{DRIVE_DIR}/models/gru_keyboard/vocab_{name}.json'\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, separators=(',', ':'))\n",
    "    \n",
    "    size_kb = os.path.getsize(path) / 1024\n",
    "    print(f\"   âœ“ vocab_{name}.json ({len(chunk_words):,} words, {size_kb:.1f}KB)\")\n",
    "\n",
    "# 2. Build Compact Trie\n",
    "print(\"\\n[2/5] Building compact trie...\")\n",
    "\n",
    "def build_compact_trie(words_with_indices, max_per_node=50):\n",
    "    \"\"\"Build a compact trie where each node stores word indices.\"\"\"\n",
    "    trie = {}\n",
    "    \n",
    "    for word, idx in words_with_indices:\n",
    "        node = trie\n",
    "        for char in word.lower():\n",
    "            if char not in node:\n",
    "                node[char] = {}\n",
    "            node = node[char]\n",
    "        \n",
    "        if \"$\" not in node:\n",
    "            node[\"$\"] = []\n",
    "        if len(node[\"$\"]) < max_per_node:\n",
    "            node[\"$\"].append(idx)\n",
    "    \n",
    "    return trie\n",
    "\n",
    "words_with_idx = [(w, idx) for w, idx in word_to_index.items() if idx <= 25000]\n",
    "compact_trie = build_compact_trie(words_with_idx)\n",
    "\n",
    "path = f'{DRIVE_DIR}/models/gru_keyboard/compact_trie.json'\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(compact_trie, f, separators=(',', ':'))\n",
    "size_kb = os.path.getsize(path) / 1024\n",
    "print(f\"   âœ“ compact_trie.json ({size_kb:.1f}KB)\")\n",
    "\n",
    "# 3. Build BK-Tree (NEW - Memory Efficient)\n",
    "print(\"\\n[3/5] Building BK-Tree for typo correction...\")\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    \"\"\"Calculate Levenshtein distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    prev = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = prev[j + 1] + 1\n",
    "            deletions = curr[j] + 1\n",
    "            substitutions = prev[j] + (c1 != c2)\n",
    "            curr.append(min(insertions, deletions, substitutions))\n",
    "        prev = curr\n",
    "    return prev[-1]\n",
    "\n",
    "def build_bk_tree(words_with_indices):\n",
    "    \"\"\"\n",
    "    Build BK-Tree and serialize to compact format.\n",
    "    Format: [[word_idx, [[distance, child_node], [distance, child_node], ...]]]\n",
    "    \"\"\"\n",
    "    if not words_with_indices:\n",
    "        return None\n",
    "    \n",
    "    # Take first word as root\n",
    "    root_word, root_idx = words_with_indices[0]\n",
    "    tree = {\"w\": root_idx, \"c\": {}}  # word index, children by distance\n",
    "    \n",
    "    for word, idx in words_with_indices[1:]:\n",
    "        node = tree\n",
    "        while True:\n",
    "            dist = levenshtein(index_to_word[node[\"w\"]].lower(), word.lower())\n",
    "            if dist == 0:\n",
    "                break  # Duplicate word\n",
    "            dist_str = str(dist)\n",
    "            if dist_str not in node[\"c\"]:\n",
    "                node[\"c\"][dist_str] = {\"w\": idx, \"c\": {}}\n",
    "                break\n",
    "            node = node[\"c\"][dist_str]\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def serialize_bk_tree(node):\n",
    "    \"\"\"Convert BK-tree to compact array format for smaller JSON.\"\"\"\n",
    "    if node is None:\n",
    "        return None\n",
    "    \n",
    "    # Format: [word_idx, {dist: child, ...}]\n",
    "    children = {}\n",
    "    for dist, child in node[\"c\"].items():\n",
    "        children[dist] = serialize_bk_tree(child)\n",
    "    \n",
    "    return [node[\"w\"], children] if children else [node[\"w\"]]\n",
    "\n",
    "# Build BK-tree from top 15000 words (high + medium frequency)\n",
    "bk_words = [(w, word_to_index[w]) for w in vocab_words[:15000]]\n",
    "print(f\"   Building tree from {len(bk_words):,} words...\")\n",
    "bk_tree = build_bk_tree(bk_words)\n",
    "serialized_tree = serialize_bk_tree(bk_tree)\n",
    "\n",
    "path = f'{DRIVE_DIR}/models/gru_keyboard/bk_tree.json'\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(serialized_tree, f, separators=(',', ':'))\n",
    "size_kb = os.path.getsize(path) / 1024\n",
    "print(f\"   âœ“ bk_tree.json ({size_kb:.1f}KB)\")\n",
    "\n",
    "# 4. Export word_to_index\n",
    "print(\"\\n[4/5] Exporting word_to_index...\")\n",
    "path = f'{DRIVE_DIR}/models/gru_keyboard/word_to_index.json'\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(word_to_index, f, separators=(',', ':'))\n",
    "size_kb = os.path.getsize(path) / 1024\n",
    "print(f\"   âœ“ word_to_index.json ({len(word_to_index):,} words, {size_kb:.1f}KB)\")\n",
    "\n",
    "# 5. Export keyboard adjacent map\n",
    "print(\"\\n[5/5] Exporting keyboard map...\")\n",
    "KEYBOARD_ADJACENT = {\n",
    "    'q': 'wa', 'w': 'qase', 'e': 'wsdr', 'r': 'edft', 't': 'rfgy',\n",
    "    'y': 'tghu', 'u': 'yhji', 'i': 'ujko', 'o': 'iklp', 'p': 'ol',\n",
    "    'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgvc', 'g': 'ftyhbv',\n",
    "    'h': 'gyujnb', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop',\n",
    "    'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn',\n",
    "    'n': 'bhjm', 'm': 'njk'\n",
    "}\n",
    "path = f'{DRIVE_DIR}/models/gru_keyboard/keyboard_adjacent.json'\n",
    "with open(path, 'w') as f:\n",
    "    json.dump(KEYBOARD_ADJACENT, f)\n",
    "print(\"   âœ“ keyboard_adjacent.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT COMPLETE (BK-Tree + Compact Trie)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFiles in {DRIVE_DIR}/models/gru_keyboard/:\")\n",
    "for f in sorted(os.listdir(f'{DRIVE_DIR}/models/gru_keyboard/')):\n",
    "    size = os.path.getsize(f'{DRIVE_DIR}/models/gru_keyboard/{f}') / 1024\n",
    "    print(f\"   {f}: {size:.1f}KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verification Test\n",
    "\n",
    "Test the exported resources work correctly with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load exported word_to_index\n",
    "with open(f'{DRIVE_DIR}/models/gru_keyboard/word_to_index.json', 'r') as f:\n",
    "    loaded_w2i = json.load(f)\n",
    "\n",
    "# Build index_to_word from loaded mapping\n",
    "loaded_i2w = {int(idx): word for word, idx in loaded_w2i.items()}\n",
    "\n",
    "def predict_next_word(context, top_k=5):\n",
    "    \"\"\"Predict next word using loaded vocab\"\"\"\n",
    "    words = context.lower().split()\n",
    "    seq = [loaded_w2i.get(w, 0) for w in words]\n",
    "    seq = seq[-SEQUENCE_LENGTH:]\n",
    "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
    "    \n",
    "    preds = model.predict(seq, verbose=0)[0]\n",
    "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        if idx in loaded_i2w:\n",
    "            results.append((loaded_i2w[idx], preds[idx] * 100))\n",
    "    return results\n",
    "\n",
    "def complete_prefix(prefix, top_k=5):\n",
    "    \"\"\"Complete word using prefix index\"\"\"\n",
    "    with open(f'{DRIVE_DIR}/models/gru_keyboard/prefix_index.json', 'r') as f:\n",
    "        loaded_prefix = json.load(f)\n",
    "    \n",
    "    prefix = prefix.lower()\n",
    "    if prefix not in loaded_prefix:\n",
    "        return []\n",
    "    \n",
    "    indices = loaded_prefix[prefix][:top_k]\n",
    "    return [(loaded_i2w.get(idx, '?'), 100 / (i + 1)) for i, idx in enumerate(indices)]\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"How are \", \"next_word\"),\n",
    "    (\"I want to \", \"next_word\"),\n",
    "    (\"hel\", \"prefix\"),\n",
    "    (\"imp\", \"prefix\"),\n",
    "    (\"sug\", \"prefix\"),\n",
    "]\n",
    "\n",
    "for input_text, test_type in test_cases:\n",
    "    print(f\"\\nðŸ“ Input: '{input_text}' ({test_type})\")\n",
    "    \n",
    "    if test_type == \"next_word\":\n",
    "        results = predict_next_word(input_text.strip(), top_k=5)\n",
    "    else:\n",
    "        results = complete_prefix(input_text, top_k=5)\n",
    "    \n",
    "    for i, (word, score) in enumerate(results, 1):\n",
    "        emoji = \"ðŸŸ¢\" if score > 50 else \"ðŸŸ¡\" if score > 10 else \"âšª\"\n",
    "        print(f\"   {i}. {word:15s} {emoji} {score:5.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… VERIFICATION PASSED\")\n",
    "print(\"   - Model uses Zipf-sorted indices\")\n",
    "print(\"   - Exported word_to_index matches training\")\n",
    "print(\"   - Prefix index works correctly\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
