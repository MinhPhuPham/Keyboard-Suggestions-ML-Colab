{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owXaugdVe2bo"
      },
      "source": [
        "# GRU Keyboard Model (2026 Optimized)\n",
        "\n",
        "Train a GRU model with **Zipf-sorted vocabulary** for optimal mobile chunking.\n",
        "\n",
        "**Workflow:**\n",
        "1. Setup & Config\n",
        "2. Load Data\n",
        "3. Create Zipf-Sorted Vocabulary (NEW)\n",
        "4. Tokenize & Create Sequences\n",
        "5. Build & Train Model\n",
        "6. Save Model\n",
        "7. Export TFLite (Android)\n",
        "8. Export CoreML (iOS)\n",
        "9. Export Mobile Resources\n",
        "10. Verification Test\n",
        "\n",
        "**Key Feature:** Token indices match Zipf frequency order - high-frequency words have low indices.\n",
        "\n",
        "---\n",
        "**Instructions:**\n",
        "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
        "2. Set `TESTING_MODE = True` for quick test\n",
        "3. Set `TESTING_MODE = False` for full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup directories\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
        "os.makedirs(f\"{DRIVE_DIR}/models/gru_keyboard\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q tensorflow keras nltk pandas numpy scikit-learn tqdm wordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "TESTING_MODE = True  # â† Change to False for full training\n",
        "\n",
        "if TESTING_MODE:\n",
        "    NUM_EPOCHS = 3\n",
        "    BATCH_SIZE = 512\n",
        "    VOCAB_SIZE_LIMIT = 25000\n",
        "    SEQUENCE_LENGTH = 10\n",
        "else:\n",
        "    NUM_EPOCHS = 20\n",
        "    BATCH_SIZE = 512\n",
        "    VOCAB_SIZE_LIMIT = 25000\n",
        "    SEQUENCE_LENGTH = 10\n",
        "\n",
        "# Zipf thresholds for chunking\n",
        "HIGH_THRESHOLD = 4.0\n",
        "MEDIUM_THRESHOLD = 2.5\n",
        "\n",
        "print(f\"Config: vocab={VOCAB_SIZE_LIMIT:,}, seq={SEQUENCE_LENGTH}, epochs={NUM_EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "all_text = []\n",
        "\n",
        "if TESTING_MODE:\n",
        "    CORPUS_PATH = f\"{DRIVE_DIR}/datasets/keyboard_training_data.txt\"\n",
        "    if not os.path.exists(CORPUS_PATH):\n",
        "        raise FileNotFoundError(f\"Missing: {CORPUS_PATH}\")\n",
        "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
        "        all_text.append(f.read())\n",
        "    print(f\"âœ“ Loaded test corpus: {len(all_text[0]):,} chars\")\n",
        "else:\n",
        "    # Full training: Fake.csv + True.csv + 1661-0.txt\n",
        "    fake_df = pd.read_csv(f\"{DRIVE_DIR}/datasets/Fake.csv\")\n",
        "    true_df = pd.read_csv(f\"{DRIVE_DIR}/datasets/True.csv\")\n",
        "    all_text.extend(fake_df['text'].tolist())\n",
        "    all_text.extend(true_df['text'].tolist())\n",
        "    with open(f\"{DRIVE_DIR}/datasets/1661-0.txt\", 'r', encoding='utf-8') as f:\n",
        "        all_text.append(f.read())\n",
        "    print(f\"âœ“ Loaded {len(all_text):,} text segments\")\n",
        "\n",
        "# Combine and clean\n",
        "combined_text = ' '.join(all_text).lower()\n",
        "combined_text = ' '.join(combined_text.split())\n",
        "print(f\"âœ“ Total: {len(combined_text):,} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Zipf-Sorted Vocabulary (KEY STEP)\n",
        "\n",
        "This creates a vocabulary where **token indices match Zipf frequency order**:\n",
        "- Index 0: Most frequent word (\"the\")\n",
        "- Index N: Least frequent word\n",
        "\n",
        "This ensures model's output indices directly correspond to frequency-sorted vocab for mobile chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from wordfreq import zipf_frequency\n",
        "import re\n",
        "\n",
        "print(\"Creating Zipf-sorted vocabulary...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Count words in corpus\n",
        "words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
        "word_counts = Counter(words)\n",
        "print(f\"âœ“ Found {len(word_counts):,} unique words in corpus\")\n",
        "\n",
        "# 2. Get Zipf frequency for each word and sort\n",
        "word_zipf_list = []\n",
        "for word, count in word_counts.items():\n",
        "    if len(word) >= 1:  # Include all words\n",
        "        zipf = zipf_frequency(word, 'en')\n",
        "        word_zipf_list.append((word, zipf, count))\n",
        "\n",
        "# Sort by Zipf (descending) - most frequent first\n",
        "word_zipf_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 3. Limit to vocab size\n",
        "word_zipf_list = word_zipf_list[:VOCAB_SIZE_LIMIT]\n",
        "\n",
        "# 4. Create word_to_index mapping (Zipf order = index order)\n",
        "# Index 1 = most frequent (reserve 0 for padding)\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "for idx, (word, zipf, count) in enumerate(word_zipf_list, start=1):\n",
        "    word_to_index[word] = idx\n",
        "    index_to_word[idx] = word\n",
        "\n",
        "vocab_size = len(word_to_index) + 1  # +1 for padding index 0\n",
        "\n",
        "# 5. Classify into chunks\n",
        "high_words = [(w, z) for w, z, c in word_zipf_list if z >= HIGH_THRESHOLD]\n",
        "medium_words = [(w, z) for w, z, c in word_zipf_list if MEDIUM_THRESHOLD <= z < HIGH_THRESHOLD]\n",
        "low_words = [(w, z) for w, z, c in word_zipf_list if z < MEDIUM_THRESHOLD]\n",
        "\n",
        "print(f\"\\nâœ“ Vocabulary size: {vocab_size:,}\")\n",
        "print(f\"  High (Zipf â‰¥ {HIGH_THRESHOLD}):   {len(high_words):,} words\")\n",
        "print(f\"  Medium ({MEDIUM_THRESHOLD} â‰¤ Zipf < {HIGH_THRESHOLD}): {len(medium_words):,} words\")\n",
        "print(f\"  Low (Zipf < {MEDIUM_THRESHOLD}):    {len(low_words):,} words\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\nExamples (word: index â†’ Zipf):\")\n",
        "for word in ['the', 'is', 'have', 'important', 'keyboard']:\n",
        "    if word in word_to_index:\n",
        "        idx = word_to_index[word]\n",
        "        zipf = zipf_frequency(word, 'en')\n",
        "        print(f\"  '{word}': {idx} â†’ Zipf {zipf:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenize & Create Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"Tokenizing with Zipf-sorted indices...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Tokenize using our Zipf-sorted word_to_index\n",
        "words = re.findall(r'\\b[a-z]+\\b', combined_text)\n",
        "sequences = [word_to_index.get(w, 0) for w in words]  # 0 for OOV\n",
        "\n",
        "# Remove zeros (OOV words)\n",
        "sequences = [idx for idx in sequences if idx > 0]\n",
        "sequences_array = np.array(sequences)\n",
        "\n",
        "print(f\"âœ“ Tokenized {len(sequences_array):,} tokens\")\n",
        "\n",
        "# Create tf.data dataset\n",
        "dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=sequences_array[:-1],\n",
        "    targets=sequences_array[SEQUENCE_LENGTH:],\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    sequence_stride=1,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Split 90/10\n",
        "total_steps = (len(sequences) - SEQUENCE_LENGTH) // BATCH_SIZE\n",
        "val_steps = max(1, total_steps // 10)\n",
        "train_steps = total_steps - val_steps\n",
        "\n",
        "train_dataset = dataset.take(train_steps).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = dataset.skip(train_steps).take(val_steps).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"âœ“ Train: {train_steps} batches, Val: {val_steps} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build & Train GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "# Enable Mixed Precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Build model\n",
        "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
        "x = Embedding(vocab_size, 128, name='embedding')(inputs)\n",
        "x = GRU(256, dropout=0.2, recurrent_dropout=0.2, name='gru')(x)\n",
        "x = Dropout(0.3, name='dropout')(x)\n",
        "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard_zipf')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        f'{DRIVE_DIR}/models/gru_keyboard/best_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(history.history['loss'], label='Train')\n",
        "ax1.plot(history.history['val_loss'], label='Val')\n",
        "ax1.set_title('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], label='Train')\n",
        "ax2.plot(history.history['val_accuracy'], label='Val')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal: Val Acc={history.history['val_accuracy'][-1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save Keras model\n",
        "model.save(f'{DRIVE_DIR}/models/gru_keyboard/gru_model.keras')\n",
        "\n",
        "# Save word_to_index (Zipf-sorted)\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/word_to_index.json', 'w') as f:\n",
        "    json.dump(word_to_index, f, separators=(',', ':'))\n",
        "\n",
        "# Save config\n",
        "config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'sequence_length': SEQUENCE_LENGTH,\n",
        "    'embedding_dim': 128,\n",
        "    'gru_units': 256,\n",
        "    'zipf_thresholds': {'high': HIGH_THRESHOLD, 'medium': MEDIUM_THRESHOLD}\n",
        "}\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/model_config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"âœ“ Saved: gru_model.keras, word_to_index.json, model_config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export TFLite (Android)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Exporting TFLite models...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# FP16 TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "tflite_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_model_fp16.tflite'\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "size_mb = len(tflite_model) / (1024 * 1024)\n",
        "print(f\"âœ“ gru_model_fp16.tflite ({size_mb:.2f}MB)\")\n",
        "\n",
        "# Benchmark\n",
        "import time\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "\n",
        "# Warmup + benchmark\n",
        "for _ in range(10):\n",
        "    test_input = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
        "    interpreter.set_tensor(input_details['index'], test_input)\n",
        "    interpreter.invoke()\n",
        "\n",
        "latencies = []\n",
        "for _ in range(50):\n",
        "    start = time.time()\n",
        "    interpreter.set_tensor(input_details['index'], test_input)\n",
        "    interpreter.invoke()\n",
        "    latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "print(f\"âœ“ Latency: avg={np.mean(latencies):.2f}ms, min={np.min(latencies):.2f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export CoreML Weights (iOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "print(\"Exporting weights for CoreML conversion...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Export weights\n",
        "weights_list = model.get_weights()\n",
        "weights_path = f'{DRIVE_DIR}/models/gru_keyboard/gru_weights.npz'\n",
        "np.savez(weights_path, *weights_list)\n",
        "\n",
        "print(f\"âœ“ gru_weights.npz ({len(weights_list)} arrays)\")\n",
        "for i, w in enumerate(weights_list):\n",
        "    print(f\"   Weight {i}: {w.shape}\")\n",
        "\n",
        "print(f\"\\nâ†’ Run on Mac: python scripts/convert_to_coreml.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Mobile Resources\n",
        "\n",
        "Export chunked vocabulary and indices for iOS/Android:\n",
        "- `vocab_high.json` - High-frequency words (always loaded)\n",
        "- `vocab_medium.json` - Medium-frequency words (lazy load)\n",
        "- `vocab_low.json` - Low-frequency words (lazy load)\n",
        "- `prefix_index.json` - For fast prefix search\n",
        "- `soundex_index.json` - For typo correction\n",
        "- `keyboard_adjacent.json` - Keyboard proximity map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "print(\"Exporting mobile resources...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get vocab list in Zipf order (matches index order)\n",
        "vocab_list = sorted(word_to_index.items(), key=lambda x: x[1])\n",
        "vocab_words = [w for w, idx in vocab_list]\n",
        "\n",
        "# Chunk boundaries (based on Zipf classification)\n",
        "high_end = len(high_words)\n",
        "medium_end = high_end + len(medium_words)\n",
        "low_end = len(vocab_words)\n",
        "\n",
        "chunks = {\n",
        "    'high': (0, high_end),\n",
        "    'medium': (high_end, medium_end),\n",
        "    'low': (medium_end, low_end)\n",
        "}\n",
        "\n",
        "# 1. Export vocab chunks\n",
        "print(\"\\n[1/4] Exporting vocab chunks...\")\n",
        "chunk_info = {}\n",
        "for name, (start, end) in chunks.items():\n",
        "    chunk_words = vocab_words[start:end]\n",
        "    if not chunk_words:\n",
        "        continue\n",
        "    \n",
        "    data = {\n",
        "        \"chunk\": name,\n",
        "        \"startIndex\": start + 1,  # +1 because index 0 is padding\n",
        "        \"endIndex\": end + 1,\n",
        "        \"words\": chunk_words\n",
        "    }\n",
        "    \n",
        "    path = f'{DRIVE_DIR}/models/gru_keyboard/vocab_{name}.json'\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(data, f, separators=(',', ':'))\n",
        "    \n",
        "    size_kb = os.path.getsize(path) / 1024\n",
        "    chunk_info[name] = {'file': f'vocab_{name}.json', 'words': len(chunk_words), 'sizeKB': round(size_kb, 1)}\n",
        "    print(f\"   âœ“ vocab_{name}.json ({len(chunk_words):,} words, {size_kb:.1f}KB)\")\n",
        "\n",
        "# 2. Export prefix index\n",
        "print(\"\\n[2/4] Building prefix index...\")\n",
        "prefix_index = {}\n",
        "for word in vocab_words:\n",
        "    for plen in range(1, min(5, len(word) + 1)):\n",
        "        prefix = word[:plen]\n",
        "        if prefix not in prefix_index:\n",
        "            prefix_index[prefix] = []\n",
        "        if len(prefix_index[prefix]) < 50:\n",
        "            prefix_index[prefix].append(word_to_index[word])\n",
        "\n",
        "path = f'{DRIVE_DIR}/models/gru_keyboard/prefix_index.json'\n",
        "with open(path, 'w') as f:\n",
        "    json.dump(prefix_index, f, separators=(',', ':'))\n",
        "print(f\"   âœ“ prefix_index.json ({len(prefix_index):,} prefixes)\")\n",
        "\n",
        "# 3. Export soundex index\n",
        "print(\"\\n[3/4] Building soundex index...\")\n",
        "def soundex(word):\n",
        "    if not word:\n",
        "        return \"\"\n",
        "    word = word.upper()\n",
        "    mapping = {\n",
        "        'B': '1', 'F': '1', 'P': '1', 'V': '1',\n",
        "        'C': '2', 'G': '2', 'J': '2', 'K': '2', 'Q': '2', 'S': '2', 'X': '2', 'Z': '2',\n",
        "        'D': '3', 'T': '3', 'L': '4', 'M': '5', 'N': '5', 'R': '6'\n",
        "    }\n",
        "    coded = word[0]\n",
        "    prev = mapping.get(word[0], '0')\n",
        "    for c in word[1:]:\n",
        "        code = mapping.get(c, '0')\n",
        "        if code != '0' and code != prev:\n",
        "            coded += code\n",
        "        prev = code if code != '0' else prev\n",
        "    return (coded + '0000')[:4]\n",
        "\n",
        "soundex_index = {}\n",
        "for word in vocab_words[:15000]:\n",
        "    code = soundex(word)\n",
        "    if code not in soundex_index:\n",
        "        soundex_index[code] = []\n",
        "    if len(soundex_index[code]) < 30:\n",
        "        soundex_index[code].append(word_to_index[word])\n",
        "\n",
        "path = f'{DRIVE_DIR}/models/gru_keyboard/soundex_index.json'\n",
        "with open(path, 'w') as f:\n",
        "    json.dump(soundex_index, f, separators=(',', ':'))\n",
        "print(f\"   âœ“ soundex_index.json ({len(soundex_index):,} codes)\")\n",
        "\n",
        "# 4. Export keyboard adjacent map\n",
        "print(\"\\n[4/4] Exporting keyboard map...\")\n",
        "KEYBOARD_ADJACENT = {\n",
        "    'q': 'wa', 'w': 'qase', 'e': 'wsdr', 'r': 'edft', 't': 'rfgy',\n",
        "    'y': 'tghu', 'u': 'yhji', 'i': 'ujko', 'o': 'iklp', 'p': 'ol',\n",
        "    'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgvc', 'g': 'ftyhbv',\n",
        "    'h': 'gyujnb', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop',\n",
        "    'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn',\n",
        "    'n': 'bhjm', 'm': 'njk'\n",
        "}\n",
        "path = f'{DRIVE_DIR}/models/gru_keyboard/keyboard_adjacent.json'\n",
        "with open(path, 'w') as f:\n",
        "    json.dump(KEYBOARD_ADJACENT, f)\n",
        "print(\"   âœ“ keyboard_adjacent.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPORT COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nFiles in {DRIVE_DIR}/models/gru_keyboard/:\")\n",
        "for f in sorted(os.listdir(f'{DRIVE_DIR}/models/gru_keyboard/')):\n",
        "    size = os.path.getsize(f'{DRIVE_DIR}/models/gru_keyboard/{f}') / 1024\n",
        "    print(f\"   {f}: {size:.1f}KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Verification Test\n",
        "\n",
        "Test the exported resources work correctly with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VERIFICATION TEST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load exported word_to_index\n",
        "with open(f'{DRIVE_DIR}/models/gru_keyboard/word_to_index.json', 'r') as f:\n",
        "    loaded_w2i = json.load(f)\n",
        "\n",
        "# Build index_to_word from loaded mapping\n",
        "loaded_i2w = {int(idx): word for word, idx in loaded_w2i.items()}\n",
        "\n",
        "def predict_next_word(context, top_k=5):\n",
        "    \"\"\"Predict next word using loaded vocab\"\"\"\n",
        "    words = context.lower().split()\n",
        "    seq = [loaded_w2i.get(w, 0) for w in words]\n",
        "    seq = seq[-SEQUENCE_LENGTH:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
        "    \n",
        "    preds = model.predict(seq, verbose=0)[0]\n",
        "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_idx:\n",
        "        if idx in loaded_i2w:\n",
        "            results.append((loaded_i2w[idx], preds[idx] * 100))\n",
        "    return results\n",
        "\n",
        "def complete_prefix(prefix, top_k=5):\n",
        "    \"\"\"Complete word using prefix index\"\"\"\n",
        "    with open(f'{DRIVE_DIR}/models/gru_keyboard/prefix_index.json', 'r') as f:\n",
        "        loaded_prefix = json.load(f)\n",
        "    \n",
        "    prefix = prefix.lower()\n",
        "    if prefix not in loaded_prefix:\n",
        "        return []\n",
        "    \n",
        "    indices = loaded_prefix[prefix][:top_k]\n",
        "    return [(loaded_i2w.get(idx, '?'), 100 / (i + 1)) for i, idx in enumerate(indices)]\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    (\"How are \", \"next_word\"),\n",
        "    (\"I want to \", \"next_word\"),\n",
        "    (\"hel\", \"prefix\"),\n",
        "    (\"imp\", \"prefix\"),\n",
        "    (\"sug\", \"prefix\"),\n",
        "]\n",
        "\n",
        "for input_text, test_type in test_cases:\n",
        "    print(f\"\\nðŸ“ Input: '{input_text}' ({test_type})\")\n",
        "    \n",
        "    if test_type == \"next_word\":\n",
        "        results = predict_next_word(input_text.strip(), top_k=5)\n",
        "    else:\n",
        "        results = complete_prefix(input_text, top_k=5)\n",
        "    \n",
        "    for i, (word, score) in enumerate(results, 1):\n",
        "        emoji = \"ðŸŸ¢\" if score > 50 else \"ðŸŸ¡\" if score > 10 else \"âšª\"\n",
        "        print(f\"   {i}. {word:15s} {emoji} {score:5.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… VERIFICATION PASSED\")\n",
        "print(\"   - Model uses Zipf-sorted indices\")\n",
        "print(\"   - Exported word_to_index matches training\")\n",
        "print(\"   - Prefix index works correctly\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
