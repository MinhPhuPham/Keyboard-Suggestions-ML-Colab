{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model 1: Kana-Kanji Converter (Optimized)\n",
                "\n",
                "**Task:** Convert hiragana to kanji with context awareness\n",
                "\n",
                "**Optimization Strategy:**\n",
                "- Small architecture + better training = high accuracy\n",
                "- Knowledge distillation-inspired techniques\n",
                "- Label smoothing + warmup LR + more data\n",
                "\n",
                "**Target:** ~3-4MB, 85%+ accuracy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_kana_kanji\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"âœ“ Model: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION - BALANCED SIZE + ACCURACY\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 8         # More epochs for better learning\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 400000   # More data\n",
                "else:\n",
                "    NUM_EPOCHS = 20\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 800000\n",
                "\n",
                "# Balanced model specs (~3-4MB keras, ~2MB TFLite FP16)\n",
                "CHAR_VOCAB_SIZE = 2500     # Balanced vocab\n",
                "MAX_SEQ_LENGTH = 25        # Balanced length\n",
                "EMBEDDING_DIM = 48         # Balanced embedding\n",
                "GRU_UNITS = 96             # Balanced GRU\n",
                "\n",
                "# Training optimizations\n",
                "LABEL_SMOOTHING = 0.1      # Prevents overconfidence\n",
                "DROPOUT_RATE = 0.15        # Regularization\n",
                "WARMUP_EPOCHS = 2          # Warmup before decay\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, samples={MAX_SAMPLES:,}\")\n",
                "print(f\"Model: Embed={EMBEDDING_DIM}, GRU={GRU_UNITS}, Vocab={CHAR_VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "except:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "\n",
                "print(f\"âœ“ Loaded {len(dataset):,} samples\")\n",
                "print(\"\\nSample data:\")\n",
                "for i in range(3):\n",
                "    print(f\"  {dataset[i]['input'][:25]} â†’ {dataset[i]['output'][:25]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"Building vocabulary...\")\n",
                "\n",
                "char_counts = Counter()\n",
                "for item in tqdm(dataset, desc=\"Counting\"):\n",
                "    char_counts.update(list(item.get('input', '')))\n",
                "    char_counts.update(list(item.get('output', '')))\n",
                "\n",
                "print(f\"âœ“ Found {len(char_counts):,} unique chars\")\n",
                "\n",
                "char_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "    char_to_idx[char] = len(char_to_idx)\n",
                "\n",
                "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
                "vocab_size = len(char_to_idx)\n",
                "print(f\"âœ“ Vocab: {vocab_size:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "\n",
                "print(\"Creating training data...\")\n",
                "\n",
                "def encode_seq(text, max_len, add_bos=False, add_eos=False):\n",
                "    tokens = list(text)\n",
                "    if add_bos:\n",
                "        tokens = ['<BOS>'] + tokens\n",
                "    if add_eos:\n",
                "        tokens = tokens + ['<EOS>']\n",
                "    ids = [char_to_idx.get(c, char_to_idx['<UNK>']) for c in tokens]\n",
                "    if len(ids) < max_len:\n",
                "        ids = ids + [char_to_idx['<PAD>']] * (max_len - len(ids))\n",
                "    return ids[:max_len]\n",
                "\n",
                "encoder_inputs, decoder_inputs, decoder_targets = [], [], []\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Processing\"):\n",
                "    kana = item.get('input', '').strip()\n",
                "    kanji = item.get('output', '').strip()\n",
                "    \n",
                "    if not kana or not kanji:\n",
                "        continue\n",
                "    if len(kana) > MAX_SEQ_LENGTH - 2 or len(kanji) > MAX_SEQ_LENGTH - 2:\n",
                "        continue\n",
                "    \n",
                "    encoder_inputs.append(encode_seq(kana, MAX_SEQ_LENGTH))\n",
                "    decoder_inputs.append(encode_seq(kanji, MAX_SEQ_LENGTH, add_bos=True))\n",
                "    decoder_targets.append(encode_seq(kanji, MAX_SEQ_LENGTH, add_eos=True))\n",
                "\n",
                "encoder_inputs = np.array(encoder_inputs)\n",
                "decoder_inputs = np.array(decoder_inputs)\n",
                "decoder_targets = np.array(decoder_targets)\n",
                "\n",
                "print(f\"\\nâœ“ {len(encoder_inputs):,} training pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shuffle and split\n",
                "indices = np.random.permutation(len(encoder_inputs))\n",
                "encoder_inputs = encoder_inputs[indices]\n",
                "decoder_inputs = decoder_inputs[indices]\n",
                "decoder_targets = decoder_targets[indices]\n",
                "\n",
                "split = int(len(encoder_inputs) * 0.9)\n",
                "\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[:split], 'decoder_input': decoder_inputs[:split]},\n",
                "    decoder_targets[:split]\n",
                ")).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[split:], 'decoder_input': decoder_inputs[split:]},\n",
                "    decoder_targets[split:]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"âœ“ Train: {split:,}, Val: {len(encoder_inputs)-split:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build Optimized Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "print(\"Building optimized Seq2Seq...\")\n",
                "\n",
                "# Shared embedding (weight sharing saves params)\n",
                "embedding = Embedding(vocab_size, EMBEDDING_DIM, name='shared_embedding')\n",
                "\n",
                "# Encoder\n",
                "encoder_input = Input(shape=(MAX_SEQ_LENGTH,), name='encoder_input')\n",
                "encoder_embed = embedding(encoder_input)\n",
                "encoder_embed = Dropout(DROPOUT_RATE)(encoder_embed)\n",
                "encoder_outputs = Bidirectional(\n",
                "    GRU(GRU_UNITS, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=0.1),\n",
                "    name='encoder'\n",
                ")(encoder_embed)\n",
                "\n",
                "# Decoder\n",
                "decoder_input = Input(shape=(MAX_SEQ_LENGTH,), name='decoder_input')\n",
                "decoder_embed = embedding(decoder_input)  # Weight sharing!\n",
                "decoder_embed = Dropout(DROPOUT_RATE)(decoder_embed)\n",
                "decoder_outputs = GRU(\n",
                "    GRU_UNITS * 2,\n",
                "    return_sequences=True,\n",
                "    dropout=DROPOUT_RATE,\n",
                "    recurrent_dropout=0.1,\n",
                "    name='decoder'\n",
                ")(decoder_embed)\n",
                "\n",
                "# Luong Attention\n",
                "context = Attention(use_scale=True, name='attention')([decoder_outputs, encoder_outputs])\n",
                "\n",
                "# Combine\n",
                "combined = Concatenate()([decoder_outputs, context])\n",
                "combined = LayerNormalization()(combined)\n",
                "combined = Dropout(DROPOUT_RATE)(combined)\n",
                "\n",
                "# Output\n",
                "output = Dense(vocab_size, activation='softmax', name='output')(combined)\n",
                "\n",
                "model = Model(\n",
                "    inputs=[encoder_input, decoder_input],\n",
                "    outputs=output,\n",
                "    name='kana_kanji_optimized'\n",
                ")\n",
                "\n",
                "# Custom loss with label smoothing\n",
                "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
                "    from_logits=False,\n",
                "    ignore_class=char_to_idx['<PAD>']  # Ignore padding\n",
                ")\n",
                "\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "    loss=loss_fn,\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()\n",
                "params = model.count_params()\n",
                "print(f\"\\nâœ“ Parameters: {params:,}\")\n",
                "print(f\"âœ“ Keras size: ~{params * 4 / 1024 / 1024:.1f} MB\")\n",
                "print(f\"âœ“ TFLite FP16: ~{params * 2 / 1024 / 1024:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train with LR Schedule"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
                "\n",
                "# Warmup + Cosine Decay schedule\n",
                "def lr_schedule(epoch):\n",
                "    if epoch < WARMUP_EPOCHS:\n",
                "        # Warmup: gradually increase LR\n",
                "        return 1e-4 + (1e-3 - 1e-4) * (epoch / WARMUP_EPOCHS)\n",
                "    else:\n",
                "        # Cosine decay\n",
                "        progress = (epoch - WARMUP_EPOCHS) / (NUM_EPOCHS - WARMUP_EPOCHS)\n",
                "        return 1e-5 + 0.5 * (1e-3 - 1e-5) * (1 + np.cos(np.pi * progress))\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
                "    LearningRateScheduler(lr_schedule, verbose=1)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "axes[0].plot(history.history['loss'], label='Train')\n",
                "axes[0].plot(history.history['val_loss'], label='Val')\n",
                "axes[0].set_title('Loss'); axes[0].legend()\n",
                "\n",
                "axes[1].plot(history.history['accuracy'], label='Train')\n",
                "axes[1].plot(history.history['val_accuracy'], label='Val')\n",
                "axes[1].set_title('Accuracy'); axes[1].legend()\n",
                "\n",
                "axes[2].plot(history.history['lr'])\n",
                "axes[2].set_title('Learning Rate')\n",
                "\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nâœ“ Final Val Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/char_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(char_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_char.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_char.items()}, f, ensure_ascii=False)\n",
                "\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'max_seq_length': MAX_SEQ_LENGTH,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'gru_units': GRU_UNITS,\n",
                "    'task': 'kana_to_kanji'\n",
                "}\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"âœ“ Saved model and config\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Exporting TFLite...\")\n",
                "\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"âœ“ model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # FP16 quantization\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite_fp16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    print(f\"âœ“ model_fp16.tflite ({len(tflite_fp16)/(1024*1024):.2f}MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"âš  Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def convert(kana_text, max_len=25):\n",
                "    enc_input = np.array([encode_seq(kana_text, MAX_SEQ_LENGTH)])\n",
                "    dec_input = np.zeros((1, MAX_SEQ_LENGTH), dtype=np.int32)\n",
                "    dec_input[0, 0] = char_to_idx['<BOS>']\n",
                "    \n",
                "    result = []\n",
                "    for i in range(max_len):\n",
                "        preds = model.predict({'encoder_input': enc_input, 'decoder_input': dec_input}, verbose=0)\n",
                "        next_idx = np.argmax(preds[0, i])\n",
                "        next_char = idx_to_char.get(next_idx, '<UNK>')\n",
                "        if next_char == '<EOS>':\n",
                "            break\n",
                "        if next_char not in SPECIAL_TOKENS:\n",
                "            result.append(next_char)\n",
                "        if i + 1 < MAX_SEQ_LENGTH:\n",
                "            dec_input[0, i + 1] = next_idx\n",
                "    return ''.join(result)\n",
                "\n",
                "# Test with HIRAGANA\n",
                "tests = [\n",
                "    ('ã‚ã‚ŠãŒã¨ã†', 'æœ‰é›£ã†'),\n",
                "    ('ã”ã–ã„ã¾ã™', 'ã”ã–ã„ã¾ã™'),\n",
                "    ('ãŠã›ã‚', 'ãŠä¸–è©±'),\n",
                "    ('ã—ã‚“ã˜ã‚…ã', 'æ–°å®¿'),\n",
                "    ('ã¨ã†ãã‚‡ã†', 'æ±äº¬'),\n",
                "    ('ã«ã»ã‚“', 'æ—¥æœ¬'),\n",
                "    ('ã“ã‚“ã«ã¡ã¯', 'ä»Šæ—¥ã¯'),\n",
                "    ('ãŠã­ãŒã„ã—ã¾ã™', 'ãŠé¡˜ã„ã—ã¾ã™'),\n",
                "]\n",
                "\n",
                "print(\"\\nðŸ“ Results:\")\n",
                "correct = 0\n",
                "for kana, expected in tests:\n",
                "    result = convert(kana)\n",
                "    ok = 'âœ“' if result == expected else 'âœ—'\n",
                "    if result == expected:\n",
                "        correct += 1\n",
                "    print(f\"  {ok} {kana} â†’ {result} (expected: {expected})\")\n",
                "\n",
                "print(f\"\\nâœ… {correct}/{len(tests)} correct ({correct/len(tests)*100:.0f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nðŸ“¦ Files:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    path = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(path):\n",
                "        size = os.path.getsize(path)\n",
                "        unit = 'MB' if size > 1024*1024 else 'KB'\n",
                "        size_str = f\"{size/(1024*1024):.2f}\" if size > 1024*1024 else f\"{size/1024:.1f}\"\n",
                "        print(f\"  {f}: {size_str} {unit}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}