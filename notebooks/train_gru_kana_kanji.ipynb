{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Context-Aware Kana-Kanji Converter (v2)\n",
                "\n",
                "**Supports**: Google Colab & Kaggle\n",
                "\n",
                "**Key Insight**: Model size is controlled by ARCHITECTURE, not by sample count!\n",
                "- More samples = longer training but BETTER accuracy\n",
                "- Architecture params (EMBEDDING_DIM, GRU_UNITS) = control size\n",
                "\n",
                "**Input Format**: `context<SEP>kana`\n",
                "- Before `<SEP>`: context (already converted, kanji)\n",
                "- After `<SEP>`: kana to convert (hiragana)\n",
                "\n",
                "**Example**:\n",
                "```\n",
                "Input:  ÂÜôÁúü„Çí<SEP>„Å®„Çã\n",
                "Output: ÊíÆ„Çã\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect platform: Kaggle or Colab\n",
                "if os.path.exists('/kaggle'):\n",
                "    # Kaggle\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    # Google Colab\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_kana_kanji\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Model directory: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm fugashi unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# MODEL SIZE CONTROL\n",
                "# ===========================================================\n",
                "# Model size is controlled by architecture, NOT by sample count!\n",
                "# - More samples = longer training but BETTER accuracy\n",
                "# - Architecture params = control model size\n",
                "#\n",
                "# Like Zenzai GPT-2: train on ALL data, control size via architecture\n",
                "# Target: Model size under 20MB (FP16 TFLite)\n",
                "# ===========================================================\n",
                "\n",
                "TESTING_MODE = False  # Set True for quick test, False for full training\n",
                "MAX_SAMPLES = None  # Use ALL data! No limit!\n",
                "BATCH_SIZE = 512\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 10\n",
                "else:\n",
                "    NUM_EPOCHS = 20\n",
                "\n",
                "# ===========================================================\n",
                "# ARCHITECTURE CONFIG (these control model size!)\n",
                "# ===========================================================\n",
                "# Adjust these to fit model under 20MB:\n",
                "# - CHAR_VOCAB_SIZE: ~5K covers 99%+ of Japanese\n",
                "# - EMBEDDING_DIM: smaller = smaller model\n",
                "# - GRU_UNITS: smaller = smaller model\n",
                "# ===========================================================\n",
                "\n",
                "CHAR_VOCAB_SIZE = 5000   # 5K kanji/kana covers 99%+ of text\n",
                "MAX_INPUT_LEN = 50       # context + <SEP> + kana\n",
                "MAX_OUTPUT_LEN = 20      # kanji output\n",
                "EMBEDDING_DIM = 64       # ‚Üì smaller = smaller model\n",
                "GRU_UNITS = 128          # ‚Üì smaller = smaller model\n",
                "NUM_ENCODER_LAYERS = 2   # Encoder depth\n",
                "NUM_DECODER_LAYERS = 2   # Decoder depth\n",
                "\n",
                "# Estimate model size (rough calculation)\n",
                "embedding_params = CHAR_VOCAB_SIZE * EMBEDDING_DIM\n",
                "encoder_params = 2 * NUM_ENCODER_LAYERS * 3 * GRU_UNITS * (EMBEDDING_DIM + GRU_UNITS + 1)\n",
                "decoder_params = NUM_DECODER_LAYERS * 3 * (GRU_UNITS * 2) * (EMBEDDING_DIM + GRU_UNITS * 2 + 1)\n",
                "output_params = GRU_UNITS * 4 * CHAR_VOCAB_SIZE  # After concat\n",
                "total_params = embedding_params + encoder_params + decoder_params + output_params\n",
                "estimated_size_mb = total_params * 4 / 1024 / 1024\n",
                "estimated_fp16_mb = estimated_size_mb / 2\n",
                "\n",
                "print('üìä Estimated Model Size:')\n",
                "print(f'   Parameters: ~{total_params:,}')\n",
                "print(f'   FP32: ~{estimated_size_mb:.1f} MB')\n",
                "print(f'   FP16: ~{estimated_fp16_mb:.1f} MB')\n",
                "if estimated_fp16_mb < 20:\n",
                "    print('   ‚úÖ Under 20MB target!')\n",
                "else:\n",
                "    print('   ‚ö†Ô∏è Over 20MB target! Reduce EMBEDDING_DIM or GRU_UNITS')\n",
                "\n",
                "# Special tokens\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<SEP>']\n",
                "SEP_TOKEN = '<SEP>'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load zenz Dataset (ALL DATA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "\n",
                "try:\n",
                "    if MAX_SAMPLES:\n",
                "        dataset = load_dataset(\n",
                "            \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "            data_files=\"train_wikipedia.jsonl\",\n",
                "            split=f\"train[:{MAX_SAMPLES}]\"\n",
                "        )\n",
                "    else:\n",
                "        # Load ALL data!\n",
                "        dataset = load_dataset(\n",
                "            \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "            data_files=\"train_wikipedia.jsonl\",\n",
                "            split=\"train\"\n",
                "        )\n",
                "except:\n",
                "    if MAX_SAMPLES:\n",
                "        dataset = load_dataset(\n",
                "            \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "            split=f\"train[:{MAX_SAMPLES}]\"\n",
                "        )\n",
                "    else:\n",
                "        dataset = load_dataset(\n",
                "            \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "            split=\"train\"\n",
                "        )\n",
                "\n",
                "print(f\"‚úì Loaded {len(dataset):,} samples\")\n",
                "print(f\"  (Using {'ALL DATA' if not MAX_SAMPLES else f'{MAX_SAMPLES:,} samples'})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Japanese Tokenizer (MeCab/fugashi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import fugashi\n",
                "\n",
                "tagger = fugashi.Tagger()\n",
                "\n",
                "def tokenize_japanese(text):\n",
                "    \"\"\"Tokenize Japanese text and get reading (kana) for each word.\"\"\"\n",
                "    words = []\n",
                "    for word in tagger(text):\n",
                "        surface = word.surface\n",
                "        try:\n",
                "            reading = word.feature.kana or word.surface\n",
                "        except:\n",
                "            reading = word.surface\n",
                "        words.append({'surface': surface, 'reading': reading})\n",
                "    return words\n",
                "\n",
                "# Test tokenizer\n",
                "test_text = \"ÂÜôÁúü„ÇíÊíÆ„Çã\"\n",
                "tokens = tokenize_japanese(test_text)\n",
                "print(f\"Test: {test_text}\")\n",
                "for t in tokens:\n",
                "    print(f\"  {t['surface']} ‚Üí {t['reading']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Training Data with Word-Level Alignment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "from tqdm import tqdm\n",
                "\n",
                "def katakana_to_hiragana(text):\n",
                "    \"\"\"Convert katakana to hiragana.\"\"\"\n",
                "    result = []\n",
                "    for char in text:\n",
                "        code = ord(char)\n",
                "        if 0x30A1 <= code <= 0x30F6:\n",
                "            result.append(chr(code - 0x60))\n",
                "        else:\n",
                "            result.append(char)\n",
                "    return ''.join(result)\n",
                "\n",
                "def create_training_examples_from_sentence(kanji_sentence, min_context_words=1, max_target_words=3):\n",
                "    \"\"\"Create training examples by splitting at word boundaries.\"\"\"\n",
                "    examples = []\n",
                "    \n",
                "    tokens = tokenize_japanese(kanji_sentence)\n",
                "    if len(tokens) < 2:\n",
                "        return examples\n",
                "    \n",
                "    for split_idx in range(min_context_words, len(tokens)):\n",
                "        context_words = tokens[:split_idx]\n",
                "        context = ''.join([w['surface'] for w in context_words])\n",
                "        \n",
                "        end_idx = min(split_idx + max_target_words, len(tokens))\n",
                "        target_words = tokens[split_idx:end_idx]\n",
                "        \n",
                "        if not target_words:\n",
                "            continue\n",
                "        \n",
                "        target = ''.join([w['surface'] for w in target_words])\n",
                "        kana = ''.join([w['reading'] for w in target_words])\n",
                "        kana = katakana_to_hiragana(kana)\n",
                "        \n",
                "        if len(context) > 30 or len(kana) > 15 or len(target) > 15:\n",
                "            continue\n",
                "        if len(kana) < 1 or len(target) < 1:\n",
                "            continue\n",
                "        \n",
                "        input_text = f\"{context}{SEP_TOKEN}{kana}\"\n",
                "        examples.append({\n",
                "            'input': input_text,\n",
                "            'output': target,\n",
                "            'context': context,\n",
                "            'kana': kana\n",
                "        })\n",
                "    \n",
                "    return examples\n",
                "\n",
                "print(\"Generating word-aligned training data...\")\n",
                "print(f\"Processing {len(dataset):,} sentences...\")\n",
                "training_data = []\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Processing\"):\n",
                "    kanji_sentence = item['output']\n",
                "    examples = create_training_examples_from_sentence(kanji_sentence)\n",
                "    training_data.extend(examples)\n",
                "\n",
                "print(f\"\\n‚úì Generated {len(training_data):,} training examples\")\n",
                "print(\"\\nSamples:\")\n",
                "for i in range(min(10, len(training_data))):\n",
                "    d = training_data[i]\n",
                "    print(f\"  {d['context']}<SEP>{d['kana']} ‚Üí {d['output']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "char_counts = Counter()\n",
                "for d in tqdm(training_data, desc=\"Counting\"):\n",
                "    input_text = d['input'].replace(SEP_TOKEN, '')\n",
                "    char_counts.update(list(input_text))\n",
                "    char_counts.update(list(d['output']))\n",
                "\n",
                "print(f\"\\nUnique chars: {len(char_counts):,}\")\n",
                "\n",
                "char_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "    char_to_idx[char] = len(char_to_idx)\n",
                "\n",
                "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
                "vocab_size = len(char_to_idx)\n",
                "print(f\"Vocab size: {vocab_size}\")\n",
                "print(f\"<SEP> index: {char_to_idx[SEP_TOKEN]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create Training Tensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "\n",
                "PAD_IDX = char_to_idx['<PAD>']\n",
                "BOS_IDX = char_to_idx['<BOS>']\n",
                "EOS_IDX = char_to_idx['<EOS>']\n",
                "UNK_IDX = char_to_idx['<UNK>']\n",
                "SEP_IDX = char_to_idx['<SEP>']\n",
                "\n",
                "def encode_input(text, max_len):\n",
                "    tokens = []\n",
                "    i = 0\n",
                "    while i < len(text):\n",
                "        if text[i:i+5] == SEP_TOKEN:\n",
                "            tokens.append(SEP_TOKEN)\n",
                "            i += 5\n",
                "        else:\n",
                "            tokens.append(text[i])\n",
                "            i += 1\n",
                "    ids = [char_to_idx.get(t, UNK_IDX) for t in tokens]\n",
                "    while len(ids) < max_len:\n",
                "        ids.append(PAD_IDX)\n",
                "    return ids[:max_len]\n",
                "\n",
                "def encode_output(text, max_len, add_bos=False, add_eos=False):\n",
                "    tokens = list(text)\n",
                "    if add_bos:\n",
                "        tokens = ['<BOS>'] + tokens\n",
                "    if add_eos:\n",
                "        tokens = tokens + ['<EOS>']\n",
                "    ids = [char_to_idx.get(c, UNK_IDX) for c in tokens]\n",
                "    while len(ids) < max_len:\n",
                "        ids.append(PAD_IDX)\n",
                "    return ids[:max_len]\n",
                "\n",
                "encoder_inputs = []\n",
                "decoder_inputs = []\n",
                "decoder_targets = []\n",
                "\n",
                "for d in tqdm(training_data, desc=\"Encoding\"):\n",
                "    encoder_inputs.append(encode_input(d['input'], MAX_INPUT_LEN))\n",
                "    decoder_inputs.append(encode_output(d['output'], MAX_OUTPUT_LEN, add_bos=True))\n",
                "    decoder_targets.append(encode_output(d['output'], MAX_OUTPUT_LEN, add_eos=True))\n",
                "\n",
                "encoder_inputs = np.array(encoder_inputs, dtype=np.int32)\n",
                "decoder_inputs = np.array(decoder_inputs, dtype=np.int32)\n",
                "decoder_targets = np.array(decoder_targets, dtype=np.int32)\n",
                "\n",
                "print(f\"\\nShapes: {encoder_inputs.shape}, {decoder_inputs.shape}, {decoder_targets.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "idx = np.random.permutation(len(encoder_inputs))\n",
                "encoder_inputs = encoder_inputs[idx]\n",
                "decoder_inputs = decoder_inputs[idx]\n",
                "decoder_targets = decoder_targets[idx]\n",
                "\n",
                "split = int(len(encoder_inputs) * 0.9)\n",
                "\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[:split], 'decoder_input': decoder_inputs[:split]},\n",
                "    decoder_targets[:split]\n",
                ")).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[split:], 'decoder_input': decoder_inputs[split:]},\n",
                "    decoder_targets[split:]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"Train: {split:,}, Val: {len(encoder_inputs)-split:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Build Model (Size Controlled by Architecture)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "embedding = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')\n",
                "\n",
                "# Encoder\n",
                "encoder_input = Input(shape=(MAX_INPUT_LEN,), dtype='int32', name='encoder_input')\n",
                "enc_emb = embedding(encoder_input)\n",
                "enc_emb = Dropout(0.1)(enc_emb)\n",
                "\n",
                "encoder_out = enc_emb\n",
                "for i in range(NUM_ENCODER_LAYERS):\n",
                "    encoder_out = Bidirectional(\n",
                "        GRU(GRU_UNITS, return_sequences=True),\n",
                "        name=f'encoder_{i+1}'\n",
                "    )(encoder_out)\n",
                "    encoder_out = LayerNormalization()(encoder_out)\n",
                "\n",
                "# Decoder\n",
                "decoder_input = Input(shape=(MAX_OUTPUT_LEN,), dtype='int32', name='decoder_input')\n",
                "dec_emb = embedding(decoder_input)\n",
                "dec_emb = Dropout(0.1)(dec_emb)\n",
                "\n",
                "decoder_out = dec_emb\n",
                "for i in range(NUM_DECODER_LAYERS):\n",
                "    decoder_out = GRU(GRU_UNITS * 2, return_sequences=True, name=f'decoder_{i+1}')(decoder_out)\n",
                "    decoder_out = LayerNormalization()(decoder_out)\n",
                "\n",
                "# Attention\n",
                "context = Attention(use_scale=True, name='attention')([decoder_out, encoder_out])\n",
                "\n",
                "# Combine and output\n",
                "combined = Concatenate()([decoder_out, context])\n",
                "combined = LayerNormalization()(combined)\n",
                "combined = Dropout(0.2)(combined)\n",
                "combined = Dense(GRU_UNITS * 2, activation='relu')(combined)\n",
                "output = Dense(vocab_size, activation='softmax', name='output')(combined)\n",
                "\n",
                "model = Model(\n",
                "    inputs=[encoder_input, decoder_input],\n",
                "    outputs=output,\n",
                "    name='context_kana_kanji_v2'\n",
                ")\n",
                "\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()\n",
                "print(f\"\\nüìä Actual Model Stats:\")\n",
                "print(f\"   Parameters: {model.count_params():,}\")\n",
                "print(f\"   Size FP32: ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
                "print(f\"   Size FP16: ~{model.count_params() * 2 / 1024 / 1024:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
                "]\n",
                "\n",
                "history = model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=val_ds, callbacks=callbacks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "print(f\"Best val_accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/char_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(char_to_idx, f, ensure_ascii=False)\n",
                "with open(f'{MODEL_DIR}/idx_to_char.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_char.items()}, f, ensure_ascii=False)\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        'vocab_size': vocab_size,\n",
                "        'max_input_len': MAX_INPUT_LEN,\n",
                "        'max_output_len': MAX_OUTPUT_LEN,\n",
                "        'sep_token': SEP_TOKEN,\n",
                "        'sep_idx': SEP_IDX\n",
                "    }, f)\n",
                "\n",
                "print(\"‚úì Saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"‚úì model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite_fp16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite_fp16)/(1024*1024):.2f}MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö† {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Context-Aware Kana-Kanji Conversion\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nInput format: context{SEP_TOKEN}kana\")\n",
                "\n",
                "def convert(context, kana, max_len=20):\n",
                "    input_text = f\"{context}{SEP_TOKEN}{kana}\"\n",
                "    enc_in = np.array([encode_input(input_text, MAX_INPUT_LEN)], dtype=np.int32)\n",
                "    dec_in = np.zeros((1, MAX_OUTPUT_LEN), dtype=np.int32)\n",
                "    dec_in[0, 0] = BOS_IDX\n",
                "    \n",
                "    result = []\n",
                "    for i in range(max_len):\n",
                "        preds = model.predict({'encoder_input': enc_in, 'decoder_input': dec_in}, verbose=0)\n",
                "        next_idx = int(np.argmax(preds[0, i]))\n",
                "        if next_idx == EOS_IDX:\n",
                "            break\n",
                "        if next_idx not in [PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX, SEP_IDX]:\n",
                "            result.append(idx_to_char.get(next_idx, ''))\n",
                "        if i + 1 < MAX_OUTPUT_LEN:\n",
                "            dec_in[0, i + 1] = next_idx\n",
                "    return ''.join(result)\n",
                "\n",
                "# All test cases from user\n",
                "test_cases = [\n",
                "    {\"context\": \"‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇ\", \"kana\": \"„ÅÇ„Å§„ÅÑ\", \"expected\": \"Êöë„ÅÑ\", \"desc\": \"Weather hot\"},\n",
                "    {\"context\": \"„ÅäËå∂„Åå\", \"kana\": \"„ÅÇ„Å§„ÅÑ\", \"expected\": \"ÁÜ±„ÅÑ\", \"desc\": \"Temperature hot\"},\n",
                "    {\"context\": \"„Åì„ÅÆËæûÂÖ∏„ÅØ\", \"kana\": \"„ÅÇ„Å§„ÅÑ\", \"expected\": \"Âéö„ÅÑ\", \"desc\": \"Thick\"},\n",
                "    {\"context\": \"ÊØéÊúùËµ∑„Åç„Çã„ÅÆ„Åå\", \"kana\": \"„ÅØ„ÇÑ„ÅÑ\", \"expected\": \"Êó©„ÅÑ\", \"desc\": \"Early\"},\n",
                "    {\"context\": \"ÂΩº„ÅØËµ∞„Çã„ÅÆ„Åå\", \"kana\": \"„ÅØ„ÇÑ„ÅÑ\", \"expected\": \"ÈÄü„ÅÑ\", \"desc\": \"Fast\"},\n",
                "    {\"context\": \"Â∑ù„Å´\", \"kana\": \"„ÅØ„Åó\", \"expected\": \"Ê©ã\", \"desc\": \"Bridge\"},\n",
                "    {\"context\": \"„ÅîÈ£Ø„Çí\", \"kana\": \"„ÅØ„Åó\", \"expected\": \"ÁÆ∏\", \"desc\": \"Chopsticks\"},\n",
                "    {\"context\": \"ÈÅì„ÅÆ\", \"kana\": \"„ÅØ„Åó\", \"expected\": \"Á´Ø\", \"desc\": \"Edge\"},\n",
                "    {\"context\": \"Èü≥Ê•Ω„Çí\", \"kana\": \"„Åç„Åè\", \"expected\": \"ËÅ¥„Åè\", \"desc\": \"Listen\"},\n",
                "    {\"context\": \"ÈÅì„Çí\", \"kana\": \"„Åç„Åè\", \"expected\": \"ËÅû„Åè\", \"desc\": \"Ask\"},\n",
                "    {\"context\": \"„Åì„ÅÆËñ¨„ÅØ„Çà„Åè\", \"kana\": \"„Åç„Åè\", \"expected\": \"Âäπ„Åè\", \"desc\": \"Effective\"},\n",
                "    {\"context\": \"Á®éÈáë„Çí\", \"kana\": \"„Åä„Åï„ÇÅ„Çã\", \"expected\": \"Á¥ç„ÇÅ„Çã\", \"desc\": \"Pay\"},\n",
                "    {\"context\": \"ÂõΩ„Çí\", \"kana\": \"„Åä„Åï„ÇÅ„Çã\", \"expected\": \"Ê≤ª„ÇÅ„Çã\", \"desc\": \"Govern\"},\n",
                "    {\"context\": \"Â≠¶Âïè„Çí\", \"kana\": \"„Åä„Åï„ÇÅ„Çã\", \"expected\": \"‰øÆ„ÇÅ„Çã\", \"desc\": \"Master\"},\n",
                "    {\"context\": \"ÂèãÈÅî„Å´\", \"kana\": \"„ÅÇ„ÅÜ\", \"expected\": \"‰ºö„ÅÜ\", \"desc\": \"Meet\"},\n",
                "    {\"context\": \"„Çµ„Ç§„Ç∫„Åå\", \"kana\": \"„ÅÇ„ÅÜ\", \"expected\": \"Âêà„ÅÜ\", \"desc\": \"Fit\"},\n",
                "    {\"context\": \"‰∫ãÊïÖ„Å´\", \"kana\": \"„ÅÇ„ÅÜ\", \"expected\": \"ÈÅ≠„ÅÜ\", \"desc\": \"Encounter\"},\n",
                "    {\"context\": \"ÂÜôÁúü„Çí\", \"kana\": \"„Å®„Çã\", \"expected\": \"ÊíÆ„Çã\", \"desc\": \"Take photo\"},\n",
                "    {\"context\": \"Â°©„Çí\", \"kana\": \"„Å®„Çã\", \"expected\": \"Âèñ„Çã\", \"desc\": \"Take\"},\n",
                "    {\"context\": \"È≠ö„Çí\", \"kana\": \"„Å®„Çã\", \"expected\": \"Êçï„Çã\", \"desc\": \"Catch\"},\n",
                "    {\"context\": \"Áü≥„ÅØ\", \"kana\": \"„Åã„Åü„ÅÑ\", \"expected\": \"Á°¨„ÅÑ\", \"desc\": \"Hard solid\"},\n",
                "    {\"context\": \"Ê±∫ÊÑè„Åå\", \"kana\": \"„Åã„Åü„ÅÑ\", \"expected\": \"Âõ∫„ÅÑ\", \"desc\": \"Hard firm\"},\n",
                "    {\"context\": \"Êú¨„ÅÆÂÜÖÂÆπ„Åå\", \"kana\": \"„Åã„Åü„ÅÑ\", \"expected\": \"Â†Ö„ÅÑ\", \"desc\": \"Hard strict\"},\n",
                "    {\"context\": \"„ÉÜ„Çπ„Éà\", \"kana\": \"„Åç„Åã„Çì\", \"expected\": \"ÊúüÈñì\", \"desc\": \"Period\"},\n",
                "    {\"context\": \"‰∫§ÈÄö\", \"kana\": \"„Åç„Åã„Çì\", \"expected\": \"Ê©üÈñ¢\", \"desc\": \"Institution\"},\n",
                "    {\"context\": \"ÂÆáÂÆô„Åã„Çâ\", \"kana\": \"„Åç„Åã„Çì\", \"expected\": \"Â∏∞ÈÇÑ\", \"desc\": \"Return\"},\n",
                "]\n",
                "\n",
                "print(\"\\nüìù Homophone Disambiguation Test:\")\n",
                "print(\"-\" * 60)\n",
                "correct = 0\n",
                "for tc in test_cases:\n",
                "    result = convert(tc['context'], tc['kana'])\n",
                "    match = result == tc['expected'] or tc['expected'] in result or result in tc['expected']\n",
                "    if match:\n",
                "        correct += 1\n",
                "    status = \"‚úì\" if match else \"‚úó\"\n",
                "    print(f\"{status} [{tc['desc']}]\")\n",
                "    print(f\"   {tc['context']}<SEP>{tc['kana']} ‚Üí {result} (expected: {tc['expected']})\")\n",
                "\n",
                "print(f\"\\n‚úÖ Score: {correct}/{len(test_cases)} ({correct/len(test_cases)*100:.0f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nüì¶ Exported Files ({PLATFORM}):\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    path = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(path):\n",
                "        size = os.path.getsize(path)\n",
                "        if size > 1024*1024:\n",
                "            print(f\"  {f}: {size/(1024*1024):.2f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {size/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}