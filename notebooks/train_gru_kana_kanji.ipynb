{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model 1: Kana-Kanji Converter (Seq2Seq)\n",
                "\n",
                "**Task:** Convert katakana to kanji\n",
                "- Input: `„Ç™„Çª„ÉØ`\n",
                "- Output: `„Åä‰∏ñË©±`\n",
                "\n",
                "**Architecture:** Bi-GRU Encoder + Luong Attention + GRU Decoder\n",
                "\n",
                "**Target:** ~2MB, 90%+ accuracy, <5ms inference"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_kana_kanji\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"‚úì Model: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 4\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 250000\n",
                "else:\n",
                "    NUM_EPOCHS = 30\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 500000\n",
                "\n",
                "# Model specs (optimized for size + accuracy)\n",
                "CHAR_VOCAB_SIZE = 3000\n",
                "MAX_SEQ_LENGTH = 30\n",
                "EMBEDDING_DIM = 64\n",
                "ENCODER_UNITS = 128\n",
                "DECODER_UNITS = 128\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, samples={MAX_SAMPLES:,}\")\n",
                "print(f\"Model: Embed={EMBEDDING_DIM}, Encoder={ENCODER_UNITS}, Decoder={DECODER_UNITS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "except:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "\n",
                "print(f\"‚úì Loaded {len(dataset):,} samples\")\n",
                "print(f\"Sample: {dataset[0]['input'][:15]} ‚Üí {dataset[0]['output'][:15]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Character Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"Building character vocabulary...\")\n",
                "\n",
                "char_counts = Counter()\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Counting chars\"):\n",
                "    kana = item.get('input', '')\n",
                "    kanji = item.get('output', '')\n",
                "    char_counts.update(list(kana))\n",
                "    char_counts.update(list(kanji))\n",
                "\n",
                "print(f\"‚úì Found {len(char_counts):,} unique chars\")\n",
                "\n",
                "# Build vocab\n",
                "char_to_idx = {}\n",
                "for i, tok in enumerate(SPECIAL_TOKENS):\n",
                "    char_to_idx[tok] = i\n",
                "\n",
                "for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "    char_to_idx[char] = len(char_to_idx)\n",
                "\n",
                "idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
                "vocab_size = len(char_to_idx)\n",
                "\n",
                "print(f\"‚úì Vocab size: {vocab_size:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "print(\"Creating training data...\")\n",
                "\n",
                "def encode_seq(text, max_len, add_bos=False, add_eos=False):\n",
                "    tokens = list(text)\n",
                "    if add_bos:\n",
                "        tokens = ['<BOS>'] + tokens\n",
                "    if add_eos:\n",
                "        tokens = tokens + ['<EOS>']\n",
                "    \n",
                "    ids = [char_to_idx.get(c, char_to_idx['<UNK>']) for c in tokens]\n",
                "    if len(ids) < max_len:\n",
                "        ids = ids + [char_to_idx['<PAD>']] * (max_len - len(ids))\n",
                "    return ids[:max_len]\n",
                "\n",
                "encoder_inputs = []\n",
                "decoder_inputs = []\n",
                "decoder_targets = []\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Processing\"):\n",
                "    kana = item.get('input', '').strip()\n",
                "    kanji = item.get('output', '').strip()\n",
                "    \n",
                "    if not kana or not kanji:\n",
                "        continue\n",
                "    if len(kana) > MAX_SEQ_LENGTH - 2 or len(kanji) > MAX_SEQ_LENGTH - 2:\n",
                "        continue\n",
                "    \n",
                "    # Encoder: kana input\n",
                "    enc_in = encode_seq(kana, MAX_SEQ_LENGTH)\n",
                "    \n",
                "    # Decoder input: <BOS> + kanji\n",
                "    dec_in = encode_seq(kanji, MAX_SEQ_LENGTH, add_bos=True)\n",
                "    \n",
                "    # Decoder target: kanji + <EOS>\n",
                "    dec_out = encode_seq(kanji, MAX_SEQ_LENGTH, add_eos=True)\n",
                "    \n",
                "    encoder_inputs.append(enc_in)\n",
                "    decoder_inputs.append(dec_in)\n",
                "    decoder_targets.append(dec_out)\n",
                "\n",
                "encoder_inputs = np.array(encoder_inputs)\n",
                "decoder_inputs = np.array(decoder_inputs)\n",
                "decoder_targets = np.array(decoder_targets)\n",
                "\n",
                "print(f\"\\n‚úì {len(encoder_inputs):,} training pairs\")\n",
                "print(f\"‚úì Encoder shape: {encoder_inputs.shape}\")\n",
                "print(f\"‚úì Decoder shape: {decoder_inputs.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Shuffle and split\n",
                "indices = np.random.permutation(len(encoder_inputs))\n",
                "encoder_inputs = encoder_inputs[indices]\n",
                "decoder_inputs = decoder_inputs[indices]\n",
                "decoder_targets = decoder_targets[indices]\n",
                "\n",
                "split = int(len(encoder_inputs) * 0.9)\n",
                "\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[:split], 'decoder_input': decoder_inputs[:split]},\n",
                "    decoder_targets[:split]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[split:], 'decoder_input': decoder_inputs[split:]},\n",
                "    decoder_targets[split:]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"‚úì Train: {split:,}, Val: {len(encoder_inputs)-split:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build Seq2Seq Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import mixed_precision\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "print(\"Building Seq2Seq with Luong Attention...\")\n",
                "\n",
                "# Shared embedding\n",
                "embedding = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')\n",
                "\n",
                "# ============================================================\n",
                "# Encoder\n",
                "# ============================================================\n",
                "encoder_input = Input(shape=(MAX_SEQ_LENGTH,), name='encoder_input')\n",
                "encoder_embed = embedding(encoder_input)\n",
                "\n",
                "encoder_gru = Bidirectional(\n",
                "    GRU(ENCODER_UNITS, return_sequences=True, return_state=True, dropout=0.2),\n",
                "    name='encoder'\n",
                ")\n",
                "encoder_outputs, forward_h, backward_h = encoder_gru(encoder_embed)\n",
                "encoder_state = Concatenate()([forward_h, backward_h])\n",
                "\n",
                "# ============================================================\n",
                "# Decoder\n",
                "# ============================================================\n",
                "decoder_input = Input(shape=(MAX_SEQ_LENGTH,), name='decoder_input')\n",
                "decoder_embed = embedding(decoder_input)\n",
                "\n",
                "decoder_gru = GRU(\n",
                "    ENCODER_UNITS * 2,  # Match bidirectional output\n",
                "    return_sequences=True,\n",
                "    dropout=0.2,\n",
                "    name='decoder'\n",
                ")\n",
                "decoder_outputs = decoder_gru(decoder_embed, initial_state=encoder_state)\n",
                "\n",
                "# ============================================================\n",
                "# Luong Attention\n",
                "# ============================================================\n",
                "attention = Attention(use_scale=True, name='attention')\n",
                "context = attention([decoder_outputs, encoder_outputs])\n",
                "\n",
                "# Combine\n",
                "combined = Concatenate()([decoder_outputs, context])\n",
                "combined = LayerNormalization()(combined)\n",
                "combined = Dropout(0.3)(combined)\n",
                "\n",
                "# Output\n",
                "output = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(combined)\n",
                "\n",
                "model = Model(\n",
                "    inputs=[encoder_input, decoder_input],\n",
                "    outputs=output,\n",
                "    name='kana_kanji_seq2seq'\n",
                ")\n",
                "\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()\n",
                "print(f\"\\n‚úì Parameters: {model.count_params():,}\")\n",
                "print(f\"‚úì Estimated size: {model.count_params() * 4 / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n‚úì Final Val Accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/char_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(char_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_char.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_char.items()}, f, ensure_ascii=False)\n",
                "\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'max_seq_length': MAX_SEQ_LENGTH,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'encoder_units': ENCODER_UNITS,\n",
                "    'decoder_units': DECODER_UNITS,\n",
                "    'architecture': 'Seq2Seq_BiGRU_LuongAttention',\n",
                "    'task': 'kana_to_kanji',\n",
                "    'special_tokens': SPECIAL_TOKENS\n",
                "}\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"‚úì Saved model and config\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export TFLite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Exporting TFLite...\")\n",
                "\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"‚úì model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # FP16\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite_fp16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite_fp16)/(1024*1024):.2f}MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö† Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Kana ‚Üí Kanji Conversion\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def convert_kana_to_kanji(kana_text, max_len=30):\n",
                "    \"\"\"Convert katakana to kanji using beam search.\"\"\"\n",
                "    # Encode input\n",
                "    enc_input = np.array([encode_seq(kana_text, MAX_SEQ_LENGTH)])\n",
                "    \n",
                "    # Start with <BOS>\n",
                "    dec_input = np.zeros((1, MAX_SEQ_LENGTH), dtype=np.int32)\n",
                "    dec_input[0, 0] = char_to_idx['<BOS>']\n",
                "    \n",
                "    result = []\n",
                "    for i in range(max_len):\n",
                "        predictions = model.predict(\n",
                "            {'encoder_input': enc_input, 'decoder_input': dec_input},\n",
                "            verbose=0\n",
                "        )\n",
                "        \n",
                "        # Get next char\n",
                "        next_idx = np.argmax(predictions[0, i])\n",
                "        next_char = idx_to_char.get(next_idx, '<UNK>')\n",
                "        \n",
                "        if next_char == '<EOS>':\n",
                "            break\n",
                "        if next_char not in SPECIAL_TOKENS:\n",
                "            result.append(next_char)\n",
                "        \n",
                "        # Update decoder input\n",
                "        if i + 1 < MAX_SEQ_LENGTH:\n",
                "            dec_input[0, i + 1] = next_idx\n",
                "    \n",
                "    return ''.join(result)\n",
                "\n",
                "# Test cases\n",
                "tests = [\n",
                "    '„Ç¢„É™„Ç¨„Éà„Ç¶',      # ‚Üí ÊúâÈõ£„ÅÜ\n",
                "    '„Ç¥„Ç∂„Ç§„Éû„Çπ',      # ‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô\n",
                "    '„Ç™„Çª„ÉØ',          # ‚Üí „Åä‰∏ñË©±\n",
                "    '„Ç∑„É≥„Ç∏„É•„ÇØ',      # ‚Üí Êñ∞ÂÆø\n",
                "    '„Éà„Ç¶„Ç≠„Éß„Ç¶',      # ‚Üí Êù±‰∫¨\n",
                "    '„Éã„Éõ„É≥',          # ‚Üí Êó•Êú¨\n",
                "    '„Ç≥„É≥„Éã„ÉÅ„Éè',      # ‚Üí „Åì„Çì„Å´„Å°„ÅØ\n",
                "]\n",
                "\n",
                "print(\"\\nüìù Conversion Results:\")\n",
                "print(\"-\" * 40)\n",
                "for kana in tests:\n",
                "    result = convert_kana_to_kanji(kana)\n",
                "    print(f\"  {kana} ‚Üí {result}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List exports\n",
                "print(\"\\nExported files:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    path = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(path):\n",
                "        size = os.path.getsize(path)\n",
                "        if size > 1024*1024:\n",
                "            print(f\"  {f}: {size/(1024*1024):.1f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {size/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
