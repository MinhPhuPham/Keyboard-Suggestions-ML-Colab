{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Context-Aware Kana-Kanji Converter (v3.1)\n",
                "\n",
                "**Supports**: Google Colab & Kaggle (Multi-GPU)\n",
                "\n",
                "**Input Format**: `context<SEP>kana` ‚Üí `kanji`\n",
                "\n",
                "**Testing workflow**:\n",
                "1. Set `TESTING_MODE = True` ‚Üí 10K samples, 3 epochs\n",
                "2. Train ‚Üí check loss decreasing, accuracy improving\n",
                "3. Verify with real test cases from dataset\n",
                "4. Set `TESTING_MODE = False` ‚Üí full training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "\n",
                "# Auto-detect platform (Colab check first - Colab also has /kaggle dir!)\n",
                "if 'COLAB_RELEASE_TAG' in os.environ:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "elif os.path.exists('/kaggle/working'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Local'\n",
                "    DRIVE_DIR = './output'\n",
                "\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_kana_kanji\"\n",
                "CACHE_DIR = f\"{DRIVE_DIR}/cache/kana_kanji\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Model: {MODEL_DIR}\")\n",
                "print(f\"üíæ Cache: {CACHE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# ===========================================================\n",
                "# MULTI-GPU + MIXED PRECISION\n",
                "# ===========================================================\n",
                "strategy = tf.distribute.MirroredStrategy()\n",
                "NUM_GPUS = strategy.num_replicas_in_sync\n",
                "print(f\"üî• GPUs available: {NUM_GPUS}\")\n",
                "\n",
                "# Mixed precision: T4 has good FP16 Tensor Cores\n",
                "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
                "print(f\"‚ö° Mixed precision: {tf.keras.mixed_precision.global_policy().name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "# ‚ö†Ô∏è Set True for quick logic test (10K samples, 3 epochs)\n",
                "# Set False for full training (8M samples, 10 epochs)\n",
                "TESTING_MODE = True\n",
                "\n",
                "if TESTING_MODE:\n",
                "    MAX_SAMPLES = 10_000\n",
                "    NUM_EPOCHS = 3\n",
                "    CACHE_SUFFIX = '_test'\n",
                "    print(\"‚ö†Ô∏è TESTING MODE: 10K samples, 3 epochs\")\n",
                "else:\n",
                "    MAX_SAMPLES = 8_000_000\n",
                "    NUM_EPOCHS = 10\n",
                "    CACHE_SUFFIX = ''\n",
                "    print(\"üöÄ FULL TRAINING: 8M samples, 10 epochs\")\n",
                "\n",
                "BATCH_SIZE = 512 * NUM_GPUS  # Scale batch with GPUs (512 per GPU)\n",
                "FORCE_REBUILD_CACHE = False\n",
                "\n",
                "# Length limits (filter long sequences)\n",
                "MAX_CONTEXT_LEN = 30   # left_context max chars\n",
                "MAX_INPUT_LEN = 30     # kana input max chars\n",
                "MAX_OUTPUT_LEN = 30    # kanji output max chars\n",
                "MAX_ENCODER_LEN = MAX_CONTEXT_LEN + 1 + MAX_INPUT_LEN  # context + <SEP>(1 token) + input\n",
                "MAX_DECODER_LEN = MAX_OUTPUT_LEN + 2  # BOS + content + EOS\n",
                "\n",
                "# Architecture (controls model size)\n",
                "CHAR_VOCAB_SIZE = 6000\n",
                "EMBEDDING_DIM = 64\n",
                "GRU_UNITS = 128\n",
                "NUM_ENCODER_LAYERS = 2\n",
                "NUM_DECODER_LAYERS = 2\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<SEP>']\n",
                "PAD_IDX = 0  # <PAD> is always index 0\n",
                "SEP_TOKEN = '<SEP>'\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, batch={BATCH_SIZE} ({BATCH_SIZE//NUM_GPUS}/GPU)\")\n",
                "print(f\"Encoder max: {MAX_ENCODER_LEN}, Decoder max: {MAX_DECODER_LEN}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Shared Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tokenize_with_sep(text):\n",
                "    \"\"\"Tokenize text handling <SEP> as single token.\"\"\"\n",
                "    tokens = []\n",
                "    i = 0\n",
                "    while i < len(text):\n",
                "        if text[i:i+5] == '<SEP>':\n",
                "            tokens.append('<SEP>')\n",
                "            i += 5\n",
                "        else:\n",
                "            tokens.append(text[i])\n",
                "            i += 1\n",
                "    return tokens\n",
                "\n",
                "def encode_tokens(tokens, vocab, max_len, pad_id, unk_id):\n",
                "    \"\"\"Encode token list to padded integer IDs.\"\"\"\n",
                "    ids = [vocab.get(t, unk_id) for t in tokens][:max_len]\n",
                "    return ids + [pad_id] * (max_len - len(ids))\n",
                "\n",
                "def encode_encoder_input(text, vocab, pad_id, unk_id):\n",
                "    \"\"\"Encode encoder input (context<SEP>kana).\"\"\"\n",
                "    tokens = tokenize_with_sep(text)\n",
                "    return encode_tokens(tokens, vocab, MAX_ENCODER_LEN, pad_id, unk_id)\n",
                "\n",
                "def encode_decoder_seq(text, vocab, pad_id, unk_id, add_bos=False, add_eos=False):\n",
                "    \"\"\"Encode decoder sequence with optional BOS/EOS.\"\"\"\n",
                "    tokens = []\n",
                "    if add_bos:\n",
                "        tokens.append('<BOS>')\n",
                "    tokens.extend(list(text))\n",
                "    if add_eos:\n",
                "        tokens.append('<EOS>')\n",
                "    return encode_tokens(tokens, vocab, MAX_DECODER_LEN, pad_id, unk_id)\n",
                "\n",
                "print(\"‚úì Shared utilities loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load or Build Cached Data\n",
                "\n",
                "Testing mode uses separate cache files so it won't overwrite full cache."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Cache file paths ‚Äî separate for test vs full\n",
                "VOCAB_CACHE = f\"{CACHE_DIR}/vocabulary_v3{CACHE_SUFFIX}.json\"\n",
                "ENC_CACHE = f\"{CACHE_DIR}/enc_v3_1{CACHE_SUFFIX}.npy\"\n",
                "DEC_IN_CACHE = f\"{CACHE_DIR}/dec_in_v3_1{CACHE_SUFFIX}.npy\"\n",
                "DEC_TGT_CACHE = f\"{CACHE_DIR}/dec_tgt_v3_1{CACHE_SUFFIX}.npy\"\n",
                "\n",
                "def cache_exists():\n",
                "    return all(os.path.exists(f) for f in [VOCAB_CACHE, ENC_CACHE, DEC_IN_CACHE, DEC_TGT_CACHE])\n",
                "\n",
                "if cache_exists() and not FORCE_REBUILD_CACHE:\n",
                "    print(\"üì¶ Loading from cache (memory-mapped, near-zero RAM)...\")\n",
                "    \n",
                "    with open(VOCAB_CACHE, 'r', encoding='utf-8') as f:\n",
                "        vocab_data = json.load(f)\n",
                "    char_to_idx = vocab_data['char_to_idx']\n",
                "    idx_to_char = {int(k): v for k, v in vocab_data['idx_to_char'].items()}\n",
                "    vocab_size = len(char_to_idx)\n",
                "    \n",
                "    enc_mmap = np.load(ENC_CACHE, mmap_mode='r')\n",
                "    dec_in_mmap = np.load(DEC_IN_CACHE, mmap_mode='r')\n",
                "    dec_tgt_mmap = np.load(DEC_TGT_CACHE, mmap_mode='r')\n",
                "    \n",
                "    print(f\"‚úì Loaded {len(enc_mmap):,} samples (memory-mapped)\")\n",
                "    CACHE_LOADED = True\n",
                "else:\n",
                "    print(\"üî® Building from scratch (will save to drive for next time)...\")\n",
                "    CACHE_LOADED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset directly - no tokenization needed!\n",
                "if not CACHE_LOADED:\n",
                "    from datasets import load_dataset\n",
                "    \n",
                "    print(\"üì• Loading zenz dataset...\")\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=\"train\"\n",
                "    )\n",
                "    print(f\"‚úì Raw dataset: {len(dataset):,} items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter and prepare training data\n",
                "if not CACHE_LOADED:\n",
                "    print(f\"\\nüîç Filtering data (limit: {MAX_SAMPLES:,} items)...\")\n",
                "    \n",
                "    training_data = []\n",
                "    raw_samples = []  # Save for inspection\n",
                "    skipped = {'too_long': 0, 'empty': 0}\n",
                "    \n",
                "    for item in tqdm(dataset, desc=\"Processing\"):\n",
                "        kana_input = item.get('input', '') or ''\n",
                "        kanji_output = item.get('output', '') or ''\n",
                "        left_context = item.get('left_context', '') or ''\n",
                "        \n",
                "        if not kana_input or not kanji_output:\n",
                "            skipped['empty'] += 1\n",
                "            continue\n",
                "        \n",
                "        if (len(left_context) > MAX_CONTEXT_LEN or \n",
                "            len(kana_input) > MAX_INPUT_LEN or \n",
                "            len(kanji_output) > MAX_OUTPUT_LEN):\n",
                "            skipped['too_long'] += 1\n",
                "            continue\n",
                "        \n",
                "        encoder_text = f\"{left_context}<SEP>{kana_input}\"\n",
                "        training_data.append({\n",
                "            'input': encoder_text,\n",
                "            'output': kanji_output,\n",
                "            'input_len': len(kana_input)\n",
                "        })\n",
                "        \n",
                "        # Save raw samples for inspection (first 100)\n",
                "        if len(raw_samples) < 100:\n",
                "            raw_samples.append({\n",
                "                'left_context': left_context,\n",
                "                'kana_input': kana_input,\n",
                "                'kanji_output': kanji_output,\n",
                "                'encoder_text': encoder_text\n",
                "            })\n",
                "        \n",
                "        if MAX_SAMPLES and len(training_data) >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    print(f\"\\n‚úì Valid examples: {len(training_data):,}\")\n",
                "    print(f\"  Skipped (too long): {skipped['too_long']:,}\")\n",
                "    print(f\"  Skipped (empty): {skipped['empty']:,}\")\n",
                "    \n",
                "    # üíæ Save raw samples for inspection\n",
                "    SAMPLES_FILE = f\"{CACHE_DIR}/raw_samples_kkc{CACHE_SUFFIX}.json\"\n",
                "    with open(SAMPLES_FILE, 'w', encoding='utf-8') as f:\n",
                "        json.dump(raw_samples, f, ensure_ascii=False, indent=2)\n",
                "    print(f\"üíæ Saved {len(raw_samples)} raw samples ‚Üí {SAMPLES_FILE}\")\n",
                "    \n",
                "    # Show a few examples\n",
                "    print(\"\\nüìù Sample data:\")\n",
                "    for s in raw_samples[:5]:\n",
                "        print(f\"  ctx: {s['left_context'][:20]}... | {s['kana_input']} ‚Üí {s['kanji_output']}\")\n",
                "    \n",
                "    # üßπ Release HuggingFace dataset\n",
                "    del dataset, skipped, raw_samples\n",
                "    gc.collect()\n",
                "    print(\"üßπ Released dataset from memory\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sort by length for bucketing (helps GRU training stability)\n",
                "if not CACHE_LOADED:\n",
                "    print(\"\\nüìä Sorting by length (bucketing)...\")\n",
                "    training_data.sort(key=lambda x: x['input_len'])\n",
                "    \n",
                "    lengths = [d['input_len'] for d in training_data]\n",
                "    print(f\"  Short (0-10): {sum(1 for l in lengths if l <= 10):,}\")\n",
                "    print(f\"  Medium (11-20): {sum(1 for l in lengths if 10 < l <= 20):,}\")\n",
                "    print(f\"  Long (21+): {sum(1 for l in lengths if l > 20):,}\")\n",
                "    del lengths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build vocabulary\n",
                "if not CACHE_LOADED:\n",
                "    from collections import Counter\n",
                "    \n",
                "    print(\"\\nüìù Building vocabulary...\")\n",
                "    char_counts = Counter()\n",
                "    \n",
                "    for d in tqdm(training_data, desc=\"Counting chars\"):\n",
                "        text = d['input'].replace('<SEP>', '') + d['output']\n",
                "        char_counts.update(list(text))\n",
                "    \n",
                "    char_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "    for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "        char_to_idx[char] = len(char_to_idx)\n",
                "    \n",
                "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
                "    vocab_size = len(char_to_idx)\n",
                "    print(f\"‚úì Vocab size: {vocab_size}\")\n",
                "    \n",
                "    with open(VOCAB_CACHE, 'w', encoding='utf-8') as f:\n",
                "        json.dump({\n",
                "            'char_to_idx': char_to_idx,\n",
                "            'idx_to_char': {str(k): v for k, v in idx_to_char.items()}\n",
                "        }, f, ensure_ascii=False)\n",
                "    print(f\"‚úì Vocab saved to {VOCAB_CACHE}\")\n",
                "    \n",
                "    del char_counts\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode to tensors ‚Äî one array at a time to save memory!\n",
                "if not CACHE_LOADED:\n",
                "    PAD = char_to_idx['<PAD>']\n",
                "    UNK = char_to_idx['<UNK>']\n",
                "    n = len(training_data)\n",
                "    \n",
                "    print(f\"\\nüî¢ Encoding {n:,} samples (one array at a time)...\")\n",
                "    \n",
                "    # --- 1. Encoder inputs ---\n",
                "    arr = np.zeros((n, MAX_ENCODER_LEN), dtype=np.int32)\n",
                "    for i, d in enumerate(tqdm(training_data, desc=\"Enc input\")):\n",
                "        arr[i] = encode_encoder_input(d['input'], char_to_idx, PAD, UNK)\n",
                "    np.save(ENC_CACHE, arr)\n",
                "    del arr; gc.collect()\n",
                "    print(f\"‚úì Saved encoder_inputs ‚Üí {ENC_CACHE}\")\n",
                "    \n",
                "    # --- 2. Decoder inputs (with BOS) ---\n",
                "    arr = np.zeros((n, MAX_DECODER_LEN), dtype=np.int32)\n",
                "    for i, d in enumerate(tqdm(training_data, desc=\"Dec input\")):\n",
                "        arr[i] = encode_decoder_seq(d['output'], char_to_idx, PAD, UNK, add_bos=True)\n",
                "    assert arr[0][0] == char_to_idx['<BOS>'], f\"Expected BOS, got {arr[0][0]}\"\n",
                "    np.save(DEC_IN_CACHE, arr)\n",
                "    del arr; gc.collect()\n",
                "    print(f\"‚úì Saved decoder_inputs ‚Üí {DEC_IN_CACHE}\")\n",
                "    \n",
                "    # --- 3. Decoder targets (with EOS) ---\n",
                "    arr = np.zeros((n, MAX_DECODER_LEN), dtype=np.int32)\n",
                "    for i, d in enumerate(tqdm(training_data, desc=\"Dec target\")):\n",
                "        arr[i] = encode_decoder_seq(d['output'], char_to_idx, PAD, UNK, add_eos=True)\n",
                "    assert char_to_idx['<EOS>'] in list(arr[0]), \"Decoder target should contain EOS\"\n",
                "    np.save(DEC_TGT_CACHE, arr)\n",
                "    del arr; gc.collect()\n",
                "    print(f\"‚úì Saved decoder_targets ‚Üí {DEC_TGT_CACHE}\")\n",
                "    \n",
                "    # üíæ Save some test cases from the dataset for verification later\n",
                "    TEST_CASES_FILE = f\"{CACHE_DIR}/test_cases_kkc{CACHE_SUFFIX}.json\"\n",
                "    test_cases_data = []\n",
                "    # Pick diverse examples: every N-th item to get different lengths\n",
                "    step = max(1, len(training_data) // 20)\n",
                "    for i in range(0, len(training_data), step):\n",
                "        d = training_data[i]\n",
                "        # Parse back context and kana from encoder text\n",
                "        parts = d['input'].split('<SEP>')\n",
                "        if len(parts) == 2:\n",
                "            test_cases_data.append({\n",
                "                'context': parts[0],\n",
                "                'kana': parts[1],\n",
                "                'expected': d['output']\n",
                "            })\n",
                "        if len(test_cases_data) >= 20:\n",
                "            break\n",
                "    \n",
                "    with open(TEST_CASES_FILE, 'w', encoding='utf-8') as f:\n",
                "        json.dump(test_cases_data, f, ensure_ascii=False, indent=2)\n",
                "    print(f\"üíæ Saved {len(test_cases_data)} test cases ‚Üí {TEST_CASES_FILE}\")\n",
                "    \n",
                "    # üßπ Release training_data\n",
                "    del training_data, test_cases_data\n",
                "    gc.collect()\n",
                "    print(\"\\nüßπ All arrays saved. Released training_data from memory.\")\n",
                "    \n",
                "    # Load as memory-mapped\n",
                "    enc_mmap = np.load(ENC_CACHE, mmap_mode='r')\n",
                "    dec_in_mmap = np.load(DEC_IN_CACHE, mmap_mode='r')\n",
                "    dec_tgt_mmap = np.load(DEC_TGT_CACHE, mmap_mode='r')\n",
                "    print(f\"‚úì Loaded as mmap: enc={enc_mmap.shape}, dec_in={dec_in_mmap.shape}\")\n",
                "\n",
                "print(f\"\\nüìä Data: {len(enc_mmap):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_samples = len(enc_mmap)\n",
                "split = int(n_samples * 0.9)\n",
                "\n",
                "# Bucket shuffle: shuffle within length groups\n",
                "def bucket_shuffle_indices(n, bucket_size=50000):\n",
                "    indices = []\n",
                "    for start in range(0, n, bucket_size):\n",
                "        end = min(start + bucket_size, n)\n",
                "        bucket_idx = list(range(start, end))\n",
                "        np.random.shuffle(bucket_idx)\n",
                "        indices.extend(bucket_idx)\n",
                "    return np.array(indices, dtype=np.int32)\n",
                "\n",
                "all_indices = bucket_shuffle_indices(n_samples)\n",
                "train_indices = all_indices[:split]\n",
                "val_indices = all_indices[split:]\n",
                "\n",
                "def make_generator(enc, dec_in, dec_tgt, indices):\n",
                "    def gen():\n",
                "        for i in indices:\n",
                "            yield (\n",
                "                {'encoder_input': enc[i], 'decoder_input': dec_in[i]},\n",
                "                dec_tgt[i]\n",
                "            )\n",
                "    return gen\n",
                "\n",
                "output_sig = (\n",
                "    {\n",
                "        'encoder_input': tf.TensorSpec(shape=(MAX_ENCODER_LEN,), dtype=tf.int32),\n",
                "        'decoder_input': tf.TensorSpec(shape=(MAX_DECODER_LEN,), dtype=tf.int32),\n",
                "    },\n",
                "    tf.TensorSpec(shape=(MAX_DECODER_LEN,), dtype=tf.int32),\n",
                ")\n",
                "\n",
                "# .repeat() is required: from_generator is one-shot\n",
                "train_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(enc_mmap, dec_in_mmap, dec_tgt_mmap, train_indices),\n",
                "    output_signature=output_sig\n",
                ").repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_generator(\n",
                "    make_generator(enc_mmap, dec_in_mmap, dec_tgt_mmap, val_indices),\n",
                "    output_signature=output_sig\n",
                ").repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"Train: {len(train_indices):,}, Val: {len(val_indices):,}\")\n",
                "print(f\"üí° Data loaded via mmap + generator (near-zero RAM)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Model (Bidirectional GRU + Attention)\n",
                "\n",
                "Model is built inside `strategy.scope()` for multi-GPU training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, Bidirectional, Attention, Concatenate, LayerNormalization\n",
                "\n",
                "with strategy.scope():\n",
                "    # Shared embedding\n",
                "    emb = Embedding(vocab_size, EMBEDDING_DIM, mask_zero=False, name='embedding')\n",
                "\n",
                "    # Encoder (Bidirectional GRU)\n",
                "    enc_in = Input(shape=(MAX_ENCODER_LEN,), dtype='int32', name='encoder_input')\n",
                "    x = Dropout(0.1)(emb(enc_in))\n",
                "    for i in range(NUM_ENCODER_LAYERS):\n",
                "        x = LayerNormalization()(Bidirectional(GRU(GRU_UNITS, return_sequences=True), name=f'enc_{i+1}')(x))\n",
                "    enc_out = x\n",
                "\n",
                "    # Decoder (GRU with Attention)\n",
                "    dec_in = Input(shape=(MAX_DECODER_LEN,), dtype='int32', name='decoder_input')\n",
                "    y = Dropout(0.1)(emb(dec_in))\n",
                "    for i in range(NUM_DECODER_LAYERS):\n",
                "        y = LayerNormalization()(GRU(GRU_UNITS * 2, return_sequences=True, name=f'dec_{i+1}')(y))\n",
                "\n",
                "    # Attention mechanism\n",
                "    ctx = Attention(use_scale=True, name='attn')([y, enc_out])\n",
                "\n",
                "    # Output\n",
                "    combined = Concatenate()([y, ctx])\n",
                "    combined = LayerNormalization()(combined)\n",
                "    combined = Dropout(0.2)(combined)\n",
                "    combined = Dense(GRU_UNITS * 2, activation='relu')(combined)\n",
                "    out = Dense(vocab_size, activation='softmax', name='output', dtype='float32')(combined)\n",
                "\n",
                "    model = Model([enc_in, dec_in], out, name='kana_kanji_v3_1')\n",
                "\n",
                "    # Masked loss to ignore PAD tokens\n",
                "    def masked_sparse_ce(y_true, y_pred):\n",
                "        mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
                "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
                "        masked_loss = loss * mask\n",
                "        return tf.reduce_sum(masked_loss) / (tf.reduce_sum(mask) + 1e-8)\n",
                "\n",
                "    def masked_accuracy(y_true, y_pred):\n",
                "        mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
                "        pred_ids = tf.cast(tf.argmax(y_pred, axis=-1), y_true.dtype)\n",
                "        correct = tf.cast(tf.equal(y_true, pred_ids), tf.float32) * mask\n",
                "        return tf.reduce_sum(correct) / (tf.reduce_sum(mask) + 1e-8)\n",
                "\n",
                "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
                "\n",
                "    model.compile(\n",
                "        optimizer=optimizer,\n",
                "        loss=masked_sparse_ce,\n",
                "        metrics=[masked_accuracy]\n",
                "    )\n",
                "\n",
                "actual_params = model.count_params()\n",
                "print(f\"üìä Actual params: {actual_params:,}\")\n",
                "print(f\"   FP32: ~{actual_params * 4 / 1024 / 1024:.1f} MB\")\n",
                "print(f\"   FP16: ~{actual_params * 2 / 1024 / 1024:.1f} MB\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "steps_per_epoch = len(train_indices) // BATCH_SIZE\n",
                "validation_steps = max(1, len(val_indices) // BATCH_SIZE)\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best_v3_1.keras', monitor='val_masked_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
                "]\n",
                "\n",
                "print(f\"Steps/epoch: {steps_per_epoch}, Val steps: {validation_steps}\")\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    steps_per_epoch=steps_per_epoch,\n",
                "    validation_data=val_ds,\n",
                "    validation_steps=validation_steps,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.legend(); ax1.set_title('Loss (masked)')\n",
                "\n",
                "ax2.plot(history.history['masked_accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_masked_accuracy'], label='Val')\n",
                "ax2.legend(); ax2.set_title('Accuracy (masked)')\n",
                "\n",
                "plt.savefig(f'{MODEL_DIR}/training_v3_1.png')\n",
                "plt.show()\n",
                "\n",
                "# ‚úÖ Logic check\n",
                "losses = history.history['loss']\n",
                "accs = history.history['masked_accuracy']\n",
                "print(f\"\\nüìä Training Summary:\")\n",
                "print(f\"  Loss:     {losses[0]:.4f} ‚Üí {losses[-1]:.4f} ({'‚úÖ decreasing' if losses[-1] < losses[0] else '‚ùå NOT decreasing'})\")\n",
                "print(f\"  Accuracy: {accs[0]*100:.2f}% ‚Üí {accs[-1]*100:.2f}% ({'‚úÖ increasing' if accs[-1] > accs[0] else '‚ùå NOT increasing'})\")\n",
                "print(f\"  Best val accuracy: {max(history.history['val_masked_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/char_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(char_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_char.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_char.items()}, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        'vocab_size': vocab_size,\n",
                "        'max_encoder_len': MAX_ENCODER_LEN,\n",
                "        'max_decoder_len': MAX_DECODER_LEN,\n",
                "        'sep_token': SEP_TOKEN,\n",
                "        'version': 'v3.1'\n",
                "    }, f)\n",
                "\n",
                "keras_size = os.path.getsize(f'{MODEL_DIR}/model.keras')\n",
                "print(f\"‚úì Model saved: {keras_size / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export TFLite\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite)\n",
                "    print(f\"‚úì model.tflite ({len(tflite)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite16)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö† TFLite export failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verification ‚Äî Real Test Cases from Dataset\n",
                "\n",
                "Uses **real examples from training data** to verify the model learned.\n",
                "\n",
                "**What to check**:\n",
                "- ‚úÖ Output matches or partially matches expected kanji\n",
                "- ‚úÖ Model doesn't output empty strings or garbage\n",
                "- ‚úÖ Context influences the output (different context ‚Üí different result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION: Real Test Cases from Dataset\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "PAD = char_to_idx['<PAD>']\n",
                "BOS = char_to_idx['<BOS>']\n",
                "EOS = char_to_idx['<EOS>']\n",
                "UNK = char_to_idx['<UNK>']\n",
                "SEP = char_to_idx['<SEP>']\n",
                "\n",
                "def convert(context, kana):\n",
                "    \"\"\"Convert kana to kanji using context. Uses shared encode function.\"\"\"\n",
                "    enc_text = f\"{context}<SEP>{kana}\"\n",
                "    enc_ids = encode_encoder_input(enc_text, char_to_idx, PAD, UNK)\n",
                "    enc_in = np.array([enc_ids], dtype=np.int32)\n",
                "    \n",
                "    dec_in = np.zeros((1, MAX_DECODER_LEN), dtype=np.int32)\n",
                "    dec_in[0, 0] = BOS\n",
                "    \n",
                "    result = []\n",
                "    for i in range(MAX_DECODER_LEN - 1):\n",
                "        pred = model.predict({'encoder_input': enc_in, 'decoder_input': dec_in}, verbose=0)\n",
                "        next_id = int(np.argmax(pred[0, i]))\n",
                "        \n",
                "        if next_id == EOS:\n",
                "            break\n",
                "        if next_id not in [PAD, BOS, EOS, UNK, SEP]:\n",
                "            result.append(idx_to_char.get(next_id, ''))\n",
                "        \n",
                "        if i + 1 < MAX_DECODER_LEN:\n",
                "            dec_in[0, i + 1] = next_id\n",
                "    \n",
                "    return ''.join(result)\n",
                "\n",
                "\n",
                "# ==========================================================\n",
                "# Load test cases saved during data prep\n",
                "# ==========================================================\n",
                "TEST_CASES_FILE = f\"{CACHE_DIR}/test_cases_kkc{CACHE_SUFFIX}.json\"\n",
                "if os.path.exists(TEST_CASES_FILE):\n",
                "    with open(TEST_CASES_FILE, 'r', encoding='utf-8') as f:\n",
                "        test_cases = json.load(f)\n",
                "    print(f\"\\nüìù Loaded {len(test_cases)} test cases from dataset\")\n",
                "else:\n",
                "    # Fallback: use hardcoded test cases\n",
                "    print(\"\\n‚ö†Ô∏è No saved test cases found, using defaults\")\n",
                "    test_cases = [\n",
                "        {'context': '‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇ', 'kana': '„Ç¢„ÉÑ„Ç§', 'expected': 'Êöë„ÅÑ'},\n",
                "        {'context': '„ÅäËå∂„Åå', 'kana': '„Ç¢„ÉÑ„Ç§', 'expected': 'ÁÜ±„ÅÑ'},\n",
                "        {'context': 'Â∑ù„Å´', 'kana': '„Éè„Ç∑', 'expected': 'Ê©ã'},\n",
                "        {'context': '„ÅîÈ£Ø„Çí', 'kana': '„Éè„Ç∑', 'expected': 'ÁÆ∏'},\n",
                "    ]\n",
                "\n",
                "# ==========================================================\n",
                "# Run predictions\n",
                "# ==========================================================\n",
                "print(\"-\" * 60)\n",
                "\n",
                "correct = 0\n",
                "partial = 0\n",
                "\n",
                "for tc in test_cases:\n",
                "    result = convert(tc['context'], tc['kana'])\n",
                "    expected = tc['expected']\n",
                "    \n",
                "    exact_match = result == expected\n",
                "    partial_match = expected in result or result in expected\n",
                "    \n",
                "    if exact_match:\n",
                "        correct += 1\n",
                "        status = '‚úÖ'\n",
                "    elif partial_match:\n",
                "        partial += 1\n",
                "        status = 'üü°'\n",
                "    else:\n",
                "        status = '‚ùå'\n",
                "    \n",
                "    ctx_short = tc['context'][:15]\n",
                "    print(f\"  {status} {ctx_short}<SEP>{tc['kana']} ‚Üí {result} (expected: {expected})\")\n",
                "\n",
                "n = len(test_cases)\n",
                "print(f\"\\nüìä Results:\")\n",
                "print(f\"  Exact match: {correct}/{n} ({correct/n*100:.1f}%)\")\n",
                "print(f\"  Partial match: {partial}/{n} ({partial/n*100:.1f}%)\")\n",
                "print(f\"  Total useful: {correct+partial}/{n} ({(correct+partial)/n*100:.1f}%)\")\n",
                "\n",
                "if TESTING_MODE:\n",
                "    print(\"\\n‚ö†Ô∏è TESTING MODE ‚Äî results may be weak (only 10K samples).\")\n",
                "    print(\"   ‚úÖ Check: loss decreased, accuracy improved, no crashes.\")\n",
                "    print(\"   ‚Üí Set TESTING_MODE = False for real training.\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List saved files\n",
                "print(f\"\\nüì¶ Files ({PLATFORM}):\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    p = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(p):\n",
                "        s = os.path.getsize(p)\n",
                "        if s > 1024*1024:\n",
                "            print(f\"  {f}: {s/(1024*1024):.2f}MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {s/1024:.1f}KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}