{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Context-Aware Kana-Kanji Converter (v3)\n",
                "\n",
                "**Supports**: Google Colab & Kaggle\n",
                "\n",
                "**Key Features**:\n",
                "- Uses zenz dataset **directly** (no tokenization needed)\n",
                "- **Length-based bucketing** for stable GRU training\n",
                "- Bidirectional GRU + Attention (best for this task)\n",
                "- Model size controlled by architecture (<20MB FP16)\n",
                "\n",
                "**Input Format**: `context<SEP>kana` ‚Üí `kanji`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Auto-detect platform\n",
                "if os.path.exists('/kaggle'):\n",
                "    PLATFORM = 'Kaggle'\n",
                "    DRIVE_DIR = '/kaggle/working'\n",
                "else:\n",
                "    PLATFORM = 'Colab'\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_kana_kanji\"\n",
                "CACHE_DIR = f\"{DRIVE_DIR}/cache/kana_kanji\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"‚úÖ Platform: {PLATFORM}\")\n",
                "print(f\"üìÅ Model: {MODEL_DIR}\")\n",
                "print(f\"üíæ Cache: {CACHE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets numpy tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===========================================================\n",
                "# CONFIGURATION\n",
                "# ===========================================================\n",
                "TESTING_MODE = False\n",
                "MAX_SAMPLES = 5_000_000  # Start with 5M (set None for all ~17.5M)\n",
                "BATCH_SIZE = 512\n",
                "FORCE_REBUILD_CACHE = False\n",
                "\n",
                "NUM_EPOCHS = 10 if TESTING_MODE else 20\n",
                "\n",
                "# Length limits (filter long sequences)\n",
                "MAX_CONTEXT_LEN = 30   # left_context max chars\n",
                "MAX_INPUT_LEN = 30     # kana input max chars\n",
                "MAX_OUTPUT_LEN = 20    # kanji output max chars\n",
                "MAX_ENCODER_LEN = MAX_CONTEXT_LEN + 5 + MAX_INPUT_LEN  # context + <SEP> + input\n",
                "\n",
                "# Architecture (controls model size)\n",
                "CHAR_VOCAB_SIZE = 6000\n",
                "EMBEDDING_DIM = 64\n",
                "GRU_UNITS = 128\n",
                "NUM_ENCODER_LAYERS = 2\n",
                "NUM_DECODER_LAYERS = 2\n",
                "\n",
                "# Estimate size\n",
                "est_params = CHAR_VOCAB_SIZE * EMBEDDING_DIM + GRU_UNITS * 4 * CHAR_VOCAB_SIZE\n",
                "print(f'üìä Est. ~{est_params * 2 / 1024 / 1024:.1f} MB FP16')\n",
                "\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<SEP>']\n",
                "SEP_TOKEN = '<SEP>'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load or Build Cached Data\n",
                "\n",
                "Uses zenz dataset **directly** - no tokenization needed!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "VOCAB_CACHE = f\"{CACHE_DIR}/vocabulary_v3.json\"\n",
                "TENSORS_CACHE = f\"{CACHE_DIR}/tensors_v3.npz\"\n",
                "\n",
                "def cache_exists():\n",
                "    return os.path.exists(VOCAB_CACHE) and os.path.exists(TENSORS_CACHE)\n",
                "\n",
                "if cache_exists() and not FORCE_REBUILD_CACHE:\n",
                "    print(\"üì¶ Loading from cache...\")\n",
                "    \n",
                "    with open(VOCAB_CACHE, 'r', encoding='utf-8') as f:\n",
                "        vocab_data = json.load(f)\n",
                "    char_to_idx = vocab_data['char_to_idx']\n",
                "    idx_to_char = {int(k): v for k, v in vocab_data['idx_to_char'].items()}\n",
                "    vocab_size = len(char_to_idx)\n",
                "    \n",
                "    tensors = np.load(TENSORS_CACHE)\n",
                "    encoder_inputs = tensors['encoder_inputs']\n",
                "    decoder_inputs = tensors['decoder_inputs']\n",
                "    decoder_targets = tensors['decoder_targets']\n",
                "    \n",
                "    print(f\"‚úì Loaded {len(encoder_inputs):,} samples\")\n",
                "    CACHE_LOADED = True\n",
                "else:\n",
                "    print(\"üî® Building from scratch (using dataset directly)...\")\n",
                "    CACHE_LOADED = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset directly - no tokenization needed!\n",
                "if not CACHE_LOADED:\n",
                "    from datasets import load_dataset\n",
                "    \n",
                "    print(\"üì• Loading zenz dataset...\")\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=\"train\"\n",
                "    )\n",
                "    print(f\"‚úì Raw dataset: {len(dataset):,} items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter and prepare training data (fast - no tokenization!)\n",
                "if not CACHE_LOADED:\n",
                "    print(\"\\nüîç Filtering and preparing data...\")\n",
                "    \n",
                "    training_data = []\n",
                "    skipped = {'too_long': 0, 'empty': 0}\n",
                "    \n",
                "    for item in tqdm(dataset, desc=\"Processing\"):\n",
                "        # Get fields from dataset\n",
                "        kana_input = item.get('input', '') or ''  # Katakana input\n",
                "        kanji_output = item.get('output', '') or ''  # Kanji output\n",
                "        left_context = item.get('left_context', '') or ''  # Context (can be null)\n",
                "        \n",
                "        # Skip empty\n",
                "        if not kana_input or not kanji_output:\n",
                "            skipped['empty'] += 1\n",
                "            continue\n",
                "        \n",
                "        # Skip too long\n",
                "        if (len(left_context) > MAX_CONTEXT_LEN or \n",
                "            len(kana_input) > MAX_INPUT_LEN or \n",
                "            len(kanji_output) > MAX_OUTPUT_LEN):\n",
                "            skipped['too_long'] += 1\n",
                "            continue\n",
                "        \n",
                "        # Format: \"context<SEP>kana\" -> \"kanji\"\n",
                "        encoder_text = f\"{left_context}<SEP>{kana_input}\"\n",
                "        decoder_text = kanji_output\n",
                "        \n",
                "        training_data.append({\n",
                "            'input': encoder_text,\n",
                "            'output': decoder_text,\n",
                "            'input_len': len(kana_input)  # For bucketing\n",
                "        })\n",
                "        \n",
                "        # Stop if reached limit\n",
                "        if MAX_SAMPLES and len(training_data) >= MAX_SAMPLES:\n",
                "            break\n",
                "    \n",
                "    print(f\"\\n‚úì Valid examples: {len(training_data):,}\")\n",
                "    print(f\"  Skipped (too long): {skipped['too_long']:,}\")\n",
                "    print(f\"  Skipped (empty): {skipped['empty']:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sort by length for bucketing (helps GRU training stability)\n",
                "if not CACHE_LOADED:\n",
                "    print(\"\\nüìä Sorting by length (bucketing)...\")\n",
                "    training_data.sort(key=lambda x: x['input_len'])\n",
                "    \n",
                "    # Show length distribution\n",
                "    lengths = [d['input_len'] for d in training_data]\n",
                "    print(f\"  Short (0-10): {sum(1 for l in lengths if l <= 10):,}\")\n",
                "    print(f\"  Medium (11-20): {sum(1 for l in lengths if 10 < l <= 20):,}\")\n",
                "    print(f\"  Long (21+): {sum(1 for l in lengths if l > 20):,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build vocabulary\n",
                "if not CACHE_LOADED:\n",
                "    from collections import Counter\n",
                "    \n",
                "    print(\"\\nüìù Building vocabulary...\")\n",
                "    char_counts = Counter()\n",
                "    \n",
                "    for d in tqdm(training_data, desc=\"Counting chars\"):\n",
                "        # Count chars (excluding <SEP> marker)\n",
                "        text = d['input'].replace('<SEP>', '') + d['output']\n",
                "        char_counts.update(list(text))\n",
                "    \n",
                "    # Build vocab with special tokens first\n",
                "    char_to_idx = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
                "    for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE - len(SPECIAL_TOKENS)):\n",
                "        char_to_idx[char] = len(char_to_idx)\n",
                "    \n",
                "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
                "    vocab_size = len(char_to_idx)\n",
                "    print(f\"‚úì Vocab size: {vocab_size}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode to tensors\n",
                "if not CACHE_LOADED:\n",
                "    print(\"\\nüî¢ Encoding to tensors...\")\n",
                "    \n",
                "    PAD, UNK = char_to_idx['<PAD>'], char_to_idx['<UNK>']\n",
                "    \n",
                "    def encode_input(text):\n",
                "        \"\"\"Encode input with <SEP> token handling.\"\"\"\n",
                "        tokens = []\n",
                "        i = 0\n",
                "        while i < len(text):\n",
                "            if text[i:i+5] == '<SEP>':\n",
                "                tokens.append('<SEP>')\n",
                "                i += 5\n",
                "            else:\n",
                "                tokens.append(text[i])\n",
                "                i += 1\n",
                "        ids = [char_to_idx.get(t, UNK) for t in tokens][:MAX_ENCODER_LEN]\n",
                "        return ids + [PAD] * (MAX_ENCODER_LEN - len(ids))\n",
                "    \n",
                "    def encode_output(text, add_bos=False, add_eos=False):\n",
                "        \"\"\"Encode output with optional BOS/EOS.\"\"\"\n",
                "        tokens = []\n",
                "        if add_bos:\n",
                "            tokens.append('<BOS>')\n",
                "        tokens.extend(list(text))\n",
                "        if add_eos:\n",
                "            tokens.append('<EOS>')\n",
                "        ids = [char_to_idx.get(c, UNK) for c in tokens][:MAX_OUTPUT_LEN + 1]\n",
                "        return ids + [PAD] * (MAX_OUTPUT_LEN + 1 - len(ids))\n",
                "    \n",
                "    # Encode all data\n",
                "    encoder_inputs = np.array(\n",
                "        [encode_input(d['input']) for d in tqdm(training_data, desc=\"Encoding\")],\n",
                "        dtype=np.int32\n",
                "    )\n",
                "    decoder_inputs = np.array(\n",
                "        [encode_output(d['output'], add_bos=True) for d in training_data],\n",
                "        dtype=np.int32\n",
                "    )\n",
                "    decoder_targets = np.array(\n",
                "        [encode_output(d['output'], add_eos=True) for d in training_data],\n",
                "        dtype=np.int32\n",
                "    )\n",
                "    \n",
                "    # Save cache\n",
                "    print(\"\\nüíæ Saving cache...\")\n",
                "    with open(VOCAB_CACHE, 'w', encoding='utf-8') as f:\n",
                "        json.dump({\n",
                "            'char_to_idx': char_to_idx,\n",
                "            'idx_to_char': {str(k): v for k, v in idx_to_char.items()}\n",
                "        }, f, ensure_ascii=False)\n",
                "    \n",
                "    np.savez_compressed(\n",
                "        TENSORS_CACHE,\n",
                "        encoder_inputs=encoder_inputs,\n",
                "        decoder_inputs=decoder_inputs,\n",
                "        decoder_targets=decoder_targets\n",
                "    )\n",
                "    print(\"‚úì Cached!\")\n",
                "\n",
                "print(f\"\\nüìä Data shape: {encoder_inputs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Shuffle within buckets (keep similar lengths together)\n",
                "def bucket_shuffle(data, bucket_size=50000):\n",
                "    \"\"\"Shuffle within buckets to maintain length grouping.\"\"\"\n",
                "    n = len(data)\n",
                "    indices = []\n",
                "    for start in range(0, n, bucket_size):\n",
                "        end = min(start + bucket_size, n)\n",
                "        bucket_idx = list(range(start, end))\n",
                "        np.random.shuffle(bucket_idx)\n",
                "        indices.extend(bucket_idx)\n",
                "    return np.array(indices)\n",
                "\n",
                "idx = bucket_shuffle(encoder_inputs)\n",
                "encoder_inputs = encoder_inputs[idx]\n",
                "decoder_inputs = decoder_inputs[idx]\n",
                "decoder_targets = decoder_targets[idx]\n",
                "\n",
                "# Split\n",
                "split = int(len(encoder_inputs) * 0.9)\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[:split], 'decoder_input': decoder_inputs[:split]},\n",
                "    decoder_targets[:split]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((\n",
                "    {'encoder_input': encoder_inputs[split:], 'decoder_input': decoder_inputs[split:]},\n",
                "    decoder_targets[split:]\n",
                ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"Train: {split:,}, Val: {len(encoder_inputs)-split:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Model (Bidirectional GRU + Attention)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, Bidirectional, Attention, Concatenate, LayerNormalization\n",
                "\n",
                "# Shared embedding\n",
                "emb = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')\n",
                "\n",
                "# Encoder (Bidirectional GRU - reads input forward and backward)\n",
                "enc_in = Input(shape=(MAX_ENCODER_LEN,), dtype='int32', name='encoder_input')\n",
                "x = Dropout(0.1)(emb(enc_in))\n",
                "for i in range(NUM_ENCODER_LAYERS):\n",
                "    x = LayerNormalization()(Bidirectional(GRU(GRU_UNITS, return_sequences=True), name=f'enc_{i+1}')(x))\n",
                "enc_out = x\n",
                "\n",
                "# Decoder (GRU with Attention)\n",
                "dec_in = Input(shape=(MAX_OUTPUT_LEN + 1,), dtype='int32', name='decoder_input')\n",
                "y = Dropout(0.1)(emb(dec_in))\n",
                "for i in range(NUM_DECODER_LAYERS):\n",
                "    y = LayerNormalization()(GRU(GRU_UNITS * 2, return_sequences=True, name=f'dec_{i+1}')(y))\n",
                "\n",
                "# Attention mechanism\n",
                "ctx = Attention(use_scale=True, name='attn')([y, enc_out])\n",
                "\n",
                "# Output\n",
                "combined = Concatenate()([y, ctx])\n",
                "combined = LayerNormalization()(combined)\n",
                "combined = Dropout(0.2)(combined)\n",
                "combined = Dense(GRU_UNITS * 2, activation='relu')(combined)\n",
                "out = Dense(vocab_size, activation='softmax', name='output')(combined)\n",
                "\n",
                "model = Model([enc_in, dec_in], out, name='kana_kanji_v3')\n",
                "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "print(f\"üìä {model.count_params():,} params, ~{model.count_params()*2/1024/1024:.1f}MB FP16\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.legend(); ax1.set_title('Loss')\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.legend(); ax2.set_title('Accuracy')\n",
                "\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "print(f\"Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model and config\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/char_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(char_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_char.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_char.items()}, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        'vocab_size': vocab_size,\n",
                "        'max_encoder_len': MAX_ENCODER_LEN,\n",
                "        'max_output_len': MAX_OUTPUT_LEN + 1,\n",
                "        'sep_token': SEP_TOKEN\n",
                "    }, f)\n",
                "\n",
                "print(\"‚úì Model saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export TFLite\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite)\n",
                "    print(f\"‚úì model.tflite ({len(tflite)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # FP16 version\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite16)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ö† TFLite export failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "PAD, BOS, EOS, UNK, SEP = [char_to_idx[t] for t in ['<PAD>', '<BOS>', '<EOS>', '<UNK>', '<SEP>']]\n",
                "\n",
                "def encode_input_for_inference(context, kana):\n",
                "    \"\"\"Encode input for inference.\"\"\"\n",
                "    text = f\"{context}<SEP>{kana}\"\n",
                "    tokens = []\n",
                "    i = 0\n",
                "    while i < len(text):\n",
                "        if text[i:i+5] == '<SEP>':\n",
                "            tokens.append('<SEP>')\n",
                "            i += 5\n",
                "        else:\n",
                "            tokens.append(text[i])\n",
                "            i += 1\n",
                "    ids = [char_to_idx.get(t, UNK) for t in tokens][:MAX_ENCODER_LEN]\n",
                "    ids = ids + [PAD] * (MAX_ENCODER_LEN - len(ids))\n",
                "    return np.array([ids], dtype=np.int32)\n",
                "\n",
                "def convert(context, kana):\n",
                "    \"\"\"Convert kana to kanji using context.\"\"\"\n",
                "    enc_in = encode_input_for_inference(context, kana)\n",
                "    dec_in = np.zeros((1, MAX_OUTPUT_LEN + 1), dtype=np.int32)\n",
                "    dec_in[0, 0] = BOS\n",
                "    \n",
                "    result = []\n",
                "    for i in range(MAX_OUTPUT_LEN):\n",
                "        pred = model.predict({'encoder_input': enc_in, 'decoder_input': dec_in}, verbose=0)\n",
                "        next_id = int(np.argmax(pred[0, i]))\n",
                "        \n",
                "        if next_id == EOS:\n",
                "            break\n",
                "        if next_id not in [PAD, BOS, EOS, UNK, SEP]:\n",
                "            result.append(idx_to_char.get(next_id, ''))\n",
                "        \n",
                "        if i + 1 < MAX_OUTPUT_LEN + 1:\n",
                "            dec_in[0, i + 1] = next_id\n",
                "    \n",
                "    return ''.join(result)\n",
                "\n",
                "# Test cases (context, kana, expected kanji)\n",
                "tests = [\n",
                "    (\"‰ªäÊó•„ÅØ„Å®„Å¶„ÇÇ\", \"„Ç¢„ÉÑ„Ç§\", \"Êöë„ÅÑ\"),\n",
                "    (\"„ÅäËå∂„Åå\", \"„Ç¢„ÉÑ„Ç§\", \"ÁÜ±„ÅÑ\"),\n",
                "    (\"„Åì„ÅÆËæûÂÖ∏„ÅØ\", \"„Ç¢„ÉÑ„Ç§\", \"Âéö„ÅÑ\"),\n",
                "    (\"ÊØéÊúùËµ∑„Åç„Çã„ÅÆ„Åå\", \"„Éè„É§„Ç§\", \"Êó©„ÅÑ\"),\n",
                "    (\"ÂΩº„ÅØËµ∞„Çã„ÅÆ„Åå\", \"„Éè„É§„Ç§\", \"ÈÄü„ÅÑ\"),\n",
                "    (\"Â∑ù„Å´\", \"„Éè„Ç∑\", \"Ê©ã\"),\n",
                "    (\"„ÅîÈ£Ø„Çí\", \"„Éè„Ç∑\", \"ÁÆ∏\"),\n",
                "    (\"ÈÅì„ÅÆ\", \"„Éè„Ç∑\", \"Á´Ø\"),\n",
                "    (\"Èü≥Ê•Ω„Çí\", \"„Ç≠„ÇØ\", \"ËÅ¥„Åè\"),\n",
                "    (\"ÈÅì„Çí\", \"„Ç≠„ÇØ\", \"ËÅû„Åè\"),\n",
                "    (\"ÂÜôÁúü„Çí\", \"„Éà„É´\", \"ÊíÆ„Çã\"),\n",
                "    (\"Â°©„Çí\", \"„Éà„É´\", \"Âèñ„Çã\"),\n",
                "]\n",
                "\n",
                "correct = 0\n",
                "for ctx, kana, expected in tests:\n",
                "    result = convert(ctx, kana)\n",
                "    ok = result == expected or expected in result or result in expected\n",
                "    if ok:\n",
                "        correct += 1\n",
                "    print(f\"{'‚úì' if ok else '‚úó'} {ctx}<SEP>{kana} ‚Üí {result} (expected: {expected})\")\n",
                "\n",
                "print(f\"\\n‚úÖ Score: {correct}/{len(tests)} ({correct/len(tests)*100:.0f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List saved files\n",
                "print(f\"\\nüì¶ Files ({PLATFORM}):\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    p = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(p):\n",
                "        s = os.path.getsize(p)\n",
                "        if s > 1024*1024:\n",
                "            print(f\"  {f}: {s/(1024*1024):.2f}MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {s/1024:.1f}KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "gpu",
            "isGpuEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}