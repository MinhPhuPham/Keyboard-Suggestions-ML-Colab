{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "japanese_gru_header"
            },
            "source": [
                "# GRU Japanese Keyboard Model (2026) - Smart Word + Emoji Suggestions\n",
                "\n",
                "Train a GRU model for **Japanese keyboard suggestions** with:\n",
                "- **Word-level predictions**: „Åî ‚Üí „Åî„ÅØ„Çì, „Åî„Åñ„ÅÑ„Åæ„Åô\n",
                "- **Prefix completion**: „Åî„Åñ ‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô, „Åî„Åñ„ÅÑ„Åæ„Åó„Åü\n",
                "- **Phrase suggestions**: „Åî„Åñ„ÅÑ„Åæ„Åô ‚Üí „ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô\n",
                "- **Emoji suggestions**: „ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí üôè, Á¨ë ‚Üí üòä\n",
                "\n",
                "**Workflow:**\n",
                "1. Setup & Config\n",
                "2. Load Data (zenz-v2.5-dataset)\n",
                "3. Build Word Vocabulary with Prefix Index (+ Emoji)\n",
                "4. Create Training Data (Input ‚Üí Output pairs)\n",
                "5. Build & Train Model\n",
                "6. Visualize Training\n",
                "7. Save Model\n",
                "8. Export TFLite (Android)\n",
                "9. Export CoreML (iOS)\n",
                "10. Export Mobile Resources (Prefix Index, Word Lists, Emoji)\n",
                "11. Verification Test\n",
                "\n",
                "**Key Features:**\n",
                "- Uses zenz-v2.5-dataset (kana ‚Üí kanji conversion pairs)\n",
                "- Word/morpheme-level tokenization for complete word suggestions\n",
                "- Prefix matching for smart autocomplete\n",
                "- **Emoji support** - suggests emojis based on context from dataset\n",
                "- 6000 vocabulary limit for mobile optimization\n",
                "\n",
                "---\n",
                "**Instructions:**\n",
                "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
                "2. Set `TESTING_MODE = True` for quick test\n",
                "3. Set `TESTING_MODE = False` for full training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive and setup directories\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_keyboard_japanese\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"‚úì Model directory: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (including regex for emoji support)\n",
                "!pip install -q tensorflow keras datasets pandas numpy scikit-learn tqdm regex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True  # ‚Üê Change to False for full training\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 5\n",
                "    BATCH_SIZE = 256\n",
                "    VOCAB_SIZE_LIMIT = 6000  # As requested\n",
                "    SEQUENCE_LENGTH = 15    # Longer for word sequences\n",
                "    MAX_SAMPLES = 200000     # Limited for testing\n",
                "else:\n",
                "    NUM_EPOCHS = 20\n",
                "    BATCH_SIZE = 256\n",
                "    VOCAB_SIZE_LIMIT = 6000\n",
                "    SEQUENCE_LENGTH = 15\n",
                "    MAX_SAMPLES = 300000    # More samples for full training\n",
                "\n",
                "# Model architecture\n",
                "EMBEDDING_DIM = 128\n",
                "GRU_UNITS = 256\n",
                "\n",
                "# Special tokens\n",
                "PAD_TOKEN = '<PAD>'\n",
                "UNK_TOKEN = '<UNK>'\n",
                "BOS_TOKEN = '<BOS>'  # Beginning of sequence\n",
                "EOS_TOKEN = '<EOS>'  # End of sequence\n",
                "\n",
                "print(f\"Config: vocab={VOCAB_SIZE_LIMIT:,}, seq={SEQUENCE_LENGTH}, epochs={NUM_EPOCHS}\")\n",
                "print(f\"Max samples: {MAX_SAMPLES:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Datasets\n",
                "\n",
                "The zenz-v2.5-dataset contains:\n",
                "- `input`: Hiragana/Katakana input (what user types)\n",
                "- `output`: Kanji-mixed output (the conversion result)\n",
                "- `left_context`: Previous text for context\n",
                "\n",
                "This is perfect for learning word suggestions!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import re\n",
                "import regex  # For emoji support\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset from Hugging Face...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load dataset\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "    print(f\"‚úì Loaded {len(dataset):,} samples from Wikipedia subset\")\n",
                "except Exception as e:\n",
                "    print(f\"Wikipedia subset not available, trying full dataset...\")\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "    print(f\"‚úì Loaded {len(dataset):,} samples\")\n",
                "\n",
                "# Show sample data\n",
                "print(\"\\nSample entries:\")\n",
                "for i in range(min(5, len(dataset))):\n",
                "    item = dataset[i]\n",
                "    print(f\"  Input: {item['input'][:30]}...\")\n",
                "    print(f\"  Output: {item['output'][:30]}...\")\n",
                "    print(f\"  Context: {str(item.get('left_context', ''))[:30]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Word Vocabulary with Prefix Index (+ Emoji)\n",
                "\n",
                "Build a vocabulary of common Japanese words and emojis:\n",
                "- „Åî ‚Üí [„Åî„ÅØ„Çì, „Åî„ÇÅ„Çì„Å™„Åï„ÅÑ, „ÅîÂçîÂäõ, ...]\n",
                "- „Åî„Åñ ‚Üí [„Åî„Åñ„ÅÑ„Åæ„Åô, „Åî„Åñ„Çã, ...]\n",
                "- „ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí [üôè, „Åî„Åñ„ÅÑ„Åæ„Åô, ...]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import regex\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "print(\"Building word vocabulary with emoji support...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Emoji pattern (covers most common emojis)\n",
                "EMOJI_PATTERN = regex.compile(r'[\\p{Emoji_Presentation}\\p{Extended_Pictographic}]')\n",
                "\n",
                "def is_emoji(char):\n",
                "    \"\"\"Check if character is an emoji.\"\"\"\n",
                "    return bool(EMOJI_PATTERN.match(char))\n",
                "\n",
                "def extract_emojis(text):\n",
                "    \"\"\"Extract all emojis from text.\"\"\"\n",
                "    return EMOJI_PATTERN.findall(text)\n",
                "\n",
                "def segment_japanese(text):\n",
                "    \"\"\"Simple Japanese word segmentation with emoji support.\"\"\"\n",
                "    particles = r'(„ÅØ|„Åå|„Çí|„Å´|„Åß|„Å®|„ÅÆ|„Åã„Çâ|„Åæ„Åß|„Çà„Çä|„Å∏|„ÇÑ|„ÇÇ|„Åã|„Å≠|„Çà|„Çè|„Å™|„Çâ|„Åó|„Å¶|„Åü|„Å†|„Åß„Åô|„Åæ„Åô)'\n",
                "    \n",
                "    # Split by punctuation and spaces (but NOT emojis!)\n",
                "    segments = re.split(r'[„ÄÇ„ÄÅÔºÅÔºü\\s\\n„Éª„Äå„Äç„Äé„ÄèÔºàÔºâ„Äê„Äë]', text)\n",
                "    \n",
                "    words = []\n",
                "    for seg in segments:\n",
                "        if not seg:\n",
                "            continue\n",
                "        \n",
                "        # Extract emojis from this segment\n",
                "        emojis_in_seg = extract_emojis(seg)\n",
                "        \n",
                "        # Remove emojis temporarily for word splitting\n",
                "        text_only = EMOJI_PATTERN.sub('', seg)\n",
                "        \n",
                "        if text_only:\n",
                "            if len(text_only) <= 6:\n",
                "                words.append(text_only)\n",
                "            else:\n",
                "                parts = re.split(particles, text_only)\n",
                "                words.extend([p for p in parts if p])\n",
                "        \n",
                "        # Add emojis as separate tokens (they follow the word)\n",
                "        words.extend(emojis_in_seg)\n",
                "    \n",
                "    return [w for w in words if w and len(w) <= 20]\n",
                "\n",
                "# Collect all words from output text\n",
                "word_counts = Counter()\n",
                "all_outputs = []\n",
                "\n",
                "for item in dataset:\n",
                "    output = item.get('output', '')\n",
                "    if output:\n",
                "        all_outputs.append(output)\n",
                "        words = segment_japanese(output)\n",
                "        word_counts.update(words)\n",
                "    \n",
                "    # Also use left_context\n",
                "    context = item.get('left_context', '')\n",
                "    if context:\n",
                "        all_outputs.append(context)\n",
                "        words = segment_japanese(context)\n",
                "        word_counts.update(words)\n",
                "\n",
                "print(f\"‚úì Found {len(word_counts):,} unique words/tokens (including emoji)\")\n",
                "\n",
                "# Count emojis found\n",
                "emoji_count = sum(1 for w in word_counts if len(w) <= 2 and EMOJI_PATTERN.match(w))\n",
                "print(f\"‚úì Found {emoji_count:,} unique emojis in dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter to valid Japanese words and emojis\n",
                "def is_valid_japanese_word(word):\n",
                "    \"\"\"Check if word contains valid Japanese characters or emoji.\"\"\"\n",
                "    if not word or len(word) < 1:\n",
                "        return False\n",
                "    \n",
                "    # Single emoji is valid\n",
                "    if len(word) <= 2 and EMOJI_PATTERN.match(word):\n",
                "        return True\n",
                "    \n",
                "    for char in word:\n",
                "        code = ord(char)\n",
                "        if not (0x3040 <= code <= 0x309F or  # Hiragana\n",
                "                0x30A0 <= code <= 0x30FF or  # Katakana\n",
                "                0x4E00 <= code <= 0x9FFF or  # Kanji\n",
                "                0x3400 <= code <= 0x4DBF or  # CJK Extension\n",
                "                is_emoji(char) or            # Emoji\n",
                "                char in '„Éº„Äú'):\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "# Get top words by frequency\n",
                "valid_words = [(word, count) for word, count in word_counts.most_common()\n",
                "               if is_valid_japanese_word(word)]\n",
                "\n",
                "# Limit vocabulary\n",
                "valid_words = valid_words[:VOCAB_SIZE_LIMIT - 4]  # Reserve 4 for special tokens\n",
                "\n",
                "# Create word_to_index\n",
                "word_to_index = {\n",
                "    PAD_TOKEN: 0,\n",
                "    UNK_TOKEN: 1,\n",
                "    BOS_TOKEN: 2,\n",
                "    EOS_TOKEN: 3\n",
                "}\n",
                "\n",
                "for idx, (word, count) in enumerate(valid_words, start=4):\n",
                "    word_to_index[word] = idx\n",
                "\n",
                "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
                "vocab_size = len(word_to_index)\n",
                "\n",
                "print(f\"\\n‚úì Vocabulary size: {vocab_size:,}\")\n",
                "print(f\"\\nTop 20 words:\")\n",
                "for i, (word, count) in enumerate(valid_words[:20], 1):\n",
                "    idx = word_to_index[word]\n",
                "    emoji_mark = \"üìé\" if EMOJI_PATTERN.match(word) else \"\"\n",
                "    print(f\"  {i:2d}. '{word}' {emoji_mark} (idx={idx}, count={count:,})\")\n",
                "\n",
                "# Show emojis in vocabulary\n",
                "emojis_in_vocab = [(w, c) for w, c in valid_words if len(w) <= 2 and EMOJI_PATTERN.match(w)][:10]\n",
                "print(f\"\\nTop emojis in vocab:\")\n",
                "for emoji, count in emojis_in_vocab:\n",
                "    print(f\"  {emoji} (count={count:,})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build Prefix Index for smart suggestions\n",
                "# Maps prefix ‚Üí [word_indices sorted by frequency]\n",
                "\n",
                "print(\"Building prefix index...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "prefix_index = defaultdict(list)\n",
                "\n",
                "# For each word, add to all its prefixes\n",
                "for word, count in valid_words:\n",
                "    idx = word_to_index[word]\n",
                "    # Generate prefixes (min 1 char, max full word)\n",
                "    for prefix_len in range(1, len(word) + 1):\n",
                "        prefix = word[:prefix_len]\n",
                "        # Store (count, idx) for sorting\n",
                "        prefix_index[prefix].append((count, idx))\n",
                "\n",
                "# Sort each prefix's words by frequency and limit to top 20\n",
                "for prefix in prefix_index:\n",
                "    prefix_index[prefix].sort(reverse=True)  # Higher count first\n",
                "    prefix_index[prefix] = [idx for count, idx in prefix_index[prefix][:20]]\n",
                "\n",
                "print(f\"‚úì Created prefix index with {len(prefix_index):,} prefixes\")\n",
                "\n",
                "# Test examples\n",
                "test_prefixes = ['„Åî', '„Åî„Åñ', '„Åî„Åñ„ÅÑ„Åæ', '„ÅÇ„Çä', '„ÅÇ„Çä„Åå„Å®', '„Åä„ÅØ', '„Åì„Çì„Å´„Å°']\n",
                "print(\"\\nPrefix suggestions:\")\n",
                "for prefix in test_prefixes:\n",
                "    if prefix in prefix_index:\n",
                "        suggestions = [index_to_word[idx] for idx in prefix_index[prefix][:5]]\n",
                "        print(f\"  '{prefix}' ‚Üí {suggestions}\")\n",
                "    else:\n",
                "        print(f\"  '{prefix}' ‚Üí (no matches)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Training Data\n",
                "\n",
                "Create sequences for training:\n",
                "- Input: Previous words in context\n",
                "- Target: Next word to predict (can be emoji!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "\n",
                "print(\"Creating training sequences...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Tokenize all outputs into word sequences\n",
                "all_sequences = []\n",
                "\n",
                "for output in all_outputs:\n",
                "    words = segment_japanese(output)\n",
                "    # Convert to indices\n",
                "    seq = [word_to_index.get(w, 1) for w in words]  # 1 = UNK\n",
                "    if len(seq) >= 2:  # Need at least input + target\n",
                "        all_sequences.append(seq)\n",
                "\n",
                "print(f\"‚úì Created {len(all_sequences):,} sequences\")\n",
                "\n",
                "# Create input-target pairs\n",
                "# [w1, w2, w3, w4] ‚Üí input: [w1,w2,w3], target: w4\n",
                "X_data = []\n",
                "y_data = []\n",
                "\n",
                "for seq in all_sequences:\n",
                "    for i in range(1, len(seq)):\n",
                "        input_seq = seq[:i]\n",
                "        target = seq[i]\n",
                "        # Pad/truncate to SEQUENCE_LENGTH\n",
                "        if len(input_seq) > SEQUENCE_LENGTH:\n",
                "            input_seq = input_seq[-SEQUENCE_LENGTH:]\n",
                "        X_data.append(input_seq)\n",
                "        y_data.append(target)\n",
                "\n",
                "print(f\"‚úì Created {len(X_data):,} training pairs\")\n",
                "\n",
                "# Pad sequences\n",
                "X_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
                "    X_data, maxlen=SEQUENCE_LENGTH, padding='pre'\n",
                ")\n",
                "y_array = np.array(y_data)\n",
                "\n",
                "# Create tf.data dataset\n",
                "dataset_train = tf.data.Dataset.from_tensor_slices((X_padded, y_array))\n",
                "dataset_train = dataset_train.shuffle(10000).batch(BATCH_SIZE)\n",
                "\n",
                "# Split 90/10\n",
                "total_batches = len(X_data) // BATCH_SIZE\n",
                "val_batches = max(1, total_batches // 10)\n",
                "train_batches = total_batches - val_batches\n",
                "\n",
                "train_dataset = dataset_train.take(train_batches).prefetch(tf.data.AUTOTUNE)\n",
                "val_dataset = dataset_train.skip(train_batches).take(val_batches).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"‚úì Train: {train_batches} batches, Val: {val_batches} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build & Train GRU Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import mixed_precision\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
                "from tensorflow.keras.optimizers import AdamW\n",
                "\n",
                "# Enable Mixed Precision\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "# Build model\n",
                "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
                "x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "x = GRU(GRU_UNITS, dropout=0.2, recurrent_dropout=0.2, name='gru')(x)\n",
                "x = Dropout(0.3, name='dropout')(x)\n",
                "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
                "\n",
                "model = Model(inputs=inputs, outputs=outputs, name='gru_keyboard_japanese')\n",
                "\n",
                "model.compile(\n",
                "    optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(\n",
                "        f'{MODEL_DIR}/best_model.keras',\n",
                "        monitor='val_accuracy',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_dataset,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_dataset,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualize Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True)\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy')\n",
                "ax2.legend()\n",
                "ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal: Val Acc={history.history['val_accuracy'][-1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Save Keras model\n",
                "model.save(f'{MODEL_DIR}/gru_model.keras')\n",
                "\n",
                "# Save word_to_index\n",
                "with open(f'{MODEL_DIR}/word_to_index.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_to_index, f, ensure_ascii=False, separators=(',', ':'))\n",
                "\n",
                "# Save prefix_index\n",
                "with open(f'{MODEL_DIR}/prefix_index.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(dict(prefix_index), f, ensure_ascii=False, separators=(',', ':'))\n",
                "\n",
                "# Save config\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'sequence_length': SEQUENCE_LENGTH,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'gru_units': GRU_UNITS,\n",
                "    'language': 'japanese',\n",
                "    'tokenization': 'word-level',\n",
                "    'emoji_support': True,\n",
                "    'special_tokens': {\n",
                "        'PAD': 0, 'UNK': 1, 'BOS': 2, 'EOS': 3\n",
                "    }\n",
                "}\n",
                "with open(f'{MODEL_DIR}/model_config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"‚úì Saved: gru_model.keras, word_to_index.json, prefix_index.json, model_config.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export TFLite (Android)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "print(\"Exporting TFLite models...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Option 1: Standard TFLite with Flex ops\n",
                "print(\"\\n[1] Creating TFLite with Flex ops...\")\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [\n",
                "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
                "        tf.lite.OpsSet.SELECT_TF_OPS\n",
                "    ]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    tflite_path = f'{MODEL_DIR}/gru_model.tflite'\n",
                "    with open(tflite_path, 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    \n",
                "    size_mb = len(tflite_model) / (1024 * 1024)\n",
                "    print(f\"   ‚úì gru_model.tflite ({size_mb:.2f}MB)\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"   ‚úó Error: {str(e)[:100]}\")\n",
                "    tflite_path = None\n",
                "\n",
                "# Option 2: FP16 quantized (smaller)\n",
                "print(\"\\n[2] Creating FP16 quantized TFLite...\")\n",
                "try:\n",
                "    converter_fp16 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter_fp16.target_spec.supported_ops = [\n",
                "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
                "        tf.lite.OpsSet.SELECT_TF_OPS\n",
                "    ]\n",
                "    converter_fp16._experimental_lower_tensor_list_ops = False\n",
                "    converter_fp16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter_fp16.target_spec.supported_types = [tf.float16]\n",
                "    \n",
                "    tflite_fp16 = converter_fp16.convert()\n",
                "    fp16_path = f'{MODEL_DIR}/gru_model_fp16.tflite'\n",
                "    with open(fp16_path, 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    \n",
                "    size_mb = len(tflite_fp16) / (1024 * 1024)\n",
                "    print(f\"   ‚úì gru_model_fp16.tflite ({size_mb:.2f}MB)\")\n",
                "    tflite_path = fp16_path\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"   ‚úó FP16 error: {str(e)[:100]}\")\n",
                "\n",
                "# Benchmark\n",
                "print(\"\\n[3] Running latency benchmark...\")\n",
                "if tflite_path:\n",
                "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
                "    interpreter.allocate_tensors()\n",
                "    input_details = interpreter.get_input_details()[0]\n",
                "    \n",
                "    for _ in range(10):\n",
                "        test_input = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
                "        interpreter.set_tensor(input_details['index'], test_input)\n",
                "        interpreter.invoke()\n",
                "    \n",
                "    latencies = []\n",
                "    for _ in range(50):\n",
                "        start = time.time()\n",
                "        interpreter.set_tensor(input_details['index'], test_input)\n",
                "        interpreter.invoke()\n",
                "        latencies.append((time.time() - start) * 1000)\n",
                "    \n",
                "    print(f\"   ‚úì Latency: avg={np.mean(latencies):.2f}ms, min={np.min(latencies):.2f}ms\")\n",
                "\n",
                "print(\"\\nNOTE: Android needs TensorFlow Lite Flex delegate.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export CoreML Weights (iOS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "print(\"Exporting weights for CoreML conversion...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Export weights\n",
                "weights_list = model.get_weights()\n",
                "weights_path = f'{MODEL_DIR}/gru_weights.npz'\n",
                "np.savez(weights_path, *weights_list)\n",
                "\n",
                "print(f\"‚úì gru_weights.npz ({len(weights_list)} arrays)\")\n",
                "for i, w in enumerate(weights_list):\n",
                "    print(f\"   Weight {i}: {w.shape}\")\n",
                "\n",
                "print(f\"\\n‚Üí Run on Mac: python scripts/convert_to_coreml.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export Mobile Resources (+ Emoji)\n",
                "\n",
                "Export optimized data structures for iOS/Android:\n",
                "- `word_to_index.json` - Word to index mapping\n",
                "- `index_to_word.json` - Index to word mapping\n",
                "- `prefix_index.json` - Prefix ‚Üí word indices for smart suggestions\n",
                "- `emoji_suggestions.json` - Word ‚Üí emoji associations („ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí üôè)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "print(\"Exporting mobile resources (with emoji)...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# 1. Export index_to_word\n",
                "print(\"\\n[1/5] Exporting index_to_word...\")\n",
                "path = f'{MODEL_DIR}/index_to_word.json'\n",
                "i2w_str_keys = {str(k): v for k, v in index_to_word.items()}\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(i2w_str_keys, f, ensure_ascii=False, separators=(',', ':'))\n",
                "size_kb = os.path.getsize(path) / 1024\n",
                "print(f\"   ‚úì index_to_word.json ({len(index_to_word):,} words, {size_kb:.1f}KB)\")\n",
                "\n",
                "# 2. Export prefix_index\n",
                "print(\"\\n[2/5] Verifying prefix_index...\")\n",
                "path = f'{MODEL_DIR}/prefix_index.json'\n",
                "size_kb = os.path.getsize(path) / 1024\n",
                "print(f\"   ‚úì prefix_index.json ({len(prefix_index):,} prefixes, {size_kb:.1f}KB)\")\n",
                "\n",
                "# 3. Build common phrase completions\n",
                "print(\"\\n[3/5] Building phrase suggestions...\")\n",
                "word_pairs = defaultdict(Counter)\n",
                "for seq in all_sequences[:10000]:\n",
                "    for i in range(len(seq) - 1):\n",
                "        word_pairs[seq[i]][seq[i+1]] += 1\n",
                "\n",
                "phrase_suggestions = {}\n",
                "for word_idx, next_counts in word_pairs.items():\n",
                "    if word_idx < 4:\n",
                "        continue\n",
                "    word = index_to_word.get(word_idx)\n",
                "    if not word:\n",
                "        continue\n",
                "    top_next = next_counts.most_common(10)\n",
                "    phrase_suggestions[word] = [next_idx for next_idx, count in top_next]\n",
                "\n",
                "path = f'{MODEL_DIR}/phrase_suggestions.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(phrase_suggestions, f, ensure_ascii=False, separators=(',', ':'))\n",
                "size_kb = os.path.getsize(path) / 1024\n",
                "print(f\"   ‚úì phrase_suggestions.json ({len(phrase_suggestions):,} words, {size_kb:.1f}KB)\")\n",
                "\n",
                "# 4. Build word ‚Üí emoji associations\n",
                "print(\"\\n[4/5] Building word‚Üíemoji associations...\")\n",
                "word_emoji_map = defaultdict(Counter)\n",
                "for seq in all_sequences[:10000]:\n",
                "    for i in range(len(seq) - 1):\n",
                "        word = index_to_word.get(seq[i])\n",
                "        next_token = index_to_word.get(seq[i+1])\n",
                "        if word and next_token and EMOJI_PATTERN.match(next_token):\n",
                "            word_emoji_map[word][next_token] += 1\n",
                "\n",
                "emoji_suggestions = {}\n",
                "for word, emoji_counts in word_emoji_map.items():\n",
                "    if emoji_counts:\n",
                "        emoji_suggestions[word] = [emoji for emoji, count in emoji_counts.most_common(5)]\n",
                "\n",
                "path = f'{MODEL_DIR}/emoji_suggestions.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(emoji_suggestions, f, ensure_ascii=False, separators=(',', ':'))\n",
                "size_kb = os.path.getsize(path) / 1024\n",
                "print(f\"   ‚úì emoji_suggestions.json ({len(emoji_suggestions):,} word‚Üíemoji pairs, {size_kb:.1f}KB)\")\n",
                "\n",
                "# Show sample emoji associations\n",
                "print(\"\\n   Sample word‚Üíemoji:\")\n",
                "for word, emojis in list(emoji_suggestions.items())[:5]:\n",
                "    print(f\"     '{word}' ‚Üí {emojis}\")\n",
                "\n",
                "# 5. Export Japanese keyboard layout\n",
                "print(\"\\n[5/5] Exporting keyboard layout...\")\n",
                "JAPANESE_KEYBOARD = {\n",
                "    '„ÅÇ': '„ÅÑ„ÅÜ„Åà„Åä', '„Åã': '„Åç„Åè„Åë„Åì', '„Åï': '„Åó„Åô„Åõ„Åù',\n",
                "    '„Åü': '„Å°„Å§„Å¶„Å®', '„Å™': '„Å´„Å¨„Å≠„ÅÆ', '„ÅØ': '„Å≤„Åµ„Å∏„Åª',\n",
                "    '„Åæ': '„Åø„ÇÄ„ÇÅ„ÇÇ', '„ÇÑ': '„ÇÜ„Çà', '„Çâ': '„Çä„Çã„Çå„Çç',\n",
                "    '„Çè': '„Çí„Çì„Éº', '„Åå': '„Åé„Åê„Åí„Åî', '„Åñ': '„Åò„Åö„Åú„Åû',\n",
                "    '„Å†': '„Å¢„Å•„Åß„Å©', '„Å∞': '„Å≥„Å∂„Åπ„Åº', '„Å±': '„Å¥„Å∑„Å∫„ÅΩ'\n",
                "}\n",
                "path = f'{MODEL_DIR}/japanese_keyboard.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(JAPANESE_KEYBOARD, f, ensure_ascii=False)\n",
                "print(\"   ‚úì japanese_keyboard.json\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EXPORT COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nFiles in {MODEL_DIR}/:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    size = os.path.getsize(f'{MODEL_DIR}/{f}') / 1024\n",
                "    print(f\"   {f}: {size:.1f}KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Verification Test\n",
                "\n",
                "Test the model with real Japanese input examples + emoji."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION TEST - Smart Word + Emoji Suggestions\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load exported mappings\n",
                "with open(f'{MODEL_DIR}/word_to_index.json', 'r', encoding='utf-8') as f:\n",
                "    loaded_w2i = json.load(f)\n",
                "with open(f'{MODEL_DIR}/index_to_word.json', 'r', encoding='utf-8') as f:\n",
                "    loaded_i2w = {int(k): v for k, v in json.load(f).items()}\n",
                "with open(f'{MODEL_DIR}/prefix_index.json', 'r', encoding='utf-8') as f:\n",
                "    loaded_prefix = json.load(f)\n",
                "with open(f'{MODEL_DIR}/emoji_suggestions.json', 'r', encoding='utf-8') as f:\n",
                "    loaded_emoji = json.load(f)\n",
                "\n",
                "def get_prefix_suggestions(prefix, top_k=5):\n",
                "    \"\"\"Get word suggestions for a prefix.\"\"\"\n",
                "    if prefix not in loaded_prefix:\n",
                "        return []\n",
                "    indices = loaded_prefix[prefix][:top_k]\n",
                "    return [(loaded_i2w.get(idx, '?'), 100 / (i + 1)) for i, idx in enumerate(indices)]\n",
                "\n",
                "def get_emoji_suggestions(word, top_k=5):\n",
                "    \"\"\"Get emoji suggestions for a word.\"\"\"\n",
                "    if word not in loaded_emoji:\n",
                "        return []\n",
                "    return loaded_emoji[word][:top_k]\n",
                "\n",
                "def predict_next_word(context_words, top_k=5):\n",
                "    \"\"\"Predict next word using the model.\"\"\"\n",
                "    seq = [loaded_w2i.get(w, 1) for w in context_words]\n",
                "    seq = pad_sequences([seq], maxlen=SEQUENCE_LENGTH, padding='pre')\n",
                "    \n",
                "    preds = model.predict(seq, verbose=0)[0]\n",
                "    top_idx = np.argsort(preds)[-top_k:][::-1]\n",
                "    \n",
                "    results = []\n",
                "    for idx in top_idx:\n",
                "        if idx in loaded_i2w and idx >= 4:\n",
                "            results.append((loaded_i2w[idx], preds[idx] * 100))\n",
                "    return results\n",
                "\n",
                "# Test 1: Prefix completion\n",
                "print(\"\\nüìù TEST 1: Prefix Completion\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "prefix_tests = ['„Åî', '„Åî„Åñ', '„Åî„Åñ„ÅÑ„Åæ', '„ÅÇ„Çä', '„ÅÇ„Çä„Åå„Å®', '„Åä„ÅØ', '„Åì„Çì„Å´„Å°']\n",
                "for prefix in prefix_tests:\n",
                "    results = get_prefix_suggestions(prefix, top_k=5)\n",
                "    print(f\"  '{prefix}' ‚Üí {[r[0] for r in results] if results else '(no matches)'}\")\n",
                "\n",
                "# Test 2: Word ‚Üí Emoji suggestions (NEW!)\n",
                "print(\"\\n\\nüòä TEST 2: Word ‚Üí Emoji Suggestions\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "emoji_tests = ['„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åä„ÇÅ„Åß„Å®„ÅÜ', '„Åã„Çè„ÅÑ„ÅÑ', '„Åü„ÅÆ„Åó„ÅÑ', 'Á¨ë']\n",
                "for word in emoji_tests:\n",
                "    emojis = get_emoji_suggestions(word)\n",
                "    print(f\"  '{word}' ‚Üí {emojis if emojis else '(no emoji associations)'}\")\n",
                "\n",
                "# Test 3: Combined flow (prefix ‚Üí word ‚Üí emoji)\n",
                "print(\"\\n\\nüîÑ TEST 3: Complete Flow (Prefix ‚Üí Word ‚Üí Emoji)\")\n",
                "print(\"-\"*40)\n",
                "\n",
                "flow_tests = ['„ÅÇ„Çä„Åå', '„Åä„ÇÅ„Åß„Å®', '„Åã„Çè„ÅÑ']\n",
                "for prefix in flow_tests:\n",
                "    print(f\"\\n  User types: '{prefix}'\")\n",
                "    words = get_prefix_suggestions(prefix, top_k=3)\n",
                "    if words:\n",
                "        word = words[0][0]\n",
                "        print(f\"  ‚Üí Prefix suggestions: {[w[0] for w in words]}\")\n",
                "        print(f\"  ‚Üí User selects: '{word}'\")\n",
                "        emojis = get_emoji_suggestions(word)\n",
                "        if emojis:\n",
                "            print(f\"  ‚Üí Emoji suggestions: {emojis}\")\n",
                "        else:\n",
                "            print(f\"  ‚Üí No emoji suggestions\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
                "print(\"   - Prefix index provides instant word completion\")\n",
                "print(\"   - Emoji suggestions from dataset associations\")\n",
                "print(\"   - Complete flow: prefix ‚Üí word ‚Üí emoji\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage Guide\n",
                "\n",
                "### Mobile Integration Flow\n",
                "\n",
                "```\n",
                "User types: \"„ÅÇ„Çä„Åå\"\n",
                "        ‚Üì\n",
                "1. Check prefix_index.json\n",
                "   ‚Üí [\"„ÅÇ„Çä„Åå„Å®„ÅÜ\", \"„ÅÇ„Çä„Åå„Åü„ÅÑ\"...]\n",
                "        ‚Üì\n",
                "2. User selects \"„ÅÇ„Çä„Åå„Å®„ÅÜ\"\n",
                "        ‚Üì\n",
                "3. Check emoji_suggestions.json\n",
                "   ‚Üí [\"üôè\", \"üòä\"...]\n",
                "   Check phrase_suggestions.json\n",
                "   ‚Üí [\"„Åî„Åñ„ÅÑ„Åæ„Åô\", \"„Å≠\"...]\n",
                "        ‚Üì\n",
                "4. Show combined: [üôè, „Åî„Åñ„ÅÑ„Åæ„Åô, üòä, „Å≠]\n",
                "```\n",
                "\n",
                "### Files for Mobile\n",
                "- `prefix_index.json` - Fast prefix completion\n",
                "- `emoji_suggestions.json` - Word ‚Üí emoji associations\n",
                "- `phrase_suggestions.json` - Word ‚Üí next word predictions\n",
                "- `gru_model.tflite` - ML model for context-aware predictions\n",
                "- `word_to_index.json` - Vocabulary for tokenization\n",
                "- `index_to_word.json` - Decode predictions to words"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
