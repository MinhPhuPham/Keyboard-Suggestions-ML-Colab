{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Enhanced GRU Japanese Keyboard\n",
                "\n",
                "**Two Functions:**\n",
                "1. **Kana‚ÜíKanji**: „Åä„Åõ ‚Üí [„Åä‰∏ñË©±, „Åä„Åõ„Å°]\n",
                "2. **Next Phrase**: „Åä‰∏ñË©± ‚Üí [„Å´„Å™„Å£„Å¶„Åä„Çä„Åæ„Åô, „Åî„Åñ„ÅÑ„Åæ„Åô]\n",
                "\n",
                "**Architecture:** Bi-GRU + Luong Attention"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_japanese_enhanced\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"‚úì Model: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets pandas numpy tqdm fugashi unidic-lite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True  # False for full training\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 3\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 200000\n",
                "else:\n",
                "    NUM_EPOCHS = 20\n",
                "    BATCH_SIZE = 256\n",
                "    MAX_SAMPLES = 300000\n",
                "\n",
                "# Vocabulary\n",
                "CHAR_VOCAB_SIZE = 3000\n",
                "PHRASE_VOCAB_SIZE = 5000\n",
                "MAX_SEQ_LENGTH = 50\n",
                "\n",
                "# Model\n",
                "EMBEDDING_DIM = 128\n",
                "GRU_UNITS = 256\n",
                "\n",
                "# Special tokens\n",
                "SPECIAL_TOKENS = ['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<KANA>', '<KANJI>', '<CTX>', '<NEXT>']\n",
                "\n",
                "print(f\"Config: epochs={NUM_EPOCHS}, samples={MAX_SAMPLES:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "except:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "\n",
                "print(f\"‚úì Loaded {len(dataset):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tokenizers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import fugashi\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "tagger = fugashi.Tagger()\n",
                "\n",
                "def tokenize_words(text):\n",
                "    result = []\n",
                "    for t in tagger(text):\n",
                "        # Keep words AND emojis\n",
                "        if t.feature.pos1 not in ['Á©∫ÁôΩ']:  # Only filter whitespace\n",
                "            result.append(t.surface)\n",
                "    return result\n",
                "\n",
                "def tokenize_chars(text):\n",
                "    \"\"\"Char-level: „Ç¢„É™„Ç¨„Éà„Ç¶ ‚Üí [„Ç¢, „É™, „Ç¨, „Éà, „Ç¶]\"\"\"\n",
                "    return list(text.replace(' ', '').replace('\\n', ''))\n",
                "\n",
                "# Test\n",
                "print(f\"Words: {tokenize_words('ÊúâÈõ£„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô')}\")\n",
                "print(f\"Chars: {tokenize_chars('„Ç¢„É™„Ç¨„Éà„Ç¶')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Vocabulary (Chars + Phrases)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "print(\"Building vocabularies...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "char_counts = Counter()\n",
                "phrase_counts = Counter()  # word ‚Üí next_phrase patterns\n",
                "word_phrase_map = defaultdict(Counter)  # For exporting\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Extracting patterns\"):\n",
                "    kana = item.get('input', '')\n",
                "    kanji = item.get('output', '')\n",
                "    \n",
                "    # Chars from kana and kanji\n",
                "    char_counts.update(tokenize_chars(kana))\n",
                "    char_counts.update(tokenize_chars(kanji))\n",
                "    \n",
                "    # Extract word ‚Üí phrase patterns\n",
                "    words = tokenize_words(kanji)\n",
                "    for i in range(len(words) - 1):\n",
                "        word = words[i]\n",
                "        # Next phrase = 1-3 following words\n",
                "        for phrase_len in [1, 2, 3]:\n",
                "            if i + phrase_len < len(words):\n",
                "                phrase = ''.join(words[i+1:i+1+phrase_len])\n",
                "                if 1 < len(phrase) <= 15:  # Valid phrase length\n",
                "                    phrase_counts[phrase] += 1\n",
                "                    word_phrase_map[word][phrase] += 1\n",
                "\n",
                "print(f\"\\n‚úì {len(char_counts):,} unique chars\")\n",
                "print(f\"‚úì {len(phrase_counts):,} unique phrases\")\n",
                "print(f\"\\nTop phrases: {[p for p, c in phrase_counts.most_common(15)]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build UNIFIED vocab: special + chars + phrases\n",
                "\n",
                "token_to_idx = {}\n",
                "\n",
                "# 1. Special tokens\n",
                "for i, tok in enumerate(SPECIAL_TOKENS):\n",
                "    token_to_idx[tok] = i\n",
                "\n",
                "# 2. Characters (for kana-kanji)\n",
                "for char, _ in char_counts.most_common(CHAR_VOCAB_SIZE):\n",
                "    if char not in token_to_idx:\n",
                "        token_to_idx[char] = len(token_to_idx)\n",
                "\n",
                "# 3. Phrases (for next-phrase prediction)\n",
                "for phrase, _ in phrase_counts.most_common(PHRASE_VOCAB_SIZE):\n",
                "    if phrase not in token_to_idx and len(phrase) > 1:\n",
                "        token_to_idx[phrase] = len(token_to_idx)\n",
                "\n",
                "idx_to_token = {v: k for k, v in token_to_idx.items()}\n",
                "vocab_size = len(token_to_idx)\n",
                "\n",
                "# Phrase-only vocab for predictions\n",
                "phrase_vocab = [p for p, c in phrase_counts.most_common(PHRASE_VOCAB_SIZE) if len(p) > 1]\n",
                "\n",
                "print(f\"\\n‚úì Unified vocab: {vocab_size:,}\")\n",
                "print(f\"‚úì Phrase vocab: {len(phrase_vocab):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create Training Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "print(\"Creating training data...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def encode(tokens, max_len=MAX_SEQ_LENGTH):\n",
                "    ids = [token_to_idx.get(t, token_to_idx['<UNK>']) for t in tokens]\n",
                "    if len(ids) < max_len:\n",
                "        ids = ids + [token_to_idx['<PAD>']] * (max_len - len(ids))\n",
                "    return ids[:max_len]\n",
                "\n",
                "X_data = []\n",
                "y_data = []\n",
                "\n",
                "# ============================================================\n",
                "# Task 1: Kana-Kanji (char-level)\n",
                "# „Åä„Åõ„Çè ‚Üí „Åä‰∏ñË©±\n",
                "# ============================================================\n",
                "print(\"\\n[Task 1] Kana‚ÜíKanji...\")\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Kana‚ÜíKanji\"):\n",
                "    kana = item.get('input', '').strip()\n",
                "    kanji = item.get('output', '').strip()\n",
                "    \n",
                "    if not kana or not kanji or len(kana) > 30 or len(kanji) > 30:\n",
                "        continue\n",
                "    \n",
                "    input_tokens = ['<KANA>'] + tokenize_chars(kana) + ['<KANJI>']\n",
                "    target_tokens = tokenize_chars(kanji) + ['<EOS>']\n",
                "    \n",
                "    for i in range(min(len(target_tokens), 25)):\n",
                "        ctx = input_tokens + target_tokens[:i]\n",
                "        target = target_tokens[i]\n",
                "        X_data.append(encode(ctx))\n",
                "        y_data.append(token_to_idx.get(target, token_to_idx['<UNK>']))\n",
                "\n",
                "task1_count = len(X_data)\n",
                "print(f\"‚úì Task 1: {task1_count:,} samples\")\n",
                "\n",
                "# ============================================================\n",
                "# Task 2: Word ‚Üí Next Phrase (phrase-level)\n",
                "# „Åä‰∏ñË©± ‚Üí „Å´„Å™„Å£„Å¶„Åä„Çä„Åæ„Åô\n",
                "# „ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô\n",
                "# ============================================================\n",
                "print(\"\\n[Task 2] Word‚ÜíPhrase...\")\n",
                "\n",
                "for item in tqdm(dataset, desc=\"Word‚ÜíPhrase\"):\n",
                "    kanji = item.get('output', '').strip()\n",
                "    if not kanji:\n",
                "        continue\n",
                "    \n",
                "    words = tokenize_words(kanji)\n",
                "    if len(words) < 2:\n",
                "        continue\n",
                "    \n",
                "    for i in range(len(words) - 1):\n",
                "        word = words[i]\n",
                "        \n",
                "        # Try different phrase lengths (1-3 words)\n",
                "        for phrase_len in [1, 2, 3]:\n",
                "            if i + phrase_len >= len(words):\n",
                "                break\n",
                "            \n",
                "            phrase = ''.join(words[i+1:i+1+phrase_len])\n",
                "            \n",
                "            # Only train on phrases in vocabulary\n",
                "            if phrase in token_to_idx and len(phrase) > 1:\n",
                "                # Input: <CTX> + context words + <NEXT>\n",
                "                context = words[max(0, i-2):i+1]  # 2-3 words context\n",
                "                input_tokens = ['<CTX>'] + context + ['<NEXT>']\n",
                "                \n",
                "                X_data.append(encode(input_tokens))\n",
                "                y_data.append(token_to_idx[phrase])  # COMPLETE PHRASE\n",
                "\n",
                "task2_count = len(X_data) - task1_count\n",
                "print(f\"‚úì Task 2: {task2_count:,} samples\")\n",
                "print(f\"\\n‚úì Total: {len(X_data):,} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Shuffle and split\n",
                "X_data = np.array(X_data)\n",
                "y_data = np.array(y_data)\n",
                "\n",
                "indices = np.random.permutation(len(X_data))\n",
                "X_data = X_data[indices]\n",
                "y_data = y_data[indices]\n",
                "\n",
                "split = int(len(X_data) * 0.9)\n",
                "X_train, X_val = X_data[:split], X_data[split:]\n",
                "y_train, y_val = y_data[:split], y_data[split:]\n",
                "\n",
                "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"‚úì Train: {len(X_train):,}, Val: {len(X_val):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Build Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import mixed_precision\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, Embedding, GRU, Dense, Dropout,\n",
                "    Bidirectional, Attention, Concatenate, LayerNormalization\n",
                ")\n",
                "\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "inputs = Input(shape=(MAX_SEQ_LENGTH,), name='input')\n",
                "x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "\n",
                "# Bi-directional GRU\n",
                "encoder_out = Bidirectional(\n",
                "    GRU(GRU_UNITS, return_sequences=True, dropout=0.2),\n",
                "    name='bi_encoder'\n",
                ")(x)\n",
                "\n",
                "# Luong Attention\n",
                "attention_out = Attention(use_scale=True, name='attention')([encoder_out, encoder_out])\n",
                "\n",
                "# Combine\n",
                "combined = Concatenate()([encoder_out, attention_out])\n",
                "combined = LayerNormalization()(combined)\n",
                "\n",
                "# Decoder\n",
                "decoder_out = GRU(GRU_UNITS, name='decoder')(combined)\n",
                "decoder_out = Dropout(0.3)(decoder_out)\n",
                "\n",
                "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(decoder_out)\n",
                "\n",
                "model = Model(inputs, outputs, name='japanese_keyboard_gru')\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "model.summary()\n",
                "print(f\"\\n‚úì Parameters: {model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=NUM_EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=callbacks,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend()\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend()\n",
                "plt.savefig(f'{MODEL_DIR}/training.png')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n‚úì Val Acc: {history.history['val_accuracy'][-1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/token_to_idx.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(token_to_idx, f, ensure_ascii=False)\n",
                "\n",
                "with open(f'{MODEL_DIR}/idx_to_token.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in idx_to_token.items()}, f, ensure_ascii=False)\n",
                "\n",
                "# Word ‚Üí Phrase suggestions (for fast lookup)\n",
                "word_phrase_suggestions = {}\n",
                "for word, phrase_counter in word_phrase_map.items():\n",
                "    top_phrases = [p for p, c in phrase_counter.most_common(10) if p in token_to_idx]\n",
                "    if top_phrases:\n",
                "        word_phrase_suggestions[word] = top_phrases\n",
                "\n",
                "with open(f'{MODEL_DIR}/word_phrase_suggestions.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_phrase_suggestions, f, ensure_ascii=False)\n",
                "\n",
                "print(f\"‚úì word_phrase_suggestions.json ({len(word_phrase_suggestions):,} words)\")\n",
                "\n",
                "# Config\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'max_seq_length': MAX_SEQ_LENGTH,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'gru_units': GRU_UNITS,\n",
                "    'architecture': 'BiGRU_LuongAttention',\n",
                "    'tasks': ['kana_kanji', 'next_phrase'],\n",
                "    'special_tokens': SPECIAL_TOKENS\n",
                "}\n",
                "with open(f'{MODEL_DIR}/config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"‚úì Saved all resources\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export TFLite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Exporting TFLite...\")\n",
                "\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model.tflite', 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"‚úì model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # FP16\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    tflite_fp16 = converter.convert()\n",
                "    with open(f'{MODEL_DIR}/model_fp16.tflite', 'wb') as f:\n",
                "        f.write(tflite_fp16)\n",
                "    print(f\"‚úì model_fp16.tflite ({len(tflite_fp16)/(1024*1024):.2f}MB)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö† Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION TEST\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def generate_chars(input_tokens, max_len=20):\n",
                "    \"\"\"Generate characters (for kana-kanji).\"\"\"\n",
                "    current = encode(input_tokens)\n",
                "    generated = []\n",
                "    \n",
                "    for _ in range(max_len):\n",
                "        probs = model.predict(np.array([current]), verbose=0)[0]\n",
                "        next_idx = np.argmax(probs)\n",
                "        next_token = idx_to_token.get(next_idx, '<UNK>')\n",
                "        \n",
                "        if next_token in ['<EOS>', '<PAD>']:\n",
                "            break\n",
                "        generated.append(next_token)\n",
                "        current = encode(input_tokens + generated)\n",
                "    \n",
                "    return ''.join(generated)\n",
                "\n",
                "def predict_next_phrases(context_words, top_k=5):\n",
                "    \"\"\"Predict next COMPLETE PHRASES.\"\"\"\n",
                "    input_tokens = ['<CTX>'] + context_words[-3:] + ['<NEXT>']\n",
                "    current = encode(input_tokens)\n",
                "    \n",
                "    probs = model.predict(np.array([current]), verbose=0)[0]\n",
                "    top_indices = np.argsort(probs)[-100:][::-1]\n",
                "    \n",
                "    # Filter: only return phrases (length > 1), not chars\n",
                "    predictions = []\n",
                "    for idx in top_indices:\n",
                "        token = idx_to_token.get(idx, '')\n",
                "        if token and len(token) > 1 and token not in SPECIAL_TOKENS:\n",
                "            predictions.append(token)\n",
                "        if len(predictions) >= top_k:\n",
                "            break\n",
                "    \n",
                "    return predictions\n",
                "\n",
                "# ============================================================\n",
                "# Test 1: Kana ‚Üí Kanji\n",
                "# ============================================================\n",
                "print(\"\\nüìù Kana‚ÜíKanji Conversion\")\n",
                "print(\"-\" * 40)\n",
                "tests = ['„Ç¢„É™„Ç¨„Éà„Ç¶', '„Ç¥„Ç∂„Ç§„Éû„Çπ', '„Ç™„Çª„ÉØ']\n",
                "for kana in tests:\n",
                "    inp = ['<KANA>'] + tokenize_chars(kana) + ['<KANJI>']\n",
                "    result = generate_chars(inp, max_len=len(kana)*2)\n",
                "    print(f\"  {kana} ‚Üí {result}\")\n",
                "\n",
                "# ============================================================\n",
                "# Test 2: Word ‚Üí Next Phrase\n",
                "# ============================================================\n",
                "print(\"\\nüìù Next Phrase Prediction\")\n",
                "print(\"-\" * 40)\n",
                "tests = [\n",
                "    ['„ÅÇ„Çä„Åå„Å®„ÅÜ'],         # ‚Üí „Åî„Åñ„ÅÑ„Åæ„Åô, „Åî„Åñ„ÅÑ„Åæ„Åó„Åü\n",
                "    ['„Åä‰∏ñË©±'],             # ‚Üí „Å´„Å™„Å£„Å¶„Åä„Çä„Åæ„Åô\n",
                "    ['Ë°å„Åç'],               # ‚Üí „Åæ„Åô, „Åü„ÅÑ„Åß„Åô\n",
                "    ['Áî≥„ÅóË®≥'],             # ‚Üí „ÅÇ„Çä„Åæ„Åõ„Çì, „Åî„Åñ„ÅÑ„Åæ„Åõ„Çì\n",
                "    ['„Åù„ÅÜ'],               # ‚Üí „Åß„Åô„Å≠, ÊÄù„ÅÑ„Åæ„Åô\n",
                "]\n",
                "for ctx in tests:\n",
                "    result = predict_next_phrases(ctx)\n",
                "    print(f\"  {''.join(ctx)} ‚Üí {result}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show word_phrase_suggestions examples\n",
                "print(\"\\nüìö Sample Word‚ÜíPhrase Suggestions:\")\n",
                "sample_words = ['„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åä‰∏ñË©±', 'Ë°å„Åç', 'Áî≥„ÅóË®≥', '„Åù„ÅÜ', '‰ªäÊó•', 'Êó•Êú¨']\n",
                "for word in sample_words:\n",
                "    phrases = word_phrase_suggestions.get(word, [])\n",
                "    if phrases:\n",
                "        print(f\"  {word} ‚Üí {phrases[:5]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List exports\n",
                "print(\"\\nExported files:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    path = f'{MODEL_DIR}/{f}'\n",
                "    if os.path.isfile(path):\n",
                "        size = os.path.getsize(path)\n",
                "        if size > 1024*1024:\n",
                "            print(f\"  {f}: {size/(1024*1024):.1f} MB\")\n",
                "        else:\n",
                "            print(f\"  {f}: {size/1024:.1f} KB\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
