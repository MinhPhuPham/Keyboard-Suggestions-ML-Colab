{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "japanese_gru_header"
            },
            "source": [
                "# GRU Japanese Keyboard Model - Dual Mode\n",
                "\n",
                "**Architecture:** Kana-normalized model with separate kanji conversion\n",
                "\n",
                "## Features\n",
                "1. **Kana‚ÜíKanji Conversion**: „ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí [Êúâ„ÇäÈõ£„ÅÜ, ÊúâÈõ£„ÅÜ]\n",
                "2. **Next Word Prediction**: „ÅÇ„Çä„Åå„Å®„ÅÜ ‚Üí [„Åî„Åñ„ÅÑ„Åæ„Åô, „Å≠, üôè]\n",
                "3. **Prefix Completion**: „ÅÇ„Çä„Åå ‚Üí [„ÅÇ„Çä„Åå„Å®„ÅÜ, „ÅÇ„Çä„Åå„Åü„ÅÑ]\n",
                "4. **Emoji Suggestions**: From dataset context\n",
                "\n",
                "## Key Design\n",
                "- Model trains on **kana only** (no kanji mixing)\n",
                "- Separate **kana‚Üíkanji index** for display conversion\n",
                "- Consistent predictions regardless of displayed script\n",
                "\n",
                "---\n",
                "**Instructions:**\n",
                "1. Runtime ‚Üí GPU (T4)\n",
                "2. Set `TESTING_MODE = True` for quick test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
                "MODEL_DIR = f\"{DRIVE_DIR}/models/gru_keyboard_japanese\"\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"‚úì Model directory: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow keras datasets pandas numpy scikit-learn tqdm regex"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TESTING_MODE = True  # ‚Üê Change to False for full training\n",
                "\n",
                "if TESTING_MODE:\n",
                "    NUM_EPOCHS = 5\n",
                "    BATCH_SIZE = 256\n",
                "    VOCAB_SIZE_LIMIT = 6000\n",
                "    SEQUENCE_LENGTH = 15\n",
                "    MAX_SAMPLES = 200000\n",
                "else:\n",
                "    NUM_EPOCHS = 20\n",
                "    BATCH_SIZE = 256\n",
                "    VOCAB_SIZE_LIMIT = 6000\n",
                "    SEQUENCE_LENGTH = 15\n",
                "    MAX_SAMPLES = 300000\n",
                "\n",
                "EMBEDDING_DIM = 128\n",
                "GRU_UNITS = 256\n",
                "\n",
                "PAD_TOKEN = '<PAD>'\n",
                "UNK_TOKEN = '<UNK>'\n",
                "BOS_TOKEN = '<BOS>'\n",
                "EOS_TOKEN = '<EOS>'\n",
                "\n",
                "print(f\"Config: vocab={VOCAB_SIZE_LIMIT:,}, epochs={NUM_EPOCHS}, samples={MAX_SAMPLES:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import re\n",
                "import regex\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "print(\"Loading zenz-v2.5-dataset...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "try:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        data_files=\"train_wikipedia.jsonl\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "    print(f\"‚úì Loaded {len(dataset):,} samples (Wikipedia)\")\n",
                "except:\n",
                "    dataset = load_dataset(\n",
                "        \"Miwa-Keita/zenz-v2.5-dataset\",\n",
                "        split=f\"train[:{MAX_SAMPLES}]\"\n",
                "    )\n",
                "    print(f\"‚úì Loaded {len(dataset):,} samples\")\n",
                "\n",
                "# Show samples\n",
                "print(\"\\nSamples (input=kana, output=kanji):\")\n",
                "for i in range(3):\n",
                "    print(f\"  {dataset[i]['input'][:25]} ‚Üí {dataset[i]['output'][:25]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Build Kana Vocabulary + Kana‚ÜíKanji Index\n",
                "\n",
                "**Key:** Train model on KANA only, use separate kanji lookup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import regex\n",
                "from collections import Counter, defaultdict\n",
                "\n",
                "print(\"Building kana vocabulary + kanji index...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Emoji support\n",
                "EMOJI_PATTERN = regex.compile(r'[\\p{Emoji_Presentation}\\p{Extended_Pictographic}]')\n",
                "\n",
                "def is_emoji(char):\n",
                "    return bool(EMOJI_PATTERN.match(char))\n",
                "\n",
                "def extract_emojis(text):\n",
                "    return EMOJI_PATTERN.findall(text)\n",
                "\n",
                "def segment_japanese(text):\n",
                "    \"\"\"Segment Japanese text into words.\"\"\"\n",
                "    particles = r'(„ÅØ|„Åå|„Çí|„Å´|„Åß|„Å®|„ÅÆ|„Åã„Çâ|„Åæ„Åß|„Çà„Çä|„Å∏|„ÇÑ|„ÇÇ|„Åã|„Å≠|„Çà|„Çè|„Å™|„Çâ|„Åó|„Å¶|„Åü|„Å†|„Åß„Åô|„Åæ„Åô)'\n",
                "    segments = re.split(r'[„ÄÇ„ÄÅÔºÅÔºü\\s\\n„Éª„Äå„Äç„Äé„ÄèÔºàÔºâ„Äê„Äë]', text)\n",
                "    \n",
                "    words = []\n",
                "    for seg in segments:\n",
                "        if not seg:\n",
                "            continue\n",
                "        emojis = extract_emojis(seg)\n",
                "        text_only = EMOJI_PATTERN.sub('', seg)\n",
                "        \n",
                "        if text_only:\n",
                "            if len(text_only) <= 8:\n",
                "                words.append(text_only)\n",
                "            else:\n",
                "                parts = re.split(particles, text_only)\n",
                "                words.extend([p for p in parts if p])\n",
                "        words.extend(emojis)\n",
                "    \n",
                "    return [w for w in words if w and len(w) <= 20]\n",
                "\n",
                "# ============================================================\n",
                "# BUILD KANA VOCABULARY (from INPUT field only)\n",
                "# ============================================================\n",
                "print(\"\\n[1/2] Building kana vocabulary from INPUT field...\")\n",
                "\n",
                "word_counts = Counter()\n",
                "all_kana_texts = []  # For training sequences\n",
                "\n",
                "for item in dataset:\n",
                "    # Use INPUT (kana) for vocabulary\n",
                "    kana_text = item.get('input', '')\n",
                "    if kana_text:\n",
                "        all_kana_texts.append(kana_text)\n",
                "        words = segment_japanese(kana_text)\n",
                "        word_counts.update(words)\n",
                "\n",
                "print(f\"‚úì Found {len(word_counts):,} unique kana words\")\n",
                "\n",
                "# ============================================================\n",
                "# BUILD KANA‚ÜíKANJI CONVERSION INDEX\n",
                "# ============================================================\n",
                "print(\"\\n[2/2] Building kana‚Üíkanji conversion index...\")\n",
                "\n",
                "kana_kanji_counts = defaultdict(Counter)\n",
                "\n",
                "for item in dataset:\n",
                "    kana = item.get('input', '').strip()\n",
                "    kanji = item.get('output', '').strip()\n",
                "    \n",
                "    if kana and kanji and kana != kanji:\n",
                "        # Full phrase mapping\n",
                "        kana_kanji_counts[kana][kanji] += 1\n",
                "        \n",
                "        # Word-level mapping\n",
                "        kana_words = segment_japanese(kana)\n",
                "        kanji_words = segment_japanese(kanji)\n",
                "        \n",
                "        if len(kana_words) == len(kanji_words):\n",
                "            for k, j in zip(kana_words, kanji_words):\n",
                "                if k != j and len(k) > 1:\n",
                "                    kana_kanji_counts[k][j] += 1\n",
                "\n",
                "# Finalize: keep top 5 kanji per kana\n",
                "kana_kanji_index = {}\n",
                "for kana, kanji_counts in kana_kanji_counts.items():\n",
                "    top = [k for k, c in kanji_counts.most_common(5)]\n",
                "    if top:\n",
                "        kana_kanji_index[kana] = top\n",
                "\n",
                "print(f\"‚úì Built {len(kana_kanji_index):,} kana‚Üíkanji mappings\")\n",
                "\n",
                "# Show examples\n",
                "print(\"\\nSample conversions:\")\n",
                "examples = ['„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åî„Åñ„ÅÑ„Åæ„Åô', '„Åä„ÅØ„Çà„ÅÜ', '„Åì„Çì„Å´„Å°„ÅØ', '„Çè„Åü„Åó']\n",
                "for ex in examples:\n",
                "    kanji = kana_kanji_index.get(ex, ['(no conversion)'])\n",
                "    print(f\"  {ex} ‚Üí {kanji}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter valid words and build vocabulary\n",
                "def is_valid_word(word):\n",
                "    if not word or len(word) < 1:\n",
                "        return False\n",
                "    if len(word) <= 2 and EMOJI_PATTERN.match(word):\n",
                "        return True\n",
                "    for char in word:\n",
                "        code = ord(char)\n",
                "        if not (0x3040 <= code <= 0x309F or  # Hiragana\n",
                "                0x30A0 <= code <= 0x30FF or  # Katakana\n",
                "                0x4E00 <= code <= 0x9FFF or  # Kanji (allow some)\n",
                "                0x3400 <= code <= 0x4DBF or\n",
                "                is_emoji(char) or\n",
                "                char in '„Éº„Äú'):\n",
                "            return False\n",
                "    return True\n",
                "\n",
                "valid_words = [(w, c) for w, c in word_counts.most_common() if is_valid_word(w)]\n",
                "valid_words = valid_words[:VOCAB_SIZE_LIMIT - 4]\n",
                "\n",
                "word_to_index = {PAD_TOKEN: 0, UNK_TOKEN: 1, BOS_TOKEN: 2, EOS_TOKEN: 3}\n",
                "for idx, (word, count) in enumerate(valid_words, start=4):\n",
                "    word_to_index[word] = idx\n",
                "\n",
                "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
                "vocab_size = len(word_to_index)\n",
                "\n",
                "print(f\"\\n‚úì Vocabulary size: {vocab_size:,}\")\n",
                "print(f\"\\nTop 15 words:\")\n",
                "for i, (w, c) in enumerate(valid_words[:15], 1):\n",
                "    print(f\"  {i:2d}. '{w}' ({c:,})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build prefix index\n",
                "print(\"Building prefix index...\")\n",
                "\n",
                "prefix_index = defaultdict(list)\n",
                "for word, count in valid_words:\n",
                "    idx = word_to_index[word]\n",
                "    for i in range(1, len(word) + 1):\n",
                "        prefix = word[:i]\n",
                "        prefix_index[prefix].append((count, idx))\n",
                "\n",
                "for prefix in prefix_index:\n",
                "    prefix_index[prefix].sort(reverse=True)\n",
                "    prefix_index[prefix] = [idx for c, idx in prefix_index[prefix][:20]]\n",
                "\n",
                "print(f\"‚úì Prefix index: {len(prefix_index):,} prefixes\")\n",
                "\n",
                "# Test\n",
                "print(\"\\nPrefix tests:\")\n",
                "for p in ['„ÅÇ„Çä', '„ÅÇ„Çä„Åå', '„Åî„Åñ„ÅÑ„Åæ', '„Åä„ÅØ']:\n",
                "    if p in prefix_index:\n",
                "        words = [index_to_word[i] for i in prefix_index[p][:3]]\n",
                "        print(f\"  '{p}' ‚Üí {words}\")\n",
                "    else:\n",
                "        print(f\"  '{p}' ‚Üí (no match)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Training Data (Kana sequences)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "\n",
                "print(\"Creating training sequences from KANA text...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "all_sequences = []\n",
                "for text in all_kana_texts:\n",
                "    words = segment_japanese(text)\n",
                "    seq = [word_to_index.get(w, 1) for w in words]\n",
                "    if len(seq) >= 2:\n",
                "        all_sequences.append(seq)\n",
                "\n",
                "print(f\"‚úì {len(all_sequences):,} sequences\")\n",
                "\n",
                "# Create X, y pairs\n",
                "X_data, y_data = [], []\n",
                "for seq in all_sequences:\n",
                "    for i in range(1, len(seq)):\n",
                "        input_seq = seq[:i][-SEQUENCE_LENGTH:]\n",
                "        X_data.append(input_seq)\n",
                "        y_data.append(seq[i])\n",
                "\n",
                "print(f\"‚úì {len(X_data):,} training pairs\")\n",
                "\n",
                "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_data, maxlen=SEQUENCE_LENGTH, padding='pre')\n",
                "y_array = np.array(y_data)\n",
                "\n",
                "ds = tf.data.Dataset.from_tensor_slices((X_padded, y_array)).shuffle(10000).batch(BATCH_SIZE)\n",
                "\n",
                "total = len(X_data) // BATCH_SIZE\n",
                "val_size = max(1, total // 10)\n",
                "train_ds = ds.take(total - val_size).prefetch(tf.data.AUTOTUNE)\n",
                "val_ds = ds.skip(total - val_size).take(val_size).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "print(f\"‚úì Train: {total - val_size} batches, Val: {val_size} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build & Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras import mixed_precision\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
                "from tensorflow.keras.optimizers import AdamW\n",
                "\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "inputs = Input(shape=(SEQUENCE_LENGTH,), name='input')\n",
                "x = Embedding(vocab_size, EMBEDDING_DIM, name='embedding')(inputs)\n",
                "x = GRU(GRU_UNITS, dropout=0.2, recurrent_dropout=0.2, name='gru')(x)\n",
                "x = Dropout(0.3)(x)\n",
                "outputs = Dense(vocab_size, activation='softmax', dtype='float32', name='output')(x)\n",
                "\n",
                "model = Model(inputs=inputs, outputs=outputs, name='gru_japanese_kana')\n",
                "model.compile(\n",
                "    optimizer=AdamW(learning_rate=1e-3, weight_decay=1e-4),\n",
                "    loss='sparse_categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "callbacks = [\n",
                "    ModelCheckpoint(f'{MODEL_DIR}/best_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
                "]\n",
                "\n",
                "history = model.fit(train_ds, epochs=NUM_EPOCHS, validation_data=val_ds, callbacks=callbacks, verbose=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.plot(history.history['loss'], label='Train')\n",
                "ax1.plot(history.history['val_loss'], label='Val')\n",
                "ax1.set_title('Loss'); ax1.legend(); ax1.grid(True)\n",
                "\n",
                "ax2.plot(history.history['accuracy'], label='Train')\n",
                "ax2.plot(history.history['val_accuracy'], label='Val')\n",
                "ax2.set_title('Accuracy'); ax2.legend(); ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "print(f\"Final Val Acc: {history.history['val_accuracy'][-1]*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "model.save(f'{MODEL_DIR}/gru_model.keras')\n",
                "\n",
                "with open(f'{MODEL_DIR}/word_to_index.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(word_to_index, f, ensure_ascii=False, separators=(',', ':'))\n",
                "\n",
                "with open(f'{MODEL_DIR}/prefix_index.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(dict(prefix_index), f, ensure_ascii=False, separators=(',', ':'))\n",
                "\n",
                "with open(f'{MODEL_DIR}/kana_kanji_index.json', 'w', encoding='utf-8') as f:\n",
                "    json.dump(kana_kanji_index, f, ensure_ascii=False, separators=(',', ':'))\n",
                "\n",
                "config = {\n",
                "    'vocab_size': vocab_size,\n",
                "    'sequence_length': SEQUENCE_LENGTH,\n",
                "    'embedding_dim': EMBEDDING_DIM,\n",
                "    'gru_units': GRU_UNITS,\n",
                "    'language': 'japanese',\n",
                "    'tokenization': 'kana-normalized',\n",
                "    'features': ['kana_kanji_conversion', 'next_word_prediction', 'prefix_completion']\n",
                "}\n",
                "with open(f'{MODEL_DIR}/model_config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(\"‚úì Saved: gru_model.keras, word_to_index.json, prefix_index.json, kana_kanji_index.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export TFLite"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import time\n",
                "\n",
                "print(\"Exporting TFLite...\")\n",
                "\n",
                "try:\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
                "    converter._experimental_lower_tensor_list_ops = False\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    converter.target_spec.supported_types = [tf.float16]\n",
                "    \n",
                "    tflite_model = converter.convert()\n",
                "    path = f'{MODEL_DIR}/gru_model.tflite'\n",
                "    with open(path, 'wb') as f:\n",
                "        f.write(tflite_model)\n",
                "    print(f\"‚úì gru_model.tflite ({len(tflite_model)/(1024*1024):.2f}MB)\")\n",
                "    \n",
                "    # Benchmark\n",
                "    interpreter = tf.lite.Interpreter(model_path=path)\n",
                "    interpreter.allocate_tensors()\n",
                "    details = interpreter.get_input_details()[0]\n",
                "    test = np.random.randint(0, vocab_size, (1, SEQUENCE_LENGTH)).astype(np.float32)\n",
                "    \n",
                "    for _ in range(10):\n",
                "        interpreter.set_tensor(details['index'], test)\n",
                "        interpreter.invoke()\n",
                "    \n",
                "    times = []\n",
                "    for _ in range(50):\n",
                "        t = time.time()\n",
                "        interpreter.set_tensor(details['index'], test)\n",
                "        interpreter.invoke()\n",
                "        times.append((time.time() - t) * 1000)\n",
                "    print(f\"‚úì Latency: {np.mean(times):.2f}ms avg\")\n",
                "except Exception as e:\n",
                "    print(f\"‚úó Error: {str(e)[:100]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Export CoreML Weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights = model.get_weights()\n",
                "np.savez(f'{MODEL_DIR}/gru_weights.npz', *weights)\n",
                "print(f\"‚úì gru_weights.npz ({len(weights)} arrays)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export Mobile Resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "print(\"Exporting mobile resources...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# index_to_word\n",
                "path = f'{MODEL_DIR}/index_to_word.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump({str(k): v for k, v in index_to_word.items()}, f, ensure_ascii=False, separators=(',', ':'))\n",
                "print(f\"‚úì index_to_word.json ({os.path.getsize(path)/1024:.1f}KB)\")\n",
                "\n",
                "# phrase_suggestions (next word)\n",
                "print(\"Building phrase suggestions...\")\n",
                "word_pairs = defaultdict(Counter)\n",
                "for seq in all_sequences[:15000]:\n",
                "    for i in range(len(seq) - 1):\n",
                "        word_pairs[seq[i]][seq[i+1]] += 1\n",
                "\n",
                "phrase_suggestions = {}\n",
                "for word_idx, next_counts in word_pairs.items():\n",
                "    if word_idx < 4:\n",
                "        continue\n",
                "    word = index_to_word.get(word_idx)\n",
                "    if word:\n",
                "        phrase_suggestions[word] = [idx for idx, c in next_counts.most_common(10)]\n",
                "\n",
                "path = f'{MODEL_DIR}/phrase_suggestions.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(phrase_suggestions, f, ensure_ascii=False, separators=(',', ':'))\n",
                "print(f\"‚úì phrase_suggestions.json ({len(phrase_suggestions):,} words, {os.path.getsize(path)/1024:.1f}KB)\")\n",
                "\n",
                "# emoji_suggestions\n",
                "print(\"Building emoji suggestions...\")\n",
                "word_emoji = defaultdict(Counter)\n",
                "for seq in all_sequences[:15000]:\n",
                "    for i in range(len(seq) - 1):\n",
                "        w = index_to_word.get(seq[i])\n",
                "        n = index_to_word.get(seq[i+1])\n",
                "        if w and n and EMOJI_PATTERN.match(n):\n",
                "            word_emoji[w][n] += 1\n",
                "\n",
                "emoji_suggestions = {w: [e for e, c in ec.most_common(5)] for w, ec in word_emoji.items() if ec}\n",
                "path = f'{MODEL_DIR}/emoji_suggestions.json'\n",
                "with open(path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(emoji_suggestions, f, ensure_ascii=False, separators=(',', ':'))\n",
                "print(f\"‚úì emoji_suggestions.json ({len(emoji_suggestions):,} words)\")\n",
                "\n",
                "# List all files\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ALL EXPORTS:\")\n",
                "for f in sorted(os.listdir(MODEL_DIR)):\n",
                "    size = os.path.getsize(f'{MODEL_DIR}/{f}') / 1024\n",
                "    print(f\"  {f}: {size:.1f}KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Verification Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"VERIFICATION - Dual Mode Test\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load all indices\n",
                "with open(f'{MODEL_DIR}/word_to_index.json', 'r', encoding='utf-8') as f:\n",
                "    w2i = json.load(f)\n",
                "with open(f'{MODEL_DIR}/index_to_word.json', 'r', encoding='utf-8') as f:\n",
                "    i2w = {int(k): v for k, v in json.load(f).items()}\n",
                "with open(f'{MODEL_DIR}/prefix_index.json', 'r', encoding='utf-8') as f:\n",
                "    prefix_idx = json.load(f)\n",
                "with open(f'{MODEL_DIR}/kana_kanji_index.json', 'r', encoding='utf-8') as f:\n",
                "    kana_kanji = json.load(f)\n",
                "with open(f'{MODEL_DIR}/phrase_suggestions.json', 'r', encoding='utf-8') as f:\n",
                "    phrase_sug = json.load(f)\n",
                "\n",
                "def get_prefix(prefix, k=5):\n",
                "    if prefix not in prefix_idx:\n",
                "        return []\n",
                "    return [i2w.get(i, '?') for i in prefix_idx[prefix][:k]]\n",
                "\n",
                "def get_kanji(kana):\n",
                "    return kana_kanji.get(kana, [kana])\n",
                "\n",
                "def get_next(word, k=5):\n",
                "    if word not in phrase_sug:\n",
                "        return []\n",
                "    return [i2w.get(i, '?') for i in phrase_sug[word][:k]]\n",
                "\n",
                "# Test 1: Prefix completion\n",
                "print(\"\\nüìù TEST 1: Prefix Completion\")\n",
                "for prefix in ['„ÅÇ„Çä', '„ÅÇ„Çä„Åå', '„Åî„Åñ„ÅÑ„Åæ', '„Åä„ÅØ', '„Åì„Çì„Å´„Å°']:\n",
                "    words = get_prefix(prefix)\n",
                "    print(f\"  '{prefix}' ‚Üí {words if words else '(no match)'}\")\n",
                "\n",
                "# Test 2: Kana‚ÜíKanji conversion\n",
                "print(\"\\nüìù TEST 2: Kana‚ÜíKanji Conversion\")\n",
                "for kana in ['„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åî„Åñ„ÅÑ„Åæ„Åô', '„Åä„ÅØ„Çà„ÅÜ', '„Åì„Çì„Å´„Å°„ÅØ', '„Çè„Åü„Åó']:\n",
                "    kanji = get_kanji(kana)\n",
                "    print(f\"  '{kana}' ‚Üí {kanji}\")\n",
                "\n",
                "# Test 3: Next word prediction\n",
                "print(\"\\nüìù TEST 3: Next Word Prediction\")\n",
                "for word in ['„ÅÇ„Çä„Åå„Å®„ÅÜ', '„Åä„ÅØ„Çà„ÅÜ', '„Åì„Çå', '„Çè„Åü„Åó']:\n",
                "    next_words = get_next(word)\n",
                "    print(f\"  '{word}' ‚Üí {next_words if next_words else '(no prediction)'}\")\n",
                "\n",
                "# Test 4: Complete flow\n",
                "print(\"\\nüìù TEST 4: Complete Flow\")\n",
                "print(\"-\"*40)\n",
                "for prefix in ['„ÅÇ„Çä„Åå', '„Åä„ÅØ']:\n",
                "    print(f\"\\nUser types: '{prefix}'\")\n",
                "    \n",
                "    # Step 1: Prefix completion\n",
                "    kana_words = get_prefix(prefix, 3)\n",
                "    if kana_words:\n",
                "        print(f\"  1. Prefix match: {kana_words}\")\n",
                "        selected_kana = kana_words[0]\n",
                "        \n",
                "        # Step 2: Kanji options\n",
                "        kanji_options = get_kanji(selected_kana)\n",
                "        print(f\"  2. Kanji options: {kanji_options}\")\n",
                "        \n",
                "        # Step 3: Next word\n",
                "        next_words = get_next(selected_kana, 3)\n",
                "        if next_words:\n",
                "            print(f\"  3. Next word: {next_words}\")\n",
                "            # Convert next words to kanji\n",
                "            next_kanji = [get_kanji(w)[0] if w in kana_kanji else w for w in next_words]\n",
                "            print(f\"     (as kanji): {next_kanji}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ VERIFICATION COMPLETE\")\n",
                "print(\"   Features: Prefix, Kana‚ÜíKanji, Next Word Prediction\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage Guide\n",
                "\n",
                "### Mobile Integration\n",
                "\n",
                "```swift\n",
                "// 1. User types partial kana\n",
                "let prefix = \"„ÅÇ„Çä„Åå\"\n",
                "let words = prefixIndex[prefix]  // [\"„ÅÇ„Çä„Åå„Å®„ÅÜ\", \"„ÅÇ„Çä„Åå„Åü„ÅÑ\"]\n",
                "\n",
                "// 2. User selects word, show kanji options\n",
                "let kana = \"„ÅÇ„Çä„Åå„Å®„ÅÜ\"\n",
                "let kanji = kanaKanjiIndex[kana]  // [\"Êúâ„ÇäÈõ£„ÅÜ\", \"ÊúâÈõ£„ÅÜ\"]\n",
                "\n",
                "// 3. Predict next word (using kana internally)\n",
                "let next = phraseSuggestions[kana]  // [\"„Åî„Åñ„ÅÑ„Åæ„Åô\", \"„Å≠\"]\n",
                "let nextKanji = next.map { kanaKanjiIndex[$0]?.first ?? $0 }\n",
                "// [\"Âæ°Â∫ß„ÅÑ„Åæ„Åô\", \"„Å≠\"]\n",
                "```\n",
                "\n",
                "### Files\n",
                "- `prefix_index.json` - Kana prefix completion\n",
                "- `kana_kanji_index.json` - Kana‚ÜíKanji conversion\n",
                "- `phrase_suggestions.json` - Next word prediction\n",
                "- `gru_model.tflite` - ML model (optional, for complex predictions)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}