{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Download & Process HuggingFace Datasets for Keyboard Training\n",
                "\n",
                "This notebook downloads and processes datasets for training a keyboard suggestion model:\n",
                "\n",
                "1. **HuggingFaceTB/everyday-conversations-llama3.1-2k**: Extracts completion text\n",
                "2. **allenai/prosocial-dialog**: Extracts context + response\n",
                "\n",
                "**Features:**\n",
                "- Automatic split detection\n",
                "- Sentence splitting at punctuation (?, !, .)\n",
                "- Text cleaning and normalization\n",
                "- Deduplication\n",
                "- Quality filtering\n",
                "\n",
                "**Output:** Clean text files ready for model training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries\n",
                "!pip install datasets huggingface_hub pandas tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "import pandas as pd\n",
                "\n",
                "print(\"✓ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Text Processing Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_into_sentences(text):\n",
                "    \"\"\"\n",
                "    Split text into sentences at punctuation marks (., !, ?).\n",
                "    Handles common abbreviations and edge cases.\n",
                "    \"\"\"\n",
                "    if not text or pd.isna(text):\n",
                "        return []\n",
                "    \n",
                "    # Clean text\n",
                "    text = str(text).strip()\n",
                "    \n",
                "    # Split at sentence boundaries (., !, ?)\n",
                "    # Use regex to split but keep the punctuation\n",
                "    sentences = re.split(r'([.!?]+\\s+)', text)\n",
                "    \n",
                "    # Recombine sentences with their punctuation\n",
                "    result = []\n",
                "    for i in range(0, len(sentences)-1, 2):\n",
                "        sentence = sentences[i]\n",
                "        if i+1 < len(sentences):\n",
                "            sentence += sentences[i+1]\n",
                "        sentence = sentence.strip()\n",
                "        if sentence:\n",
                "            result.append(sentence)\n",
                "    \n",
                "    # Add last sentence if it doesn't end with punctuation\n",
                "    if len(sentences) % 2 == 1 and sentences[-1].strip():\n",
                "        result.append(sentences[-1].strip())\n",
                "    \n",
                "    return result\n",
                "\n",
                "def clean_text(text):\n",
                "    \"\"\"\n",
                "    Clean and normalize text for keyboard training.\n",
                "    \"\"\"\n",
                "    if not text or pd.isna(text):\n",
                "        return \"\"\n",
                "    \n",
                "    text = str(text)\n",
                "    \n",
                "    # Remove excessive whitespace\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    \n",
                "    # Remove leading/trailing whitespace\n",
                "    text = text.strip()\n",
                "    \n",
                "    return text\n",
                "\n",
                "def is_valid_sentence(sentence, min_length=10, max_length=500):\n",
                "    \"\"\"\n",
                "    Check if a sentence is valid for training.\n",
                "    \n",
                "    Filters out:\n",
                "    - Too short sentences (< 10 chars)\n",
                "    - Too long sentences (> 500 chars)\n",
                "    - Sentences with excessive special characters\n",
                "    - URLs and email addresses\n",
                "    \"\"\"\n",
                "    if not sentence:\n",
                "        return False\n",
                "    \n",
                "    # Length check\n",
                "    if len(sentence) < min_length or len(sentence) > max_length:\n",
                "        return False\n",
                "    \n",
                "    # Filter URLs\n",
                "    if re.search(r'http[s]?://|www\\.', sentence):\n",
                "        return False\n",
                "    \n",
                "    # Filter email addresses\n",
                "    if re.search(r'\\S+@\\S+\\.\\S+', sentence):\n",
                "        return False\n",
                "    \n",
                "    # Check for excessive special characters\n",
                "    special_char_ratio = len(re.findall(r'[^a-zA-Z0-9\\s.,!?\\'\\\"\\-]', sentence)) / len(sentence)\n",
                "    if special_char_ratio > 0.3:\n",
                "        return False\n",
                "    \n",
                "    # Must contain at least some letters\n",
                "    if not re.search(r'[a-zA-Z]', sentence):\n",
                "        return False\n",
                "    \n",
                "    return True\n",
                "\n",
                "def get_dataset_split(dataset):\n",
                "    \"\"\"\n",
                "    Get the first available split from a dataset.\n",
                "    Returns the split name and the data.\n",
                "    \"\"\"\n",
                "    available_splits = list(dataset.keys())\n",
                "    if not available_splits:\n",
                "        raise ValueError(\"Dataset has no splits!\")\n",
                "    \n",
                "    # Prefer 'train' if available, otherwise use first split\n",
                "    if 'train' in available_splits:\n",
                "        split_name = 'train'\n",
                "    else:\n",
                "        split_name = available_splits[0]\n",
                "    \n",
                "    return split_name, dataset[split_name]\n",
                "\n",
                "print(\"✓ Text processing functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Download Dataset 1 - Everyday Conversations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Downloading HuggingFaceTB/everyday-conversations-llama3.1-2k...\\n\")\n",
                "\n",
                "# Load dataset\n",
                "dataset1 = load_dataset(\"HuggingFaceTB/everyday-conversations-llama3.1-2k\")\n",
                "\n",
                "print(f\"✓ Dataset loaded!\")\n",
                "print(f\"\\nDataset structure:\")\n",
                "print(dataset1)\n",
                "\n",
                "# Get available split\n",
                "split1_name, split1_data = get_dataset_split(dataset1)\n",
                "print(f\"\\nUsing split: '{split1_name}'\")\n",
                "print(f\"Number of entries: {len(split1_data):,}\")\n",
                "\n",
                "# Show sample\n",
                "print(f\"\\nSample entry:\")\n",
                "print(split1_data[0])\n",
                "\n",
                "# Show available fields\n",
                "print(f\"\\nAvailable fields: {list(split1_data[0].keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Process Dataset 1 - Extract Completions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing everyday-conversations dataset...\\n\")\n",
                "\n",
                "sentences_dataset1 = []\n",
                "\n",
                "# Extract completion field from each entry\n",
                "for entry in tqdm(split1_data, desc=\"Processing entries\"):\n",
                "    # Get completion text (try different field names)\n",
                "    completion = entry.get('completion', entry.get('text', entry.get('response', '')))\n",
                "    \n",
                "    if completion:\n",
                "        # Clean text\n",
                "        cleaned = clean_text(completion)\n",
                "        \n",
                "        # Split into sentences\n",
                "        sentences = split_into_sentences(cleaned)\n",
                "        \n",
                "        # Filter valid sentences\n",
                "        for sentence in sentences:\n",
                "            if is_valid_sentence(sentence):\n",
                "                sentences_dataset1.append(sentence)\n",
                "\n",
                "print(f\"\\n✓ Extracted {len(sentences_dataset1):,} sentences from everyday-conversations\")\n",
                "print(f\"\\nSample sentences:\")\n",
                "for i, sent in enumerate(sentences_dataset1[:5]):\n",
                "    print(f\"{i+1}. {sent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Download Dataset 2 - Prosocial Dialog"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Downloading allenai/prosocial-dialog...\\n\")\n",
                "\n",
                "# Load dataset\n",
                "dataset2 = load_dataset(\"allenai/prosocial-dialog\")\n",
                "\n",
                "print(f\"✓ Dataset loaded!\")\n",
                "print(f\"\\nDataset structure:\")\n",
                "print(dataset2)\n",
                "\n",
                "# Get available split\n",
                "split2_name, split2_data = get_dataset_split(dataset2)\n",
                "print(f\"\\nUsing split: '{split2_name}'\")\n",
                "print(f\"Number of entries: {len(split2_data):,}\")\n",
                "\n",
                "# Show sample\n",
                "print(f\"\\nSample entry:\")\n",
                "print(split2_data[0])\n",
                "\n",
                "# Show available fields\n",
                "print(f\"\\nAvailable fields: {list(split2_data[0].keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Process Dataset 2 - Extract Context + Response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Processing prosocial-dialog dataset...\\n\")\n",
                "\n",
                "sentences_dataset2 = []\n",
                "\n",
                "# Extract context and response from each entry\n",
                "for entry in tqdm(split2_data, desc=\"Processing entries\"):\n",
                "    # Get context and response (try different field names)\n",
                "    context = entry.get('context', entry.get('prompt', ''))\n",
                "    response = entry.get('response', entry.get('completion', entry.get('text', '')))\n",
                "    \n",
                "    # Process context\n",
                "    if context:\n",
                "        cleaned = clean_text(context)\n",
                "        sentences = split_into_sentences(cleaned)\n",
                "        for sentence in sentences:\n",
                "            if is_valid_sentence(sentence):\n",
                "                sentences_dataset2.append(sentence)\n",
                "    \n",
                "    # Process response\n",
                "    if response:\n",
                "        cleaned = clean_text(response)\n",
                "        sentences = split_into_sentences(cleaned)\n",
                "        for sentence in sentences:\n",
                "            if is_valid_sentence(sentence):\n",
                "                sentences_dataset2.append(sentence)\n",
                "\n",
                "print(f\"\\n✓ Extracted {len(sentences_dataset2):,} sentences from prosocial-dialog\")\n",
                "print(f\"\\nSample sentences:\")\n",
                "for i, sent in enumerate(sentences_dataset2[:5]):\n",
                "    print(f\"{i+1}. {sent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Combine and Deduplicate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Combining and deduplicating sentences...\\n\")\n",
                "\n",
                "# Combine all sentences\n",
                "all_sentences = sentences_dataset1 + sentences_dataset2\n",
                "\n",
                "print(f\"Total sentences before deduplication: {len(all_sentences):,}\")\n",
                "\n",
                "# Deduplicate (case-sensitive)\n",
                "unique_sentences = list(set(all_sentences))\n",
                "\n",
                "print(f\"Unique sentences after deduplication: {len(unique_sentences):,}\")\n",
                "print(f\"Duplicates removed: {len(all_sentences) - len(unique_sentences):,}\")\n",
                "\n",
                "# Sort by length (shorter sentences first - better for keyboard training)\n",
                "unique_sentences.sort(key=len)\n",
                "\n",
                "print(f\"\\n✓ Sentences sorted by length\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Save to Text Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Saving sentences to text files...\\n\")\n",
                "\n",
                "# Save all sentences to one file\n",
                "with open('keyboard_training_data.txt', 'w', encoding='utf-8') as f:\n",
                "    for sentence in unique_sentences:\n",
                "        f.write(sentence + '\\n')\n",
                "\n",
                "print(f\"✓ Saved {len(unique_sentences):,} sentences to 'keyboard_training_data.txt'\")\n",
                "\n",
                "# Also save separate files for each dataset (optional)\n",
                "with open('everyday_conversations.txt', 'w', encoding='utf-8') as f:\n",
                "    for sentence in sentences_dataset1:\n",
                "        f.write(sentence + '\\n')\n",
                "\n",
                "print(f\"✓ Saved {len(sentences_dataset1):,} sentences to 'everyday_conversations.txt'\")\n",
                "\n",
                "with open('prosocial_dialog.txt', 'w', encoding='utf-8') as f:\n",
                "    for sentence in sentences_dataset2:\n",
                "        f.write(sentence + '\\n')\n",
                "\n",
                "print(f\"✓ Saved {len(sentences_dataset2):,} sentences to 'prosocial_dialog.txt'\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PROCESSING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nDownload files from the Files panel on the left.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Dataset Statistics & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"DATASET STATISTICS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Length statistics\n",
                "lengths = [len(s) for s in unique_sentences]\n",
                "\n",
                "print(f\"\\nSentence count: {len(unique_sentences):,}\")\n",
                "print(f\"\\nLength statistics:\")\n",
                "print(f\"  Shortest: {min(lengths)} characters\")\n",
                "print(f\"  Longest: {max(lengths)} characters\")\n",
                "print(f\"  Average: {sum(lengths)/len(lengths):.1f} characters\")\n",
                "print(f\"  Median: {sorted(lengths)[len(lengths)//2]} characters\")\n",
                "\n",
                "# Word count statistics\n",
                "word_counts = [len(s.split()) for s in unique_sentences]\n",
                "print(f\"\\nWord count statistics:\")\n",
                "print(f\"  Average words per sentence: {sum(word_counts)/len(word_counts):.1f}\")\n",
                "print(f\"  Total words: {sum(word_counts):,}\")\n",
                "\n",
                "# Sample sentences by length\n",
                "print(f\"\\nSample short sentences (10-30 chars):\")\n",
                "short_sentences = [s for s in unique_sentences if 10 <= len(s) <= 30]\n",
                "for sent in short_sentences[:5]:\n",
                "    print(f\"  - {sent}\")\n",
                "\n",
                "print(f\"\\nSample medium sentences (50-100 chars):\")\n",
                "medium_sentences = [s for s in unique_sentences if 50 <= len(s) <= 100]\n",
                "for sent in medium_sentences[:5]:\n",
                "    print(f\"  - {sent}\")\n",
                "\n",
                "print(f\"\\nSample long sentences (150-200 chars):\")\n",
                "long_sentences = [s for s in unique_sentences if 150 <= len(s) <= 200]\n",
                "for sent in long_sentences[:5]:\n",
                "    print(f\"  - {sent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary & Recommendations\n",
                "\n",
                "### What This Notebook Does:\n",
                "\n",
                "1. **Downloads** two HuggingFace datasets\n",
                "2. **Auto-detects** available splits (train, test, validation, etc.)\n",
                "3. **Extracts** relevant text fields (completion, context, response)\n",
                "4. **Splits** long text into sentences at punctuation (., !, ?)\n",
                "5. **Cleans** and normalizes text\n",
                "6. **Filters** invalid sentences (too short/long, URLs, etc.)\n",
                "7. **Deduplicates** to remove redundant data\n",
                "8. **Saves** to text files ready for training\n",
                "\n",
                "### Output Files:\n",
                "\n",
                "- **`keyboard_training_data.txt`**: All unique sentences (recommended for training)\n",
                "- **`everyday_conversations.txt`**: Only everyday conversations\n",
                "- **`prosocial_dialog.txt`**: Only prosocial dialog\n",
                "\n",
                "### Recommendations for Better Training:\n",
                "\n",
                "#### 1. **Add More Diverse Datasets**\n",
                "Consider adding:\n",
                "- **SMS/Text messages**: More casual, short-form text\n",
                "- **Social media**: Twitter, Reddit comments\n",
                "- **Chat logs**: Discord, Slack conversations\n",
                "- **Email datasets**: Professional communication\n",
                "\n",
                "#### 2. **Sentence Length Optimization**\n",
                "- **Focus on 10-100 character sentences** (most common in typing)\n",
                "- **Weight shorter sentences more** (people type short messages more often)\n",
                "- Consider creating separate training sets by length\n",
                "\n",
                "#### 3. **Context-Aware Training**\n",
                "- Keep **conversation pairs** together (context → response)\n",
                "- Train on **sentence transitions** (what typically follows what)\n",
                "- Include **common phrases** and **idioms**\n",
                "\n",
                "#### 4. **Domain-Specific Data**\n",
                "Add datasets for specific use cases:\n",
                "- **Technical writing** (if users are developers)\n",
                "- **Customer service** (if for support chat)\n",
                "- **Casual chat** (if for messaging apps)\n",
                "\n",
                "#### 5. **Data Augmentation**\n",
                "- **Typo injection**: Add common typos to learn corrections\n",
                "- **Abbreviation expansion**: \"u\" → \"you\", \"ur\" → \"your\"\n",
                "- **Case variations**: Train on mixed case\n",
                "\n",
                "#### 6. **Quality Improvements**\n",
                "- **Remove repetitive patterns** (\"I think I think I think\")\n",
                "- **Filter profanity** (if needed)\n",
                "- **Balance dataset** (equal representation of question/statement/exclamation)\n",
                "\n",
                "#### 7. **Tokenization Strategy**\n",
                "For a tiny keyboard model:\n",
                "- Use **word-level** or **subword tokenization** (BPE)\n",
                "- Keep vocabulary **small** (10k-30k tokens)\n",
                "- Focus on **most frequent words** from your `single_word_freq.csv`\n",
                "\n",
                "#### 8. **Training Format**\n",
                "Consider creating:\n",
                "- **Prefix → completion** pairs (\"How are\" → \"you\")\n",
                "- **N-gram sequences** (trigrams, 4-grams)\n",
                "- **Masked language modeling** (\"I ___ going to the store\")\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. Download and review the generated text files\n",
                "2. Combine with your existing `single_word_freq.csv` for vocabulary\n",
                "3. Create training pairs (input → output)\n",
                "4. Train a small transformer model (e.g., GPT-2 tiny, BERT tiny)\n",
                "5. Evaluate on real typing scenarios"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
