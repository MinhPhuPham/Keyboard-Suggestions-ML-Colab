{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Keyboard Suggestion Model Training (with Google Drive Integration)\\n",
    "\\n",
    "This notebook trains an English keyboard suggestion model using **DistilGPT2** with LoRA fine-tuning.\\n",
    "\\n",
    "**Model:** DistilGPT2 (82M parameters, optimized for mobile keyboards)\\n",
    "\\n",
    "**Features:**\\n",
    "- Automatic Google Drive data management\\n",
    "- Real SwiftKey corpus loading (100K sentences)\\n",
    "- LoRA fine-tuning for efficiency\\n",
    "- ONNX export with INT4 quantization\\n",
    "- Performance validation (perplexity, latency)\\n",
    "- Email notifications on completion\\n",
    "\\n",
    "**Target Specifications:**\\n",
    "- Model Size: < 50 MB (after INT4 quantization)\\n",
    "- Latency: < 20 ms (P95)\\n",
    "- Perplexity: < 30\\n",
    "- Sequence Length: 32 tokens (keyboard context)\\n",
    "\\n",
    "**Why DistilGPT2:**\\n",
    "- Small size: 82M parameters (vs 1.5B in Qwen)\\n",
    "- Perfect for text prediction (causal LM)\\n",
    "- Mobile-optimized architecture\\n",
    "- Industry standard for keyboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\u2713 Running in Google Colab\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Ensure we're in /content\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo if it exists (for re-runs)\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        import shutil\n",
    "        shutil.rmtree('Keyboard-Suggestions-ML-Colab')\n",
    "        print(\"\u2713 Removed existing repository\")\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"\u2713 Repository cloned\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (self-contained with fallbacks)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('./src')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import custom utilities with error handling\n",
    "try:\n",
    "    from data_prep import clean_english_text, augment_with_emojis, split_dataset\n",
    "    from model_utils import (\n",
    "        load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "        prune_model, quantize_model, merge_lora_weights\n",
    "    )\n",
    "    from export_utils import (\n",
    "        export_to_onnx, export_to_coreml, verify_model_size,\n",
    "        benchmark_latency, package_for_download\n",
    "    )\n",
    "    from colab_data_manager import (\n",
    "        mount_google_drive, setup_english_data, send_notification_email\n",
    "    )\n",
    "    print(\"\u2713 All custom modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0 Import error: {e}\")\n",
    "    print(\"  Make sure you've cloned the repository and src/ directory exists\")\n",
    "    print(\"  Some functions may not be available\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    mount_success = mount_google_drive()\n",
    "    if not mount_success:\n",
    "        raise Exception(\"Failed to mount Google Drive\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up English training data\n",
    "# This will check Drive first, download if needed\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Phu's Data development\"\n",
    "    data_path = setup_english_data(DRIVE_BASE)\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"\u26a0 Data setup failed. Please check errors above.\")\n",
    "        print(\"\\nManual setup instructions:\")\n",
    "        print(\"1. Download SwiftKey dataset from Kaggle:\")\n",
    "        print(\"   https://www.kaggle.com/datasets/therohk/tweets-blogs-news-swiftkey-dataset-4million\")\n",
    "        print(f\"2. Upload to: {DRIVE_BASE}/data/english/\")\n",
    "    else:\n",
    "        print(f\"\\n\u2713 Data ready at: {data_path}\")\n",
    "else:\n",
    "    data_path = \"./data/english\"\n",
    "    print(f\"Using local data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare training data from SwiftKey corpus\n",
    "import random\n",
    "\n",
    "def load_swiftkey_data(data_path, max_samples=100000):\n",
    "    \"\"\"Load sentences from SwiftKey corpus files.\"\"\"\n",
    "    sentences = []\n",
    "    files = ['en_US.twitter.txt', 'en_US.blogs.txt', 'en_US.news.txt']\n",
    "    \n",
    "    for filename in files:\n",
    "        filepath = os.path.join(data_path, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"  Loading {filename}...\")\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                # Read lines and take subset\n",
    "                lines = []\n",
    "                for idx, line in enumerate(f):\n",
    "                    if idx >= max_samples // 3:  # Distribute evenly\n",
    "                        break\n",
    "                    line = line.strip()\n",
    "                    if line:  # Skip empty lines\n",
    "                        lines.append(line)\n",
    "                sentences.extend(lines)\n",
    "                print(f\"    Loaded {len(lines)} sentences\")\n",
    "        else:\n",
    "            print(f\"  \u26a0 {filename} not found, skipping\")\n",
    "    \n",
    "    # Shuffle and limit total\n",
    "    random.shuffle(sentences)\n",
    "    return sentences[:max_samples]\n",
    "\n",
    "# Load real data if available, fallback to samples\n",
    "if data_path and os.path.exists(data_path):\n",
    "    print(\"Loading SwiftKey corpus...\")\n",
    "    raw_sentences = load_swiftkey_data(data_path, max_samples=100000)\n",
    "    print(f\"\u2713 Loaded {len(raw_sentences)} sentences from SwiftKey corpus\")\n",
    "else:\n",
    "    print(\"\u26a0 SwiftKey data not found, using sample data\")\n",
    "    raw_sentences = [\n",
    "        \"Today is a beautiful day\",\n",
    "        \"I love programming in Python\",\n",
    "        \"The weather is nice today\",\n",
    "        \"Let's meet tomorrow morning\",\n",
    "        \"Thank you for your help\",\n",
    "        \"How are you doing today\",\n",
    "        \"See you later tonight\",\n",
    "        \"Have a great weekend\",\n",
    "    ]\n",
    "    print(f\"  Using {len(raw_sentences)} sample sentences\")\n",
    "\n",
    "# Clean text\n",
    "print(\"Cleaning text...\")\n",
    "cleaned = [clean_english_text(s) for s in raw_sentences if s.strip()]\n",
    "print(f\"\u2713 Cleaned {len(cleaned)} sentences\")\n",
    "\n",
    "# Augment with emojis\n",
    "print(\"Augmenting with emojis...\")\n",
    "augmented = augment_with_emojis(cleaned, emoji_ratio=0.1)\n",
    "print(f\"\u2713 Augmented to {len(augmented)} samples\")\n",
    "\n",
    "# Split into train/eval\n",
    "split_idx = int(len(augmented) * 0.9)\n",
    "train_sentences = augmented[:split_idx]\n",
    "eval_sentences = augmented[split_idx:]\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training: {len(train_sentences)} samples\")\n",
    "print(f\"  Evaluation: {len(eval_sentences)} samples\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for sent in train_sentences[:3]:\n",
    "    print(f\"  {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization for Colab free tier\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear any existing allocations\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory allocation strategy\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"\u2713 Memory optimizations applied\")\n",
    "print(f\"\u2713 GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2713 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilGPT2 model with LoRA\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    ")\n",
    "\n",
    "# LoRA configuration for DistilGPT2\n",
    "# Target modules: c_attn (combined QKV), c_proj (output projection)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # DistilGPT2 attention modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\u2713 DistilGPT2 loaded with LoRA adapters\")\n",
    "print(f\"  Base model size: ~330MB\")\n",
    "print(f\"  After INT4 quantization: ~40MB (target)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "# Use keyboard-appropriate sequence length (32 tokens)\n",
    "MAX_SEQ_LENGTH = 32\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with keyboard-appropriate length\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "# Create training dataset\n",
    "train_data = Dataset.from_dict({'text': train_sentences})\n",
    "train_dataset = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_data = Dataset.from_dict({'text': eval_sentences})\n",
    "eval_dataset = eval_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"\u2713 Evaluation dataset: {len(eval_dataset)} samples\")\n",
    "print(f\"\u2713 Sequence length: {MAX_SEQ_LENGTH} tokens\")\n",
    "print(f\"\u2713 Dataset columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Save checkpoints locally (Drive can cause connection errors)\n",
    "checkpoint_dir = \"./checkpoints/english\"\n",
    "\n",
    "trainer = train_causal_lm(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=eval_dataset,  # Use val_dataset parameter\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training complete!\")\n",
    "\n",
    "# Copy final checkpoint to Drive (optional)\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    drive_checkpoint_dir = os.path.join(DRIVE_BASE, \"checkpoints\", \"english\")\n",
    "    try:\n",
    "        print(f\"Copying checkpoint to Drive: {drive_checkpoint_dir}\")\n",
    "        os.makedirs(os.path.dirname(drive_checkpoint_dir), exist_ok=True)\n",
    "        if os.path.exists(drive_checkpoint_dir):\n",
    "            shutil.rmtree(drive_checkpoint_dir)\n",
    "        shutil.copytree(checkpoint_dir, drive_checkpoint_dir)\n",
    "        print(\"\u2713 Checkpoint copied to Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 Could not copy to Drive: {e}\")\n",
    "        print(\"  Checkpoint saved locally only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model\n",
    "print(\"Merging LoRA weights...\")\n",
    "\n",
    "# Use PEFT's built-in merge method\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"\u2713 LoRA weights merged into base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "# Calculate perplexity on evaluation set\n",
    "perplexity = evaluate_perplexity(model, tokenizer, eval_dataset)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "print(f\"Target: < 20\")\n",
    "print(f\"Status: {'\u2713 PASS' if perplexity < 20 else '\u2717 FAIL'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Store for later reporting\n",
    "model_metrics = {\n",
    "    'perplexity': perplexity,\n",
    "    'perplexity_target': 20,\n",
    "    'perplexity_pass': perplexity < 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (full precision)\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Install onnxscript if needed\n",
    "try:\n",
    "    import onnxscript\n",
    "except ImportError:\n",
    "    print(\"Installing onnxscript...\")\n",
    "    !pip install -q onnxscript\n",
    "    print(\"\u2713 onnxscript installed\")\n",
    "\n",
    "# Set model output directory\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(DRIVE_BASE, \"models\", \"english\")\n",
    "else:\n",
    "    model_dir = \"./models/english\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Exporting to ONNX...\")\n",
    "\n",
    "# Prepare dummy input (use keyboard-appropriate length)\n",
    "dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\", padding=True, max_length=MAX_SEQ_LENGTH, truncation=True)\n",
    "model.eval()\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = os.path.join(model_dir, \"english_model.onnx\")\n",
    "\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input['input_ids'],),\n",
    "        onnx_path,\n",
    "        input_names=['input_ids'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}},\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"\u2713 ONNX model exported: {onnx_path}\")\n",
    "    \n",
    "    # Check size\n",
    "    size_mb = os.path.getsize(onnx_path) / 1e6\n",
    "    print(f\"  Model size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Store for metrics\n",
    "    model_metrics['onnx_size_mb'] = size_mb\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 ONNX export failed: {e}\")\n",
    "    print(f\"  Error details: {type(e).__name__}\")\n",
    "    onnx_path = None\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize ONNX model (INT8)\n",
    "if onnx_path:\n",
    "    try:\n",
    "        from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "        \n",
    "        print(\"Quantizing ONNX model...\")\n",
    "        \n",
    "        quantized_path = onnx_path.replace('.onnx', '_quantized.onnx')\n",
    "        \n",
    "        quantize_dynamic(\n",
    "            model_input=onnx_path,\n",
    "            model_output=quantized_path,\n",
    "            weight_type=QuantType.QUInt8\n",
    "        )\n",
    "        \n",
    "        print(f\"\u2713 Quantized ONNX model: {quantized_path}\")\n",
    "        \n",
    "        # Check size reduction\n",
    "        original_size = os.path.getsize(onnx_path) / 1e6\n",
    "        quantized_size = os.path.getsize(quantized_path) / 1e6\n",
    "        reduction = (1 - quantized_size/original_size) * 100\n",
    "        \n",
    "        print(f\"  Original: {original_size:.1f} MB\")\n",
    "        print(f\"  Quantized: {quantized_size:.1f} MB\")\n",
    "        print(f\"  Reduction: {reduction:.1f}%\")\n",
    "        \n",
    "        # Use quantized model for further steps\n",
    "        onnx_path = quantized_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 ONNX quantization failed: {e}\")\n",
    "        print(\"  Using non-quantized ONNX model\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping quantization (ONNX export failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model\n",
    "if onnx_path:\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        import numpy as np\n",
    "        \n",
    "        print(\"Validating ONNX model...\")\n",
    "        \n",
    "        # Load ONNX model\n",
    "        session = ort.InferenceSession(onnx_path)\n",
    "        \n",
    "        # Test inference\n",
    "        test_input = tokenizer(\"Hello world\", return_tensors=\"pt\", padding=True, max_length=128)\n",
    "        onnx_output = session.run(\n",
    "            None,\n",
    "            {\"input_ids\": test_input['input_ids'].numpy()}\n",
    "        )[0]\n",
    "        \n",
    "        print(\"\u2713 ONNX model validated\")\n",
    "        print(f\"  Output shape: {onnx_output.shape}\")\n",
    "        print(f\"  Sample logits: {onnx_output[0, 0, :5]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 ONNX validation failed: {e}\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping validation (no ONNX model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Core ML (optional - skip if low on memory)\n",
    "import gc\n",
    "\n",
    "if onnx_path:  # Only if ONNX export succeeded\n",
    "    try:\n",
    "        print(\"Exporting to Core ML...\")\n",
    "        coreml_path = export_to_coreml(\n",
    "            onnx_path=onnx_path,\n",
    "            output_path=os.path.join(model_dir, \"english_model.mlmodel\"),\n",
    "            model_name=\"EnglishKeyboardSuggestion\"\n",
    "        )\n",
    "        if coreml_path:\n",
    "            print(f\"\u2713 Core ML model saved to: {coreml_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 Core ML export failed: {e}\")\n",
    "        print(\"Skipping Core ML export\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping Core ML export (ONNX export failed)\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model size\n",
    "size_mb, meets_req = verify_model_size(\n",
    "    model_path=onnx_path,\n",
    "    max_size_mb=30\n",
    ")\n",
    "\n",
    "if meets_req:\n",
    "    print(f\"\u2713 Model size requirement met: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\u26a0 Model size exceeds target: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for download\n",
    "zip_path = package_for_download(\n",
    "    model_dir=model_dir,\n",
    "    output_zip=\"english_model.zip\"\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model packaged: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send completion notification\n",
    "send_notification_email(\n",
    "    subject=\"English Model Training Complete! \ud83c\udf89\",\n",
    "    message=f\"\"\"\n",
    "English keyboard suggestion model training has completed successfully!\n",
    "\n",
    "Model Details:\n",
    "- Size: {size_mb:.2f} MB\n",
    "- Location: {model_dir}\n",
    "- Package: {zip_path}\n",
    "\n",
    "The model is ready for integration into your keyboard app.\n",
    "\n",
    "Next steps:\n",
    "1. Download the model package\n",
    "2. Integrate into iOS/Android app\n",
    "3. Test on actual devices\n",
    "    \"\"\",\n",
    "    to_email=\"phamminhphueur@gmail.com\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Package: {zip_path}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading model package...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"\u2713 Download started\")\n",
    "else:\n",
    "    print(f\"Model saved locally to: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}