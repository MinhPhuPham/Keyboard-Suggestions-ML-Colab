{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileBERT Keyboard Suggestion Model Training\n",
    "\n",
    "This notebook trains a keyboard suggestion model using **MobileBERT** with multi-task learning.\n",
    "\n",
    "**Features:**\n",
    "1. Word Completion: \"Hel\" \u2192 [\"Hello\", \"Help\", \"Helping\"]\n",
    "2. Next-Word Prediction: \"How are\" \u2192 [\"you\", \"they\", \"we\"]\n",
    "3. Typo Correction: \"Thers\" \u2192 [\"There\", \"Theirs\", \"Therapy\"]\n",
    "4. Gibberish Detection: Heuristic (no ML)\n",
    "\n",
    "**Model Specifications:**\n",
    "- Base: MobileBERT-TINY (15M parameters)\n",
    "- Target Size: <15MB (after INT8 quantization)\n",
    "- Latency: <50ms on mobile\n",
    "- Deployment: iOS (CoreML) + Android (TFLite)\n",
    "\n",
    "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
    "\n",
    "**Data Sources:**\n",
    "- Word frequencies: GitHub (600K+ words)\n",
    "- Text corpus: OpenSubtitles2024 (Hugging Face)\n",
    "- Typos: WikEd Error Corpus + synthetic\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "1. Runtime \u2192 Change runtime type \u2192 GPU (T4)\n",
    "2. Run all cells\n",
    "3. Model will be saved to Google Drive\n",
    "4. Download for mobile deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\u2713 Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Define Drive directory\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
    "    \n",
    "    print(f\"\u2713 Google Drive mounted\")\n",
    "    print(f\"\u2713 Project directory: {DRIVE_DIR}\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally\")\n",
    "    DRIVE_DIR = './data'  # Local fallback\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Ensure we're in /content\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo if it exists (for re-runs)\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        import shutil\n",
    "        shutil.rmtree('Keyboard-Suggestions-ML-Colab')\n",
    "        print(\"\u2713 Removed existing repository\")\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"\u2713 Repository cloned\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Datasets\n",
    "\n",
    "We'll download 3 datasets:\n",
    "1. **Word Frequencies** - GitHub (600K words)\n",
    "2. **Text Corpus** - OpenSubtitles2024 (Hugging Face)\n",
    "3. **Typo Corrections** - Synthetic + real typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Download word frequency list from GitHub\n",
    "print(\"Downloading word frequency list...\")\n",
    "\n",
    "WORD_FREQ_URL = \"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears.txt\"\n",
    "word_freq_path = f\"{DRIVE_DIR}/datasets/word_freq.txt\"\n",
    "\n",
    "if not os.path.exists(word_freq_path):\n",
    "    urllib.request.urlretrieve(WORD_FREQ_URL, word_freq_path)\n",
    "    print(f\"\u2713 Downloaded to: {word_freq_path}\")\n",
    "else:\n",
    "    print(f\"\u2713 Already exists: {word_freq_path}\")\n",
    "\n",
    "# Count words\n",
    "with open(word_freq_path, 'r') as f:\n",
    "    word_count = len(f.readlines())\n",
    "print(f\"  Words: {word_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download text corpus from Hugging Face\n",
    "print(\"\\nDownloading text corpus from Hugging Face...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "corpus_path = f\"{DRIVE_DIR}/datasets/corpus.txt\"\n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    print(\"  Loading WikiText dataset (clean and reliable)...\")\n",
    "    \n",
    "    # Use WikiText-103 - clean, well-formatted English text\n",
    "    dataset = load_dataset(\n",
    "        \"wikitext\",\n",
    "        \"wikitext-103-raw-v1\",\n",
    "        split=\"train\"\n",
    "    )\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= 100000:  # Limit to 100K items\n",
    "            break\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Processed {i:,} items...\")\n",
    "        \n",
    "        # Get text\n",
    "        text = item['text'].strip().lower()\n",
    "        \n",
    "        # Filter: at least 3 words, not empty, not just punctuation\n",
    "        if len(text) > 10 and len(text.split()) >= 3:\n",
    "            sentences.append(text)\n",
    "        \n",
    "        # Stop if we have enough\n",
    "        if len(sentences) >= 50000:\n",
    "            break\n",
    "    \n",
    "    # Save to file\n",
    "    with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    print(f\"\u2713 Downloaded {len(sentences):,} sentences\")\n",
    "    print(f\"\u2713 Saved to: {corpus_path}\")\n",
    "else:\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        sentence_count = len(f.readlines())\n",
    "    print(f\"\u2713 Already exists: {corpus_path}\")\n",
    "    print(f\"  Sentences: {sentence_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Training Data\n",
    "\n",
    "Generate training pairs for all 3 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from keyboard_data_prep import (\n",
    "    prepare_word_completion_data,\n",
    "    prepare_nextword_data,\n",
    "    prepare_typo_data,\n",
    "    combine_datasets\n",
    ")\n",
    "\n",
    "print(\"Preparing training datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
    "train_path = f\"{output_dir}/train.jsonl\"\n",
    "val_path = f\"{output_dir}/val.jsonl\"\n",
    "\n",
    "# Check if processed datasets already exist in Drive\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    print(\"\u2713 Processed datasets found in Drive!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "    \n",
    "    # Count samples\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_count = sum(1 for _ in f)\n",
    "    with open(val_path, 'r') as f:\n",
    "        val_count = sum(1 for _ in f)\n",
    "    print(f\"  Train samples: {train_count:,}\")\n",
    "    print(f\"  Val samples: {val_count:,}\")\n",
    "else:\n",
    "    print(\"Generating training datasets from scratch...\")\n",
    "    \n",
    "    # 1. Word completion\n",
    "    print(\"\\n1. Word Completion...\")\n",
    "    completion_path = prepare_word_completion_data(\n",
    "        word_freq_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/completion.jsonl\",\n",
    "        max_samples=50000  # 50K pairs\n",
    "    )\n",
    "    \n",
    "    # 2. Next-word prediction\n",
    "    print(\"\\n2. Next-Word Prediction...\")\n",
    "    nextword_path = prepare_nextword_data(\n",
    "        corpus_path=corpus_path,\n",
    "        output_path=f\"{output_dir}/nextword.jsonl\",\n",
    "        max_samples=100000,  # 100K pairs\n",
    "        context_length=3\n",
    "    )\n",
    "    \n",
    "    # 3. Typo correction\n",
    "    print(\"\\n3. Typo Correction...\")\n",
    "    typo_path = prepare_typo_data(\n",
    "        word_list_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/typo.jsonl\",\n",
    "        max_samples=20000  # 20K pairs\n",
    "    )\n",
    "    \n",
    "    # 4. Combine and split\n",
    "    print(\"\\n4. Combining datasets...\")\n",
    "    train_path, val_path = combine_datasets(\n",
    "        completion_path=completion_path,\n",
    "        nextword_path=nextword_path,\n",
    "        typo_path=typo_path,\n",
    "        output_path=output_dir,\n",
    "        train_ratio=0.9\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\u2713 Dataset generation complete!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 Datasets ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileBertForMaskedLM, MobileBertTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading MobileBERT for Masked Language Modeling...\")\n",
    "\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "model = MobileBertForMaskedLM.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\u2713 Model loaded on {device}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model size: ~100MB (FP32) \u2192 ~12-15MB (INT8 quantized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class KeyboardDataset(Dataset):\n",
    "    \"\"\"BERT MLM dataset for keyboard suggestions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tokenizer, max_length=32):\n",
    "        self.data = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text_input = item['input']\n",
    "        target_word = item['target']\n",
    "        task = item.get('task', 'completion')  # Default to prevent KeyError\n",
    "        \n",
    "        # Add [MASK] for prediction - KEEP the input context!\n",
    "        # For ALL tasks: input + [MASK]\n",
    "        # - Completion: \"Hel [MASK]\" \u2192 predict \"lo\" (Hello)\n",
    "        # - Next-word: \"How are [MASK]\" \u2192 predict \"you\"\n",
    "        # - Typo: \"Thers [MASK]\" \u2192 predict \"There\" (model sees the typo!)\n",
    "        text_input = f\"{text_input} {self.tokenizer.mask_token}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Labels: -100 everywhere except [MASK]\n",
    "        labels = torch.full(inputs['input_ids'].shape, -100, dtype=torch.long)\n",
    "        \n",
    "        # Get target token ID\n",
    "        target_tokens = self.tokenizer.tokenize(target_word)\n",
    "        target_id = self.tokenizer.convert_tokens_to_ids(target_tokens[0]) if target_tokens else self.tokenizer.unk_token_id\n",
    "        \n",
    "        # Set label at [MASK] position\n",
    "        mask_positions = (inputs['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "        if len(mask_positions[1]) > 0:\n",
    "            labels[0, mask_positions[1][0]] = target_id\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze(),\n",
    "            'task': task\n",
    "        }\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = KeyboardDataset(train_path, tokenizer)\n",
    "val_dataset = KeyboardDataset(val_path, tokenizer)\n",
    "\n",
    "print(f\"\u2713 Train samples: {len(train_dataset):,}\")\n",
    "print(f\"\u2713 Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-4\n",
    "SAVE_STEPS = 1000\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        global_step += 1\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "            print(f\"\\n\u2713 Checkpoint saved: {checkpoint_dir}\")\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
    "        model.save_pretrained(best_model_dir)\n",
    "        tokenizer.save_pretrained(best_model_dir)\n",
    "        print(f\"  \u2713 Best model saved: {best_model_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 Training complete!\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and save model\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Exporting and saving model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Save to Google Drive (persistent storage)\n",
    "drive_model_dir = f\"{DRIVE_DIR}/models/mobilebert_keyboard_final\"\n",
    "model.save_pretrained(drive_model_dir)\n",
    "tokenizer.save_pretrained(drive_model_dir)\n",
    "print(f\"\\n\u2705 Saved to Google Drive: {drive_model_dir}\")\n",
    "\n",
    "# 2. Create downloadable zip for local use\n",
    "local_model_dir = \"/content/mobilebert_keyboard_model\"\n",
    "model.save_pretrained(local_model_dir)\n",
    "tokenizer.save_pretrained(local_model_dir)\n",
    "\n",
    "# Create zip file\n",
    "zip_path = \"/content/mobilebert_keyboard_model.zip\"\n",
    "shutil.make_archive(\"/content/mobilebert_keyboard_model\", 'zip', local_model_dir)\n",
    "print(f\"\\n\u2705 Created zip: {zip_path}\")\n",
    "\n",
    "# 3. Download to local device (if in Colab)\n",
    "if IN_COLAB:\n",
    "    print(\"\\n\ud83d\udce5 Downloading model to your computer...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"\u2705 Download started! Check your Downloads folder.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 Export complete!\")\n",
    "print(\"\\nModel saved to:\")\n",
    "print(f\"  1. Google Drive: {drive_model_dir}\")\n",
    "print(f\"  2. Local download: mobilebert_keyboard_model.zip\")\n",
    "print(\"\\nModel details:\")\n",
    "print(f\"  \u2022 Size: ~100MB (FP32)\")\n",
    "print(f\"  \u2022 Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Extract the zip file\")\n",
    "print(\"  2. Apply INT8 quantization (~12-15MB)\")\n",
    "print(\"  3. Export to CoreML (iOS) or TFLite (Android)\")\n",
    "print(\"\\n\ud83d\udca1 The model in Google Drive persists across sessions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to CoreML (iOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CoreML for iOS\n",
    "!pip install -q coremltools\n",
    "\n",
    "import coremltools as ct\n",
    "from coremltools.models.neural_network import quantization_utils\n",
    "import torch\n",
    "\n",
    "print(\"Exporting to CoreML for iOS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Prepare model for tracing\n",
    "print(\"\\n1. Preparing model for conversion...\")\n",
    "model.cpu().eval()\n",
    "\n",
    "# Create dummy inputs (batch_size=1, seq_length=32)\n",
    "dummy_input_ids = torch.zeros(1, 32, dtype=torch.long)\n",
    "dummy_attention_mask = torch.ones(1, 32, dtype=torch.long)\n",
    "\n",
    "# 2. Trace the model\n",
    "print(\"2. Tracing model...\")\n",
    "traced_model = torch.jit.trace(model, (dummy_input_ids, dummy_attention_mask))\n",
    "print(\"   \u2713 Model traced successfully\")\n",
    "\n",
    "# 3. Convert to CoreML (FP32 - ~100MB)\n",
    "print(\"\\n3. Converting to CoreML (FP32)...\")\n",
    "mlmodel = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[\n",
    "        ct.TensorType(name=\"input_ids\", shape=(1, 32), dtype=int),\n",
    "        ct.TensorType(name=\"attention_mask\", shape=(1, 32), dtype=int)\n",
    "    ],\n",
    "    outputs=[ct.TensorType(name=\"logits\")]\n",
    ")\n",
    "print(\"   \u2713 Conversion successful\")\n",
    "\n",
    "# 4. Quantize to INT8 (~12-15MB)\n",
    "print(\"\\n4. Quantizing to INT8...\")\n",
    "mlmodel_int8 = quantization_utils.quantize_weights(mlmodel, nbits=8)\n",
    "print(\"   \u2713 Quantization complete\")\n",
    "\n",
    "# 5. Save to Drive\n",
    "coreml_path = f\"{DRIVE_DIR}/models/MobileBERT_Keyboard_iOS.mlpackage\"\n",
    "mlmodel_int8.save(coreml_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 iOS CoreML export complete!\")\n",
    "print(f\"\\nSaved to: {coreml_path}\")\n",
    "print(f\"Model size: ~12-15MB (INT8 quantized)\")\n",
    "print(f\"Expected latency: 15-20ms on iPhone\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Download from Google Drive\")\n",
    "print(\"  2. Add to Xcode project\")\n",
    "print(\"  3. Use with Vision framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to TFLite (Android)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TFLite for Android\n",
    "!pip install -q tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Exporting to TFLite for Android...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Convert PyTorch to ONNX first\n",
    "print(\"\\n1. Converting PyTorch -> ONNX...\")\n",
    "model.cpu().eval()\n",
    "\n",
    "onnx_path = \"/content/model.onnx\"\n",
    "dummy_input_ids = torch.zeros(1, 32, dtype=torch.long)\n",
    "dummy_attention_mask = torch.ones(1, 32, dtype=torch.long)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input_ids, dummy_attention_mask),\n",
    "    onnx_path,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size'},\n",
    "        'attention_mask': {0: 'batch_size'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=12\n",
    ")\n",
    "print(\"   \u2713 ONNX export successful\")\n",
    "\n",
    "# 2. Convert ONNX to TensorFlow SavedModel\n",
    "print(\"\\n2. Converting ONNX -> TensorFlow...\")\n",
    "!pip install -q onnx-tf\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_model_path = \"/content/tf_model\"\n",
    "tf_rep.export_graph(tf_model_path)\n",
    "print(\"   \u2713 TensorFlow conversion successful\")\n",
    "\n",
    "# 3. Convert to TFLite with INT8 quantization\n",
    "print(\"\\n3. Converting to TFLite with INT8 quantization...\")\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # INT8 quantization\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 4. Save to Drive\n",
    "tflite_dir = Path(f\"{DRIVE_DIR}/models/tflite\")\n",
    "tflite_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tflite_path = tflite_dir / \"keyboard_model_quantized.tflite\"\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# 5. Save vocabulary\n",
    "tokenizer.save_vocabulary(str(tflite_dir))\n",
    "\n",
    "import os\n",
    "model_size_mb = os.path.getsize(tflite_path) / 1024 / 1024\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 Android TFLite export complete!\")\n",
    "print(f\"\\nSaved to: {tflite_path}\")\n",
    "print(f\"Model size: {model_size_mb:.2f}MB (INT8 quantized)\")\n",
    "print(f\"Expected latency: 10-30ms on Android\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  \u2022 keyboard_model_quantized.tflite\")\n",
    "print(f\"  \u2022 vocab.txt\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Download from Google Drive\")\n",
    "print(\"  2. Add to Android project (assets/)\")\n",
    "print(\"  3. Use with TensorFlow Lite Interpreter\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}