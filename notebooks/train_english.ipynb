{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Keyboard Suggestion Model Training (with Google Drive Integration)\n",
    "\n",
    "This notebook trains an English keyboard suggestion model using Microsoft Phi-3 Mini with LoRA fine-tuning.\n",
    "\n",
    "**Features:**\n",
    "- Automatic Google Drive data management\n",
    "- Checks for existing data before downloading\n",
    "- Email notifications on completion\n",
    "- Saves models to Drive for persistence\n",
    "\n",
    "**Target Specifications:**\n",
    "- Model Size: 20-30 MB (after optimization)\n",
    "- Latency: < 50 ms\n",
    "- Perplexity: < 20\n",
    "- Top-3 Accuracy: > 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\u2713 Running in Google Colab\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Ensure we're in /content\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo if it exists (for re-runs)\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        import shutil\n",
    "        shutil.rmtree('Keyboard-Suggestions-ML-Colab')\n",
    "        print(\"\u2713 Removed existing repository\")\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"\u2713 Repository cloned\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Import custom utilities\n",
    "from data_prep import clean_english_text, augment_with_emojis, split_dataset\n",
    "from model_utils import (\n",
    "    load_model_with_lora, train_causal_lm, evaluate_perplexity,\n",
    "    prune_model, quantize_model, merge_lora_weights\n",
    ")\n",
    "from export_utils import (\n",
    "    export_to_onnx, export_to_coreml, verify_model_size,\n",
    "    benchmark_latency, package_for_download\n",
    ")\n",
    "from colab_data_manager import (\n",
    "    mount_google_drive, setup_english_data, send_notification_email\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    mount_success = mount_google_drive()\n",
    "    if not mount_success:\n",
    "        raise Exception(\"Failed to mount Google Drive\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up English training data\n",
    "# This will check Drive first, download if needed\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Phu's Data development\"\n",
    "    data_path = setup_english_data(DRIVE_BASE)\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"\u26a0 Data setup failed. Please check errors above.\")\n",
    "        print(\"\\nManual setup instructions:\")\n",
    "        print(\"1. Download SwiftKey dataset from Kaggle:\")\n",
    "        print(\"   https://www.kaggle.com/datasets/therohk/tweets-blogs-news-swiftkey-dataset-4million\")\n",
    "        print(f\"2. Upload to: {DRIVE_BASE}/data/english/\")\n",
    "    else:\n",
    "        print(f\"\\n\u2713 Data ready at: {data_path}\")\n",
    "else:\n",
    "    data_path = \"./data/english\"\n",
    "    print(f\"Using local data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare training data\n",
    "# For demonstration, using sample data\n",
    "# Replace with actual SwiftKey corpus processing\n",
    "\n",
    "sample_sentences = [\n",
    "    \"Today is a beautiful day\",\n",
    "    \"I love programming in Python\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Let's meet tomorrow morning\",\n",
    "    \"Thank you for your help\",\n",
    "    \"How are you doing today\",\n",
    "    \"See you later tonight\",\n",
    "    \"Have a great weekend\",\n",
    "]\n",
    "\n",
    "# Clean text\n",
    "cleaned = [clean_english_text(s) for s in sample_sentences]\n",
    "\n",
    "# Augment with emojis\n",
    "augmented = augment_with_emojis(cleaned, emoji_ratio=0.2)\n",
    "\n",
    "print(f\"Sample augmented sentences:\")\n",
    "for sent in augmented[:5]:\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(augmented)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization for Colab free tier\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear any existing allocations\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory allocation strategy\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"\u2713 Memory optimizations applied\")\n",
    "print(f\"\u2713 GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2713 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Authentication (required for Gemma-2)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "# This will use HF_TOKEN from Colab secrets or prompt for manual login\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, new_session=False)\n",
    "    print(\"\u2713 Logged in to Hugging Face using token\")\n",
    "except:\n",
    "    # Fallback to interactive login\n",
    "    print(\"HF_TOKEN not found, using interactive login...\")\n",
    "    login(new_session=False)\n",
    "    print(\"\u2713 Logged in to Hugging Face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with LoRA\n",
    "# Using Gemma-2-2B with official configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load with official Gemma-2 settings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with official Gemma-2 dtype (bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Official Gemma-2 dtype\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\u2713 Model loaded with LoRA adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors=None  # Return lists, not tensors (for batching)\n",
    "    )\n",
    "\n",
    "# Create dataset\n",
    "train_data = Dataset.from_dict({'text': augmented})\n",
    "train_dataset = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text column\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Training dataset prepared: {len(train_dataset)} samples\")\n",
    "print(f\"\u2713 Dataset columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Set checkpoint directory to Drive if in Colab\n",
    "if IN_COLAB:\n",
    "    checkpoint_dir = os.path.join(DRIVE_BASE, \"checkpoints\", \"english\")\n",
    "else:\n",
    "    checkpoint_dir = \"./checkpoints/english\"\n",
    "\n",
    "trainer = train_causal_lm(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,  # Reduced for free tier\n",
    "    learning_rate=1e-5,\n",
    "    max_seq_length=8,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights\n",
    "model = merge_lora_weights(model)\n",
    "print(\"\u2713 LoRA weights merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune model (optional - skip if running out of memory)\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # Clear GPU cache before pruning\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Attempting to prune model...\")\n",
    "    model = prune_model(model, amount=0.2)  # Reduced from 0.3 to 0.2\n",
    "    print(\"\u2713 Model pruned\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Pruning failed (likely OOM): {e}\")\n",
    "    print(\"Skipping pruning - model will be larger but still functional\")\n",
    "    # Continue without pruning\n",
    "    pass\n",
    "\n",
    "# Clear cache again\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model (optional - may cause OOM on free tier)\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear memory before quantization\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    print(\"Attempting quantization...\")\n",
    "    model = quantize_model(model, dtype=torch.qint8)\n",
    "    print(\"\u2713 Model quantized\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Quantization failed: {e}\")\n",
    "    print(\"Skipping quantization - exporting float16 model instead\")\n",
    "    # Continue with float16 model\n",
    "\n",
    "# Clear memory again\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set model output directory\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(DRIVE_BASE, \"models\", \"english\")\n",
    "else:\n",
    "    model_dir = \"./models/english\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Clear memory before export\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    print(\"Exporting to ONNX...\")\n",
    "    onnx_path = export_to_onnx(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        output_path=os.path.join(model_dir, \"english_model.onnx\"),\n",
    "        max_seq_length=128\n",
    "    )\n",
    "    print(f\"\u2713 ONNX model saved to: {onnx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 ONNX export failed: {e}\")\n",
    "    print(\"Saving PyTorch model instead...\")\n",
    "    # Save as PyTorch\n",
    "    torch_path = os.path.join(model_dir, \"english_model.pt\")\n",
    "    torch.save(model.state_dict(), torch_path)\n",
    "    print(f\"\u2713 PyTorch model saved to: {torch_path}\")\n",
    "    onnx_path = None\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Core ML (optional - skip if low on memory)\n",
    "import gc\n",
    "\n",
    "if onnx_path:  # Only if ONNX export succeeded\n",
    "    try:\n",
    "        print(\"Exporting to Core ML...\")\n",
    "        coreml_path = export_to_coreml(\n",
    "            onnx_path=onnx_path,\n",
    "            output_path=os.path.join(model_dir, \"english_model.mlmodel\"),\n",
    "            model_name=\"EnglishKeyboardSuggestion\"\n",
    "        )\n",
    "        if coreml_path:\n",
    "            print(f\"\u2713 Core ML model saved to: {coreml_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 Core ML export failed: {e}\")\n",
    "        print(\"Skipping Core ML export\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping Core ML export (ONNX export failed)\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model size\n",
    "size_mb, meets_req = verify_model_size(\n",
    "    model_path=onnx_path,\n",
    "    max_size_mb=30\n",
    ")\n",
    "\n",
    "if meets_req:\n",
    "    print(f\"\u2713 Model size requirement met: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\u26a0 Model size exceeds target: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for download\n",
    "zip_path = package_for_download(\n",
    "    model_dir=model_dir,\n",
    "    output_zip=\"english_model.zip\"\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model packaged: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send completion notification\n",
    "send_notification_email(\n",
    "    subject=\"English Model Training Complete! \ud83c\udf89\",\n",
    "    message=f\"\"\"\n",
    "English keyboard suggestion model training has completed successfully!\n",
    "\n",
    "Model Details:\n",
    "- Size: {size_mb:.2f} MB\n",
    "- Location: {model_dir}\n",
    "- Package: {zip_path}\n",
    "\n",
    "The model is ready for integration into your keyboard app.\n",
    "\n",
    "Next steps:\n",
    "1. Download the model package\n",
    "2. Integrate into iOS/Android app\n",
    "3. Test on actual devices\n",
    "    \"\"\",\n",
    "    to_email=\"phamminhphueur@gmail.com\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Package: {zip_path}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading model package...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"\u2713 Download started\")\n",
    "else:\n",
    "    print(f\"Model saved locally to: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}