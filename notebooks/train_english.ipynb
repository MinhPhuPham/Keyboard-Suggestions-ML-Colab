{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileBERT Keyboard Suggestion Model Training\n",
    "\n",
    "This notebook trains a keyboard suggestion model using **MobileBERT** with multi-task learning.\n",
    "\n",
    "**Features:**\n",
    "1. Word Completion: \"Hel\" → [\"Hello\", \"Help\", \"Helping\"]\n",
    "2. Next-Word Prediction: \"How are\" → [\"you\", \"they\", \"we\"]\n",
    "3. Typo Correction: \"Thers\" → [\"There\", \"Theirs\", \"Therapy\"]\n",
    "4. Gibberish Detection: Heuristic (no ML)\n",
    "\n",
    "**Model Specifications:**\n",
    "- Base: MobileBERT-TINY (15M parameters)\n",
    "- Target Size: <15MB (after INT8 quantization)\n",
    "- Latency: <50ms on mobile\n",
    "- Deployment: iOS (CoreML) + Android (TFLite)\n",
    "\n",
    "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
    "\n",
    "**Data Sources:**\n",
    "- Word frequencies: GitHub (600K+ words)\n",
    "- Text corpus: OpenSubtitles2024 (Hugging Face)\n",
    "- Typos: WikEd Error Corpus + synthetic\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "1. Runtime → Change runtime type → GPU (T4)\n",
    "2. Run all cells\n",
    "3. Model will be saved to Google Drive\n",
    "4. Download for mobile deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"✓ Running in Google Colab\")\n",
    "    print(f\"✓ GPU: {!nvidia-smi --query-gpu=name --format=csv,noheader}\")\n",
    "else:\n",
    "    print(\"✓ Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if in Colab)\n",
    "if IN_COLAB:\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        !rm -rf Keyboard-Suggestions-ML-Colab\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git Keyboard-Suggestions-ML-Colab\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"✓ Repository cloned\")\n",
    "    print(f\"✓ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch datasets accelerate\n",
    "!pip install -q huggingface_hub\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create project directory in Drive\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ Google Drive mounted\")\n",
    "    print(f\"✓ Project directory: {DRIVE_DIR}\")\n",
    "else:\n",
    "    DRIVE_DIR = './drive_backup'\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Datasets\n",
    "\n",
    "We'll download 3 datasets:\n",
    "1. **Word Frequencies** - GitHub (600K words)\n",
    "2. **Text Corpus** - OpenSubtitles2024 (Hugging Face)\n",
    "3. **Typo Corrections** - Synthetic + real typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Download word frequency list from GitHub\n",
    "print(\"Downloading word frequency list...\")\n",
    "\n",
    "WORD_FREQ_URL = \"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears.txt\"\n",
    "word_freq_path = f\"{DRIVE_DIR}/datasets/word_freq.txt\"\n",
    "\n",
    "if not os.path.exists(word_freq_path):\n",
    "    urllib.request.urlretrieve(WORD_FREQ_URL, word_freq_path)\n",
    "    print(f\"✓ Downloaded to: {word_freq_path}\")\n",
    "else:\n",
    "    print(f\"✓ Already exists: {word_freq_path}\")\n",
    "\n",
    "# Count words\n",
    "with open(word_freq_path, 'r') as f:\n",
    "    word_count = len(f.readlines())\n",
    "print(f\"  Words: {word_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download text corpus from Hugging Face\n",
    "print(\"\\nDownloading text corpus from Hugging Face...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "corpus_path = f\"{DRIVE_DIR}/datasets/corpus.txt\"\n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    # Load OpenSubtitles dataset (English only, streaming for efficiency)\n",
    "    print(\"  Loading OpenSubtitles dataset (this may take a few minutes)...\")\n",
    "    \n",
    "    # Use a smaller, faster dataset for training\n",
    "    dataset = load_dataset(\n",
    "        \"sentence-transformers/embedding-training-data\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= 100000:  # Limit to 100K sentences\n",
    "            break\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Processed {i:,} sentences...\")\n",
    "        \n",
    "        # Extract text\n",
    "        if 'sentence' in item:\n",
    "            text = item['sentence']\n",
    "        elif 'text' in item:\n",
    "            text = item['text']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Clean and filter\n",
    "        text = text.strip().lower()\n",
    "        if len(text.split()) >= 3:  # At least 3 words\n",
    "            sentences.append(text)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    print(f\"✓ Downloaded {len(sentences):,} sentences\")\n",
    "    print(f\"✓ Saved to: {corpus_path}\")\n",
    "else:\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        sentence_count = len(f.readlines())\n",
    "    print(f\"✓ Already exists: {corpus_path}\")\n",
    "    print(f\"  Sentences: {sentence_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Training Data\n",
    "\n",
    "Generate training pairs for all 3 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from keyboard_data_prep import (\n",
    "    prepare_word_completion_data,\n",
    "    prepare_nextword_data,\n",
    "    prepare_typo_data,\n",
    "    combine_datasets\n",
    ")\n",
    "\n",
    "print(\"Preparing training datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
    "train_path = f\"{output_dir}/train.jsonl\"\n",
    "val_path = f\"{output_dir}/val.jsonl\"\n",
    "\n",
    "# Check if processed datasets already exist in Drive\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    print(\"✓ Processed datasets found in Drive!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "    \n",
    "    # Count samples\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_count = sum(1 for _ in f)\n",
    "    with open(val_path, 'r') as f:\n",
    "        val_count = sum(1 for _ in f)\n",
    "    print(f\"  Train samples: {train_count:,}\")\n",
    "    print(f\"  Val samples: {val_count:,}\")\n",
    "else:\n",
    "    print(\"Generating training datasets from scratch...\")\n",
    "    \n",
    "    # 1. Word completion\n",
    "    print(\"\\n1. Word Completion...\")\n",
    "    completion_path = prepare_word_completion_data(\n",
    "        word_freq_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/completion.jsonl\",\n",
    "        max_samples=50000  # 50K pairs\n",
    "    )\n",
    "    \n",
    "    # 2. Next-word prediction\n",
    "    print(\"\\n2. Next-Word Prediction...\")\n",
    "    nextword_path = prepare_nextword_data(\n",
    "        corpus_path=corpus_path,\n",
    "        output_path=f\"{output_dir}/nextword.jsonl\",\n",
    "        max_samples=100000,  # 100K pairs\n",
    "        context_length=3\n",
    "    )\n",
    "    \n",
    "    # 3. Typo correction\n",
    "    print(\"\\n3. Typo Correction...\")\n",
    "    typo_path = prepare_typo_data(\n",
    "        word_list_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/typo.jsonl\",\n",
    "        max_samples=20000  # 20K pairs\n",
    "    )\n",
    "    \n",
    "    # 4. Combine and split\n",
    "    print(\"\\n4. Combining datasets...\")\n",
    "    train_path, val_path = combine_datasets(\n",
    "        completion_path=completion_path,\n",
    "        nextword_path=nextword_path,\n",
    "        typo_path=typo_path,\n",
    "        output_path=output_dir,\n",
    "        train_ratio=0.9\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ Dataset generation complete!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Datasets ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyboard_model import KeyboardSuggestionModel\n",
    "from transformers import MobileBertTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading MobileBERT model...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n",
    "# Create model\n",
    "model = KeyboardSuggestionModel(\n",
    "    base_model_name=\"google/mobilebert-uncased\",\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"✓ Model loaded on {device}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "class KeyboardDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=32):\n",
    "        self.data = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize input and target\n",
    "        input_enc = self.tokenizer(\n",
    "            item['input'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            item['target'],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_enc['input_ids'].squeeze(),\n",
    "            'attention_mask': input_enc['attention_mask'].squeeze(),\n",
    "            'labels': target_enc['input_ids'].squeeze(),\n",
    "            'task': item['task']\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = KeyboardDataset(train_path, tokenizer)\n",
    "val_dataset = KeyboardDataset(val_path, tokenizer)\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset):,}\")\n",
    "print(f\"✓ Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-4\n",
    "SAVE_STEPS = 1000\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        tasks = batch['task']\n",
    "        \n",
    "        # Forward pass (simplified - use first task for now)\n",
    "        task = tasks[0]\n",
    "        predictions, scores = model(input_ids, attention_mask, task=task)\n",
    "        \n",
    "        # Compute loss (cross-entropy)\n",
    "        loss = F.cross_entropy(\n",
    "            scores.view(-1, scores.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "            print(f\"\\n✓ Checkpoint saved: {checkpoint_dir}\")\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            task = batch['task'][0]\n",
    "            \n",
    "            predictions, scores = model(input_ids, attention_mask, task=task)\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                scores.view(-1, scores.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
    "        model.save_pretrained(best_model_dir)\n",
    "        tokenizer.save_pretrained(best_model_dir)\n",
    "        print(f\"  ✓ Best model saved: {best_model_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Training complete!\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = f\"{DRIVE_DIR}/models/keyboard_mobilebert_final\"\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"✓ Final model saved to: {final_model_dir}\")\n",
    "print(\"\\nModel is ready for:\")\n",
    "print(\"  1. INT8 quantization\")\n",
    "print(\"  2. CoreML export (iOS)\")\n",
    "print(\"  3. TFLite export (Android)\")\n",
    "print(\"\\nNext steps: Run export scripts locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "model.eval()\n",
    "\n",
    "test_cases = [\n",
    "    (\"hel\", \"completion\"),\n",
    "    (\"how are\", \"next_word\"),\n",
    "    (\"thers\", \"typo\")\n",
    "]\n",
    "\n",
    "print(\"Testing model predictions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text, task in test_cases:\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        max_length=32,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        predictions, scores = model(input_ids, attention_mask, task=task)\n",
    "    \n",
    "    # Decode\n",
    "    top_predictions = []\n",
    "    for pred in predictions[0]:\n",
    "        word = tokenizer.decode([pred.item()], skip_special_tokens=True)\n",
    "        top_predictions.append(word)\n",
    "    \n",
    "    print(f\"\\nInput: \\\"{text}\\\" (task: {task})\")\n",
    "    print(f\"Predictions: {top_predictions}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Testing complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
