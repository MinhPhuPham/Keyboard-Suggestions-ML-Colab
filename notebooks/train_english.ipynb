{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileBERT Keyboard Suggestion Model Training\n",
    "\n",
    "This notebook trains a keyboard suggestion model using **MobileBERT** with multi-task learning.\n",
    "\n",
    "**Features:**\n",
    "1. Word Completion: \"Hel\" â†’ [\"Hello\", \"Help\", \"Helping\"]\n",
    "2. Next-Word Prediction: \"How are\" â†’ [\"you\", \"they\", \"we\"]\n",
    "3. Typo Correction: \"Thers\" â†’ [\"There\", \"Theirs\", \"Therapy\"]\n",
    "4. Gibberish Detection: Heuristic (no ML)\n",
    "\n",
    "**Model Specifications:**\n",
    "- Base: MobileBERT-TINY (15M parameters)\n",
    "- Target Size: <15MB (after INT8 quantization)\n",
    "- Latency: <50ms on mobile\n",
    "- Deployment: iOS (CoreML) + Android (TFLite)\n",
    "\n",
    "**Training Time:** 2-4 hours on Colab GPU (T4)\n",
    "\n",
    "**Data Sources:**\n",
    "- Word frequencies: GitHub (600K+ words)\n",
    "- Text corpus: OpenSubtitles2024 (Hugging Face)\n",
    "- Typos: WikEd Error Corpus + synthetic\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. Run all cells\n",
    "3. Model will be saved to Google Drive\n",
    "4. Download for mobile deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"âœ“ Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Define Drive directory\n",
    "    DRIVE_DIR = '/content/drive/MyDrive/Keyboard-Suggestions-ML-Colab'\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)\n",
    "    \n",
    "    print(f\"âœ“ Google Drive mounted\")\n",
    "    print(f\"âœ“ Project directory: {DRIVE_DIR}\")\n",
    "else:\n",
    "    print(\"âœ“ Running locally\")\n",
    "    DRIVE_DIR = './data'  # Local fallback\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_DIR}/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Ensure we're in /content\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo if it exists (for re-runs)\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        import shutil\n",
    "        shutil.rmtree('Keyboard-Suggestions-ML-Colab')\n",
    "        print(\"âœ“ Removed existing repository\")\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"âœ“ Repository cloned\")\n",
    "    print(f\"âœ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Datasets\n",
    "\n",
    "We'll download 3 datasets:\n",
    "1. **Word Frequencies** - GitHub (600K words)\n",
    "2. **Text Corpus** - OpenSubtitles2024 (Hugging Face)\n",
    "3. **Typo Corrections** - Synthetic + real typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Download word frequency list from GitHub\n",
    "print(\"Downloading word frequency list...\")\n",
    "\n",
    "WORD_FREQ_URL = \"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa-no-swears.txt\"\n",
    "word_freq_path = f\"{DRIVE_DIR}/datasets/word_freq.txt\"\n",
    "\n",
    "if not os.path.exists(word_freq_path):\n",
    "    urllib.request.urlretrieve(WORD_FREQ_URL, word_freq_path)\n",
    "    print(f\"âœ“ Downloaded to: {word_freq_path}\")\n",
    "else:\n",
    "    print(f\"âœ“ Already exists: {word_freq_path}\")\n",
    "\n",
    "# Count words\n",
    "with open(word_freq_path, 'r') as f:\n",
    "    word_count = len(f.readlines())\n",
    "print(f\"  Words: {word_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download text corpus from Hugging Face\n",
    "print(\"\\nDownloading text corpus from Hugging Face...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "corpus_path = f\"{DRIVE_DIR}/datasets/corpus.txt\"\n",
    "\n",
    "if not os.path.exists(corpus_path):\n",
    "    # Load OpenSubtitles dataset (English only, streaming for efficiency)\n",
    "    print(\"  Loading OpenSubtitles dataset (this may take a few minutes)...\")\n",
    "    \n",
    "    # Use a smaller, faster dataset for training\n",
    "    dataset = load_dataset(\n",
    "        \"sentence-transformers/embedding-training-data\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= 100000:  # Limit to 100K sentences\n",
    "            break\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Processed {i:,} sentences...\")\n",
    "        \n",
    "        # Extract text\n",
    "        if 'sentence' in item:\n",
    "            text = item['sentence']\n",
    "        elif 'text' in item:\n",
    "            text = item['text']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Clean and filter\n",
    "        text = text.strip().lower()\n",
    "        if len(text.split()) >= 3:  # At least 3 words\n",
    "            sentences.append(text)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    print(f\"âœ“ Downloaded {len(sentences):,} sentences\")\n",
    "    print(f\"âœ“ Saved to: {corpus_path}\")\n",
    "else:\n",
    "    with open(corpus_path, 'r') as f:\n",
    "        sentence_count = len(f.readlines())\n",
    "    print(f\"âœ“ Already exists: {corpus_path}\")\n",
    "    print(f\"  Sentences: {sentence_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Training Data\n",
    "\n",
    "Generate training pairs for all 3 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from keyboard_data_prep import (\n",
    "    prepare_word_completion_data,\n",
    "    prepare_nextword_data,\n",
    "    prepare_typo_data,\n",
    "    combine_datasets\n",
    ")\n",
    "\n",
    "print(\"Preparing training datasets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_dir = f\"{DRIVE_DIR}/datasets/processed\"\n",
    "train_path = f\"{output_dir}/train.jsonl\"\n",
    "val_path = f\"{output_dir}/val.jsonl\"\n",
    "\n",
    "# Check if processed datasets already exist in Drive\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    print(\"âœ“ Processed datasets found in Drive!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "    \n",
    "    # Count samples\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_count = sum(1 for _ in f)\n",
    "    with open(val_path, 'r') as f:\n",
    "        val_count = sum(1 for _ in f)\n",
    "    print(f\"  Train samples: {train_count:,}\")\n",
    "    print(f\"  Val samples: {val_count:,}\")\n",
    "else:\n",
    "    print(\"Generating training datasets from scratch...\")\n",
    "    \n",
    "    # 1. Word completion\n",
    "    print(\"\\n1. Word Completion...\")\n",
    "    completion_path = prepare_word_completion_data(\n",
    "        word_freq_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/completion.jsonl\",\n",
    "        max_samples=50000  # 50K pairs\n",
    "    )\n",
    "    \n",
    "    # 2. Next-word prediction\n",
    "    print(\"\\n2. Next-Word Prediction...\")\n",
    "    nextword_path = prepare_nextword_data(\n",
    "        corpus_path=corpus_path,\n",
    "        output_path=f\"{output_dir}/nextword.jsonl\",\n",
    "        max_samples=100000,  # 100K pairs\n",
    "        context_length=3\n",
    "    )\n",
    "    \n",
    "    # 3. Typo correction\n",
    "    print(\"\\n3. Typo Correction...\")\n",
    "    typo_path = prepare_typo_data(\n",
    "        word_list_path=word_freq_path,\n",
    "        output_path=f\"{output_dir}/typo.jsonl\",\n",
    "        max_samples=20000  # 20K pairs\n",
    "    )\n",
    "    \n",
    "    # 4. Combine and split\n",
    "    print(\"\\n4. Combining datasets...\")\n",
    "    train_path, val_path = combine_datasets(\n",
    "        completion_path=completion_path,\n",
    "        nextword_path=nextword_path,\n",
    "        typo_path=typo_path,\n",
    "        output_path=output_dir,\n",
    "        train_ratio=0.9\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ Dataset generation complete!\")\n",
    "    print(f\"  Train: {train_path}\")\n",
    "    print(f\"  Val: {val_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Datasets ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileBertForMaskedLM, MobileBertTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Loading MobileBERT for Masked Language Modeling...\")\n",
    "\n",
    "tokenizer = MobileBertTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "model = MobileBertForMaskedLM.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ“ Model loaded on {device}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model size: ~100MB (FP32) â†’ ~12-15MB (INT8 quantized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class KeyboardDataset(Dataset):\n",
    "    \"\"\"BERT MLM dataset for keyboard suggestions.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, tokenizer, max_length=32):\n",
    "        self.data = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text_input = item['input']\n",
    "        target_word = item['target']\n",
    "        task = item.get('task', 'completion')  # Default to prevent KeyError\n",
    "        \n",
    "        # Add [MASK] for prediction - KEEP the input context!\n",
    "        # For ALL tasks: input + [MASK]\n",
    "        # - Completion: \"Hel [MASK]\" â†’ predict \"lo\" (Hello)\n",
    "        # - Next-word: \"How are [MASK]\" â†’ predict \"you\"\n",
    "        # - Typo: \"Thers [MASK]\" â†’ predict \"There\" (model sees the typo!)\n",
    "        text_input = f\"{text_input} {self.tokenizer.mask_token}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text_input,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Labels: -100 everywhere except [MASK]\n",
    "        labels = torch.full(inputs['input_ids'].shape, -100, dtype=torch.long)\n",
    "        \n",
    "        # Get target token ID\n",
    "        target_tokens = self.tokenizer.tokenize(target_word)\n",
    "        target_id = self.tokenizer.convert_tokens_to_ids(target_tokens[0]) if target_tokens else self.tokenizer.unk_token_id\n",
    "        \n",
    "        # Set label at [MASK] position\n",
    "        mask_positions = (inputs['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "        if len(mask_positions[1]) > 0:\n",
    "            labels[0, mask_positions[1][0]] = target_id\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze(),\n",
    "            'task': task\n",
    "        }\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = KeyboardDataset(train_path, tokenizer)\n",
    "val_dataset = KeyboardDataset(val_path, tokenizer)\n",
    "\n",
    "print(f\"âœ“ Train samples: {len(train_dataset):,}\")\n",
    "print(f\"âœ“ Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-4\n",
    "SAVE_STEPS = 1000\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        global_step += 1\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        if global_step % SAVE_STEPS == 0:\n",
    "            checkpoint_dir = f\"{DRIVE_DIR}/models/checkpoint-{global_step}\"\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "            print(f\"\\nâœ“ Checkpoint saved: {checkpoint_dir}\")\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"  Val loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_dir = f\"{DRIVE_DIR}/models/best_model\"\n",
    "        model.save_pretrained(best_model_dir)\n",
    "        tokenizer.save_pretrained(best_model_dir)\n",
    "        print(f\"  âœ“ Best model saved: {best_model_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Training complete!\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = f\"{DRIVE_DIR}/models/keyboard_mobilebert_final\"\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"âœ“ Final model saved to: {final_model_dir}\")\n",
    "print(\"\\nModel is ready for:\")\n",
    "print(\"  1. INT8 quantization\")\n",
    "print(\"  2. CoreML export (iOS)\")\n",
    "print(\"  3. TFLite export (Android)\")\n",
    "print(\"\\nNext steps: Run export scripts locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with configurable top-k (3 or 30)\n",
    "model.eval()\n",
    "\n",
    "def predict_keyboard(text, top_k=3):\n",
    "    \"\"\"Predict top-k words at [MASK] position.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding='max_length', max_length=32, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    mask_positions = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "    if len(mask_positions[1]) > 0:\n",
    "        mask_pos = mask_positions[1][0]\n",
    "        mask_predictions = predictions[0, mask_pos]\n",
    "        top_k_results = torch.topk(mask_predictions, k=min(top_k, mask_predictions.size(0)))\n",
    "        return [tokenizer.decode([idx]) for idx in top_k_results.indices]\n",
    "    return []\n",
    "\n",
    "test_cases = [\n",
    "    (\"hel [MASK]\", \"completion\"),      # Should predict \"hello\", \"help\"\n",
    "    (\"how are [MASK]\", \"next_word\"),   # Should predict \"you\"\n",
    "    (\"thers [MASK]\", \"typo\")           # <--- FIX: Give it the typo \"thers\" to correct to \"there\"\n",
    "]\n",
    "\n",
    "print(\"Testing model predictions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text, task in test_cases:\n",
    "    print(f\"\\nInput: \\\"{text}\\\" (task: {task})\")\n",
    "    print(f\"  Top-3:  {predict_keyboard(text, top_k=3)}\")\n",
    "    print(f\"  Top-30: {predict_keyboard(text, top_k=30)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ Testing complete!\")\n",
    "print(\"\\nðŸ’¡ Usage:\")\n",
    "print(\"  predict_keyboard('hello [MASK]', top_k=3)   # Quick suggestions\")\n",
    "print(\"  predict_keyboard('hello [MASK]', top_k=30)  # Extended suggestions\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
