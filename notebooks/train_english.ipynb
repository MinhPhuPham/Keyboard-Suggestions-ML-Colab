{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Keyboard Suggestion Model Training (with Google Drive Integration)\n",
    "\n",
    "This notebook trains an English keyboard suggestion model using Microsoft Phi-3 Mini with LoRA fine-tuning.\n",
    "\n",
    "**Features:**\n",
    "- Automatic Google Drive data management\n",
    "- Checks for existing data before downloading\n",
    "- Email notifications on completion\n",
    "- Saves models to Drive for persistence\n",
    "\n",
    "**Target Specifications:**\n",
    "- Model Size: 20-30 MB (after optimization)\n",
    "- Latency: < 50 ms\n",
    "- Perplexity: < 20\n",
    "- Top-3 Accuracy: > 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\u2713 Running in Google Colab\")\n",
    "else:\n",
    "    print(\"\u2713 Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Ensure we're in /content\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Remove existing repo if it exists (for re-runs)\n",
    "    if os.path.exists('Keyboard-Suggestions-ML-Colab'):\n",
    "        import shutil\n",
    "        shutil.rmtree('Keyboard-Suggestions-ML-Colab')\n",
    "        print(\"\u2713 Removed existing repository\")\n",
    "    \n",
    "    # Clone fresh copy\n",
    "    !git clone https://github.com/MinhPhuPham/Keyboard-Suggestions-ML-Colab.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Keyboard-Suggestions-ML-Colab')\n",
    "    \n",
    "    print(f\"\u2713 Repository cloned\")\n",
    "    print(f\"\u2713 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Setup and Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    mount_success = mount_google_drive()\n",
    "    if not mount_success:\n",
    "        raise Exception(\"Failed to mount Google Drive\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up English training data\n",
    "# This will check Drive first, download if needed\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Phu's Data development\"\n",
    "    data_path = setup_english_data(DRIVE_BASE)\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"\u26a0 Data setup failed. Please check errors above.\")\n",
    "        print(\"\\nManual setup instructions:\")\n",
    "        print(\"1. Download SwiftKey dataset from Kaggle:\")\n",
    "        print(\"   https://www.kaggle.com/datasets/therohk/tweets-blogs-news-swiftkey-dataset-4million\")\n",
    "        print(f\"2. Upload to: {DRIVE_BASE}/data/english/\")\n",
    "    else:\n",
    "        print(f\"\\n\u2713 Data ready at: {data_path}\")\n",
    "else:\n",
    "    data_path = \"./data/english\"\n",
    "    print(f\"Using local data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare training data\n",
    "# For demonstration, using sample data\n",
    "# Replace with actual SwiftKey corpus processing\n",
    "\n",
    "sample_sentences = [\n",
    "    \"Today is a beautiful day\",\n",
    "    \"I love programming in Python\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Let's meet tomorrow morning\",\n",
    "    \"Thank you for your help\",\n",
    "    \"How are you doing today\",\n",
    "    \"See you later tonight\",\n",
    "    \"Have a great weekend\",\n",
    "]\n",
    "\n",
    "# Clean text\n",
    "cleaned = [clean_english_text(s) for s in sample_sentences]\n",
    "\n",
    "# Augment with emojis\n",
    "augmented = augment_with_emojis(cleaned, emoji_ratio=0.2)\n",
    "\n",
    "print(f\"Sample augmented sentences:\")\n",
    "for sent in augmented[:5]:\n",
    "    print(f\"  {sent}\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(augmented)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization for Colab free tier\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear any existing allocations\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Set memory allocation strategy\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"\u2713 Memory optimizations applied\")\n",
    "print(f\"\u2713 GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2713 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with LoRA\n",
    "# Using Qwen2.5-1.5B-Instruct (ungated, no token needed)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Official Qwen2.5 configuration\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model (Qwen2.5 uses bfloat16 by default)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# LoRA configuration for Qwen2.5\n",
    "# Target modules: q_proj, k_proj, v_proj, o_proj (standard for Qwen)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\u2713 Model loaded with LoRA adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text with proper padding and truncation\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors=None  # Return lists, not tensors (for batching)\n",
    "    )\n",
    "\n",
    "# Create dataset\n",
    "train_data = Dataset.from_dict({'text': augmented})\n",
    "train_dataset = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove original text column\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Training dataset prepared: {len(train_dataset)} samples\")\n",
    "print(f\"\u2713 Dataset columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# IMPORTANT: Save checkpoints locally first (Drive can cause connection errors)\n",
    "checkpoint_dir = \"./checkpoints/english\"\n",
    "\n",
    "trainer = train_causal_lm(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    output_dir=checkpoint_dir,\n",
    "    num_epochs=3,\n",
    "    batch_size=2,  # Reduced for free tier\n",
    "    learning_rate=1e-5,\n",
    "    max_seq_length=8,\n",
    "    save_steps=100\n",
    ")\n",
    "\n",
    "print(\"\u2713 Training complete!\")\n",
    "\n",
    "# Copy final checkpoint to Drive (optional)\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    drive_checkpoint_dir = os.path.join(DRIVE_BASE, \"checkpoints\", \"english\")\n",
    "    try:\n",
    "        print(f\"Copying checkpoint to Drive: {drive_checkpoint_dir}\")\n",
    "        os.makedirs(os.path.dirname(drive_checkpoint_dir), exist_ok=True)\n",
    "        if os.path.exists(drive_checkpoint_dir):\n",
    "            shutil.rmtree(drive_checkpoint_dir)\n",
    "        shutil.copytree(checkpoint_dir, drive_checkpoint_dir)\n",
    "        print(\"\u2713 Checkpoint copied to Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 Could not copy to Drive: {e}\")\n",
    "        print(\"  Checkpoint saved locally only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model\n",
    "print(\"Merging LoRA weights...\")\n",
    "\n",
    "# Use PEFT's built-in merge method\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(\"\u2713 LoRA weights merged into base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (full precision)\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set model output directory\n",
    "if IN_COLAB:\n",
    "    model_dir = os.path.join(DRIVE_BASE, \"models\", \"english\")\n",
    "else:\n",
    "    model_dir = \"./models/english\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Exporting to ONNX...\")\n",
    "\n",
    "# Prepare dummy input\n",
    "dummy_input = tokenizer(\"Hello\", return_tensors=\"pt\", padding=True, max_length=128)\n",
    "model.eval()\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = os.path.join(model_dir, \"english_model.onnx\")\n",
    "\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input['input_ids'],),\n",
    "        onnx_path,\n",
    "        input_names=['input_ids'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}},\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    print(f\"\u2713 ONNX model exported: {onnx_path}\")\n",
    "    \n",
    "    # Check size\n",
    "    size_mb = os.path.getsize(onnx_path) / 1e6\n",
    "    print(f\"  Model size: {size_mb:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 ONNX export failed: {e}\")\n",
    "    onnx_path = None\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize ONNX model (INT8)\n",
    "if onnx_path:\n",
    "    try:\n",
    "        from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "        \n",
    "        print(\"Quantizing ONNX model...\")\n",
    "        \n",
    "        quantized_path = onnx_path.replace('.onnx', '_quantized.onnx')\n",
    "        \n",
    "        quantize_dynamic(\n",
    "            model_input=onnx_path,\n",
    "            model_output=quantized_path,\n",
    "            weight_type=QuantType.QUInt8\n",
    "        )\n",
    "        \n",
    "        print(f\"\u2713 Quantized ONNX model: {quantized_path}\")\n",
    "        \n",
    "        # Check size reduction\n",
    "        original_size = os.path.getsize(onnx_path) / 1e6\n",
    "        quantized_size = os.path.getsize(quantized_path) / 1e6\n",
    "        reduction = (1 - quantized_size/original_size) * 100\n",
    "        \n",
    "        print(f\"  Original: {original_size:.1f} MB\")\n",
    "        print(f\"  Quantized: {quantized_size:.1f} MB\")\n",
    "        print(f\"  Reduction: {reduction:.1f}%\")\n",
    "        \n",
    "        # Use quantized model for further steps\n",
    "        onnx_path = quantized_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 ONNX quantization failed: {e}\")\n",
    "        print(\"  Using non-quantized ONNX model\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping quantization (ONNX export failed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model\n",
    "if onnx_path:\n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        import numpy as np\n",
    "        \n",
    "        print(\"Validating ONNX model...\")\n",
    "        \n",
    "        # Load ONNX model\n",
    "        session = ort.InferenceSession(onnx_path)\n",
    "        \n",
    "        # Test inference\n",
    "        test_input = tokenizer(\"Hello world\", return_tensors=\"pt\", padding=True, max_length=128)\n",
    "        onnx_output = session.run(\n",
    "            None,\n",
    "            {\"input_ids\": test_input['input_ids'].numpy()}\n",
    "        )[0]\n",
    "        \n",
    "        print(\"\u2713 ONNX model validated\")\n",
    "        print(f\"  Output shape: {onnx_output.shape}\")\n",
    "        print(f\"  Sample logits: {onnx_output[0, 0, :5]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 ONNX validation failed: {e}\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping validation (no ONNX model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Core ML (optional - skip if low on memory)\n",
    "import gc\n",
    "\n",
    "if onnx_path:  # Only if ONNX export succeeded\n",
    "    try:\n",
    "        print(\"Exporting to Core ML...\")\n",
    "        coreml_path = export_to_coreml(\n",
    "            onnx_path=onnx_path,\n",
    "            output_path=os.path.join(model_dir, \"english_model.mlmodel\"),\n",
    "            model_name=\"EnglishKeyboardSuggestion\"\n",
    "        )\n",
    "        if coreml_path:\n",
    "            print(f\"\u2713 Core ML model saved to: {coreml_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0 Core ML export failed: {e}\")\n",
    "        print(\"Skipping Core ML export\")\n",
    "else:\n",
    "    print(\"\u26a0 Skipping Core ML export (ONNX export failed)\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model size\n",
    "size_mb, meets_req = verify_model_size(\n",
    "    model_path=onnx_path,\n",
    "    max_size_mb=30\n",
    ")\n",
    "\n",
    "if meets_req:\n",
    "    print(f\"\u2713 Model size requirement met: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\u26a0 Model size exceeds target: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package model for download\n",
    "zip_path = package_for_download(\n",
    "    model_dir=model_dir,\n",
    "    output_zip=\"english_model.zip\"\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model packaged: {zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send completion notification\n",
    "send_notification_email(\n",
    "    subject=\"English Model Training Complete! \ud83c\udf89\",\n",
    "    message=f\"\"\"\n",
    "English keyboard suggestion model training has completed successfully!\n",
    "\n",
    "Model Details:\n",
    "- Size: {size_mb:.2f} MB\n",
    "- Location: {model_dir}\n",
    "- Package: {zip_path}\n",
    "\n",
    "The model is ready for integration into your keyboard app.\n",
    "\n",
    "Next steps:\n",
    "1. Download the model package\n",
    "2. Integrate into iOS/Android app\n",
    "3. Test on actual devices\n",
    "    \"\"\",\n",
    "    to_email=\"phamminhphueur@gmail.com\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "print(f\"Package: {zip_path}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading model package...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"\u2713 Download started\")\n",
    "else:\n",
    "    print(f\"Model saved locally to: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}