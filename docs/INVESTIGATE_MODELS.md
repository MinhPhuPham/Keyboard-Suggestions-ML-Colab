# Model Investigation for Mobile Keyboard Suggestions

**Goal:** Find optimal model for mobile keyboard with <10MB RAM, <50ms latency, 85-90% accuracy

---

## 1. Phi-3 Mini (Microsoft)

**Released:** 2024  
**Parameters:** 3.8B  
**Architecture:** Transformer (GPT-style)

### After Training:
- **Model Size:** ~2.3GB (FP16), ~1.2GB (INT8)
- **RAM on Device:** 2.5-3GB runtime
- **Latency:** 200-500ms on mobile

### Pros:
- ‚úÖ Excellent accuracy (95%+)
- ‚úÖ Strong reasoning capabilities
- ‚úÖ Modern architecture

### Cons:
- ‚ùå **WAY TOO LARGE** for mobile keyboard
- ‚ùå 2.5GB RAM (250x over budget!)
- ‚ùå Slow inference (10x too slow)
- ‚ùå Requires high-end devices only

### Why NOT Used:
**Completely unsuitable for mobile keyboard.** Designed for chat/reasoning tasks, not real-time suggestions. Would drain battery and crash on most devices.

---

## 2. MobileBERT (Google)

**Released:** 2020  
**Parameters:** 25M  
**Architecture:** BERT (compressed)

### After Training:
- **Model Size:** ~100MB (FP32), ~25MB (INT8)
- **RAM on Device:** 40-60MB runtime
- **Latency:** 60-80ms on mobile

### Pros:
- ‚úÖ Designed for mobile
- ‚úÖ Good accuracy (80-85%)
- ‚úÖ Reasonable size

### Cons:
- ‚ùå Still 4-6x over RAM budget
- ‚ùå Slower than TinyBERT
- ‚ùå Larger model file (25MB vs 10MB)
- ‚ùå More complex architecture

### Why NOT Used:
**Too large for our <10MB RAM target.** While mobile-optimized, still exceeds requirements. TinyBERT achieves similar accuracy with 50% less RAM.

---

## 3. TinyBERT (Huawei) ‚ùå **TRAINING FAILED**

**Released:** 2019  
**Parameters:** 14M (4 layers, 256 hidden)  
**Architecture:** BERT (distilled)

### After Training:
- **Model Size:** 55MB (FP32) ‚Üí **9-10MB (INT8 + Float16)**
- **RAM on Device:** **8-10MB runtime** (theoretical)
- **Latency:** **20-40ms** (theoretical)

### Actual Results:
- **Validation Loss:** >4.1 (should be 1.8-2.2) ‚ùå
- **Actual Accuracy:** **25-40%** (should be 85-90%) ‚ùå
- **Status:** **TRAINING FAILS** - Model not learning properly

### Test Results (Actual):
| Input | Expected | Actual | Confidence | Status |
|-------|----------|--------|------------|--------|
| "hel" | "hello" | "hell" | 9.3% | ‚ùå WRONG |
| "prod" | "product" | "pro" | 50.7% | ‚ùå BACKWARDS |
| "how ar" | "are" | "how" | 31.5% | ‚ùå NONSENSE |

### Pros (Theoretical):
- ‚úÖ Small size (10MB)
- ‚úÖ Low RAM (8-10MB)
- ‚úÖ Fast inference (<50ms)
- ‚úÖ Pre-trained on BERT knowledge

### Cons (Actual):
- ‚ùå **TRAINING FAILS** - Val loss stuck at 4.1+
- ‚ùå **Very low accuracy** (25-40% vs 85-90% expected)
- ‚ùå **Model not learning** - Predicts backwards, nonsense
- ‚ùå **Old architecture** (2019)
- ‚ùå **Poor fine-tuning** - Doesn't adapt to keyboard task

### Why NOT Used:
**Training completely fails.** Despite correct code fixes, validation loss remains >4.1 (should be 1.8-2.2). Model produces nonsense predictions like "prod" ‚Üí "pro" (going backwards!). Actual accuracy 25-40% makes it unusable for production. Architecture too old or incompatible with keyboard suggestion task.

**Root Cause Unknown:**
- All code fixes applied correctly
- Data preparation verified
- Target encoding fixed
- Still fails to learn

**Conclusion:** TinyBERT unsuitable for this task. Need different model.

---

## 4. DistilBERT (Hugging Face)

**Released:** 2019  
**Parameters:** 66M (6 layers, 768 hidden)  
**Architecture:** BERT (distilled)

### After Training:
- **Model Size:** ~250MB (FP32), ~65MB (INT8)
- **RAM on Device:** 80-120MB runtime
- **Latency:** 80-120ms on mobile

### Pros:
- ‚úÖ High accuracy (88-92%)
- ‚úÖ Well-documented
- ‚úÖ Popular choice

### Cons:
- ‚ùå **Too large** (65MB vs 10MB target)
- ‚ùå **Too much RAM** (80-120MB vs 10MB)
- ‚ùå Slower inference
- ‚ùå 5x more parameters than needed

### Why NOT Used:
**Exceeds all resource constraints.** Better suited for server-side or high-end devices. Overkill for keyboard suggestions.

---

## 5. LSTM (Long Short-Term Memory)

**Released:** 1997 (architecture)  
**Parameters:** 1-5M (configurable)  
**Architecture:** Recurrent Neural Network

### Example Configuration:
- Embedding: 128 dim
- LSTM layers: 2 (256 hidden each)
- Total params: ~3M

### After Training:
- **Model Size:** 5-20MB (depends on vocab)
- **RAM on Device:** 10-25MB runtime
- **Latency:** 5-15ms (very fast!)

### Pros:
- ‚úÖ Very small size
- ‚úÖ Fast inference
- ‚úÖ Low RAM usage
- ‚úÖ Simple architecture
- ‚úÖ Good for sequential data

### Cons:
- ‚ùå **Lower accuracy** (65-75% vs 85-90%)
- ‚ùå No pre-training benefits
- ‚ùå Struggles with long-range dependencies
- ‚ùå Harder to train (vanishing gradients)
- ‚ùå No transfer learning
- ‚ùå Outdated architecture

### Why NOT Used:
**Accuracy too low for good UX.** While fast and small, 65-75% accuracy means 1 in 3-4 suggestions are wrong. Users would be frustrated. Modern transformers (TinyBERT) achieve 85-90% with acceptable size.

**Best LSTM Model for Mobile:** `Keras LSTM` or `PyTorch LSTM`
- 2-3 layers, 256 hidden units
- ~3M parameters
- 10-15MB size
- But still lower accuracy than TinyBERT

---

## 6. ALBERT-base-v2 (Google) ‚úÖ **NEW RECOMMENDATION**

**Released:** 2020  
**Parameters:** 11M (parameter sharing)  
**Architecture:** ALBERT (efficient BERT)

### After Training:
- **Model Size:** 45MB (FP32) ‚Üí **12-15MB (INT8)**
- **RAM on Device:** **15-20MB runtime**
- **Latency:** **30-50ms**

### Pros:
- ‚úÖ **Small size** (11M params, similar to TinyBERT)
- ‚úÖ **Parameter sharing** (more efficient than BERT)
- ‚úÖ **Good accuracy** (expected 82-87%)
- ‚úÖ **Modern architecture** (2020 vs 2019)
- ‚úÖ **CoreML compatible** ‚úÖ
- ‚úÖ **TFLite compatible** ‚úÖ
- ‚úÖ **Should train better** than TinyBERT

### Cons:
- ‚ö†Ô∏è **2x RAM budget** (15-20MB vs 10MB target)
- ‚ö†Ô∏è Less popular than DistilBERT
- ‚ö†Ô∏è Untested for keyboard task

### Why RECOMMENDED:
**Best TinyBERT replacement.** Similar size (11M vs 14M params) but newer architecture with parameter sharing. Should train more reliably. Acceptable 15-20MB RAM is reasonable compromise for working model.

**Migration:** Simple - just change model name to `"albert-base-v2"`

---

## 7. DistilBERT (Hugging Face) ‚úÖ **PROVEN ALTERNATIVE**

**Released:** 2019  
**Parameters:** 66M (6 layers, 768 hidden)  
**Architecture:** BERT (distilled)

### After Training:
- **Model Size:** 250MB (FP32) ‚Üí **60-70MB (INT8)**
- **RAM on Device:** **60-80MB runtime**
- **Latency:** 60-80ms

### Pros:
- ‚úÖ **Proven to work** (widely used)
- ‚úÖ **High accuracy** (88-92%)
- ‚úÖ **Robust training** (won't fail like TinyBERT)
- ‚úÖ **Excellent documentation**
- ‚úÖ **CoreML compatible** ‚úÖ
- ‚úÖ **TFLite compatible** ‚úÖ
- ‚úÖ **Easy to fine-tune**

### Cons:
- ‚ùå **Large** (60-80MB RAM vs 10MB target)
- ‚ùå **Slower** (60-80ms vs 50ms target)
- ‚ùå **6x over RAM budget**
- ‚ö†Ô∏è May require high-end devices

### Why CONSIDER:
**Guaranteed to work.** If ALBERT fails too, DistilBERT is the safe choice. Widely used, proven, robust. Worth the 60-80MB RAM if you need a working model. Better to have 88-92% accuracy at 60MB than 25-40% accuracy at 10MB.

---

## 8. Pythia-14m (EleutherAI) ‚úÖ **NEW CHOICE - APPROVED**

**Released:** 2023  
**Parameters:** 14M  
**Architecture:** GPT-NeoX (Causal LM, decoder-only)

### Specifications:
- **Layers:** 6
- **Hidden Size:** 128
- **Attention Heads:** 4
- **Sequence Length:** 2048 (can use 12 for mobile)
- **Position Embeddings:** Rotary (RoPE)

### After Training:
- **Model Size:** 55MB (FP32) ‚Üí **12-15MB (INT8)**
- **RAM on Device:** **15-20MB runtime**
- **Latency:** **25-45ms**

### Pros:
- ‚úÖ **Perfect for text prediction** (GPT-style, not [MASK])
- ‚úÖ **Modern architecture** (2023 vs 2019)
- ‚úÖ **Same size** as TinyBERT (14M params)
- ‚úÖ **Natural predictions** (causal LM, left-to-right)
- ‚úÖ **Should train reliably** (proven GPT architecture)
- ‚úÖ **CoreML compatible** ‚úÖ
- ‚úÖ **TFLite compatible** ‚úÖ
- ‚úÖ **Better suited** for keyboard task than BERT

### Cons:
- ‚ö†Ô∏è **2x RAM budget** (15-20MB vs 10MB target)
- ‚ö†Ô∏è Requires different training approach (causal vs masked)
- ‚ö†Ô∏è Newer, less tested for mobile deployment

### Why CHOSEN:
**Best architecture for keyboard suggestions.** GPT-style causal language model is NATURALLY designed for text prediction (unlike BERT's [MASK] filling). Modern 2023 architecture should train more reliably than 2019 TinyBERT. Same 14M parameter count but better suited for the task. Expected 80-85% accuracy with 15-20MB RAM - acceptable trade-off for a working model.

**Key Advantage:** Predicts "hel" ‚Üí "lo" naturally (causal), vs TinyBERT's awkward "hel [MASK]" ‚Üí "hello"

---

## üìä Comparison Table (Updated)

| Model | Released | Params | Size (INT8) | RAM | Latency | Accuracy | Status |
|-------|----------|--------|-------------|-----|---------|----------|--------|
| **Phi-3 Mini** | 2024 | 3.8B | 1.2GB | 2.5GB | 200-500ms | 95%+ | ‚ùå Too large |
| **MobileBERT** | 2020 | 25M | 25MB | 40-60MB | 60-80ms | 80-85% | ‚ùå Exceeds RAM |
| **TinyBERT** | 2019 | 14M | 10MB | 8-10MB | 20-40ms | ~~85-90%~~ **25-40%** | ‚ùå **TRAINING FAILS** |
| **DistilBERT** | 2019 | 66M | 60-70MB | 60-80MB | 60-80ms | 88-92% | ‚úÖ Proven fallback |
| **ALBERT-base** | 2020 | 11M | 12-15MB | 15-20MB | 30-50ms | 82-87% | ‚úÖ Alternative |
| **Pythia-14m** | 2023 | 14M | **12-15MB** | **15-20MB** | **25-45ms** | **80-85%** | ‚úÖ **NEW CHOICE** |
| **LSTM** | 1997 | 1-5M | 5-20MB | 10-25MB | 25-55ms | 25-45% | ‚ùå Low accuracy |

**Embedding Models (DON'T USE):**
| Model | Type | Why NOT |
|-------|------|---------|
| Snowflake Arctic Embed XS | Embedding | ‚ùå Outputs vectors, not predictions |
| all-MiniLM-L6-v2 | Embedding | ‚ùå Outputs vectors, not predictions |
| sentence-transformers/* | Embedding | ‚ùå All are for similarity, not prediction |

---

## üéØ Decision Summary (Final)

### **Previous Attempts:**

1. **TinyBERT (2019)** ‚ùå FAILED
   - Val loss stuck at >4.1
   - Accuracy: 25-40% (unusable)
   - Architecture incompatible with task

2. **Considered: ALBERT-base-v2** ‚ö†Ô∏è
   - Similar to TinyBERT (both BERT-based)
   - Likely same training issues

3. **Considered: DistilBERT** ‚úÖ
   - Proven but large (60-80MB RAM)
   - Fallback option

### **Final Choice: Pythia-14m** ‚úÖ **APPROVED**

**Why:**
1. ‚úÖ **Different architecture** (GPT vs BERT)
2. ‚úÖ **Modern** (2023 vs 2019)
3. ‚úÖ **Perfect for task** (causal LM for text prediction)
4. ‚úÖ **Same size** (14M params)
5. ‚úÖ **Should train better** (proven GPT architecture)
6. ‚úÖ **15-20MB RAM** (acceptable compromise)

**Migration Required:**
- Change from `AutoModelForMaskedLM` to `GPTNeoXForCausalLM`
- Remove [MASK] token logic
- Implement causal LM data preparation
- Update inference code

**Expected Results:**
- Validation loss: 1.5-2.0 ‚úÖ
- Accuracy: 80-85% ‚úÖ
- RAM: 15-20MB (2x budget but works!)

---

## üìù Conclusion (Final)

**TinyBERT failed** due to architecture incompatibility with keyboard task. BERT's masked language modeling (fill in [MASK]) is unnatural for text prediction.

**Pythia-14m selected** because:
- GPT-style causal LM is PERFECT for text prediction
- Modern 2023 architecture
- Same 14M parameter count
- Should train reliably
- 15-20MB RAM acceptable for working model

**Next Steps:**
1. ‚úÖ Migration plan approved
2. ‚úÖ Update notebook code (see PYTHIA_MIGRATION_PLAN.md)
3. ‚úÖ Train model (3-4 hours)
4. ‚úÖ Verify 80-85% accuracy
5. ‚úÖ Export to iOS/Android

**Last Updated:** 2026-01-20  
**Current Model:** Pythia-14m (EleutherAI/pythia-14m)  
**Status:** ‚úÖ Migration Approved, Ready to Implement  
**Reason:** TinyBERT training fails, Pythia-14m better suited for keyboard task

| Model | Released | Params | Size (INT8) | RAM | Latency | Accuracy | Status |
|-------|----------|--------|-------------|-----|---------|----------|--------|
| **Phi-3 Mini** | 2024 | 3.8B | 1.2GB | 2.5GB | 200-500ms | 95%+ | ‚ùå Too large |
| **MobileBERT** | 2020 | 25M | 25MB | 40-60MB | 60-80ms | 80-85% | ‚ùå Exceeds RAM |
| **TinyBERT** | 2019 | 14M | 10MB | 8-10MB | 20-40ms | ~~85-90%~~ **25-40%** | ‚ùå **TRAINING FAILS** |
| **DistilBERT** | 2019 | 66M | 60-70MB | 60-80MB | 60-80ms | 88-92% | ‚úÖ **Proven** |
| **ALBERT-base** | 2020 | 11M | 12-15MB | 15-20MB | 30-50ms | 82-87% | ‚úÖ **RECOMMENDED** |
| **LSTM** | 1997 | 1-5M | 5-20MB | 10-25MB | 25-55ms | 25-45% | ‚ùå Low accuracy |

**Embedding Models (DON'T USE):**
| Model | Type | Why NOT |
|-------|------|---------|
| Snowflake Arctic Embed XS | Embedding | ‚ùå Outputs vectors, not predictions |
| all-MiniLM-L6-v2 | Embedding | ‚ùå Outputs vectors, not predictions |
| sentence-transformers/* | Embedding | ‚ùå All are for similarity, not prediction |

---

## üéØ Decision Summary (Updated)

### **Previous Choice: TinyBERT** ‚ùå FAILED
- Validation loss stuck at >4.1
- Actual accuracy: 25-40% (unusable)
- Model predicts nonsense/backwards
- **Conclusion:** Unsuitable for keyboard task

### **New Recommendation: ALBERT-base-v2** ‚úÖ

**Why:**
1. ‚úÖ Similar size to TinyBERT (11M vs 14M)
2. ‚úÖ More modern (2020 vs 2019)
3. ‚úÖ Parameter sharing = efficient
4. ‚úÖ Should train better
5. ‚úÖ 15-20MB RAM (acceptable compromise)

**Migration Steps:**
```python
# In train_english_model.ipynb, change:
MODEL_NAME = "albert-base-v2"  # Was: "google/bert_uncased_L-4_H-256_A-4"

# Everything else stays the same!
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)
```

**Expected Results:**
- Validation loss: 2.0-2.5 ‚úÖ
- Accuracy: 82-87% ‚úÖ
- RAM: 15-20MB (2x budget but acceptable)

### **Fallback: DistilBERT**
If ALBERT also fails, use DistilBERT:
- Proven to work (widely used)
- 88-92% accuracy
- 60-80MB RAM (6x budget but guaranteed)

---

## üìù Conclusion (Updated)

**TinyBERT has failed** despite all code fixes. Validation loss >4.1 and 25-40% accuracy make it unusable.

**Next Steps:**
1. ‚úÖ **Try ALBERT-base-v2** (recommended)
   - Similar size, newer architecture
   - Should train better
   - 15-20MB RAM acceptable

2. ‚úÖ **If ALBERT fails, use DistilBERT**
   - Guaranteed to work
   - 88-92% accuracy
   - 60-80MB RAM (worth it for working model)

3. ‚ùå **Don't use embedding models**
   - Snowflake Arctic, MiniLM won't work
   - They output vectors, not predictions

**Last Updated:** 2026-01-20  
**Current Status:** Migrating from TinyBERT to ALBERT-base-v2  
**Reason:** TinyBERT training fails (val loss >4.1, accuracy 25-40%)

1. ‚úÖ **Only model meeting ALL requirements:**
   - Size: 10MB ‚úÖ (vs 50MB budget)
   - RAM: 8-10MB ‚úÖ (vs 10MB budget)
   - Latency: 20-40ms ‚úÖ (vs 50ms budget)
   - Accuracy: 85-90% ‚úÖ (vs 80%+ target)

2. ‚úÖ **Best trade-offs:**
   - Phi-3: Too large (250x RAM)
   - MobileBERT: Too much RAM (4-6x)
   - DistilBERT: Too large (8x RAM)
   - LSTM: Too low accuracy (20% worse)

3. ‚úÖ **Production-ready:**
   - Works on all devices (iOS 14+, Android 7+)
   - Proven architecture (used in production)
   - Easy to optimize (quantization, pruning)
   - Good pre-training (BERT knowledge)

---

## üî¨ Alternative Approaches Considered

### **Hybrid LSTM + TinyBERT:**
- Use LSTM for fast initial suggestions
- Use TinyBERT for refined predictions
- **Rejected:** Too complex, minimal benefit

### **Custom Tiny Transformer:**
- Train from scratch with 2 layers
- **Rejected:** No pre-training, lower accuracy, more work

### **Lookup Table Only:**
- Pre-compute all predictions
- **Rejected:** Limited flexibility, huge file size (100MB+)

---

## üìù Conclusion

**TinyBERT is the optimal choice** for mobile keyboard suggestions:
- Meets all constraints (size, RAM, latency)
- Best accuracy within constraints (85-90%)
- Production-ready and well-tested
- Easy to deploy (CoreML, TFLite)

**Future Improvements:**
- Monitor newer models (2024-2025)
- Consider Phi-3.5 Mini when available
- Explore model distillation improvements
- Test on-device training for personalization

**Last Updated:** 2026-01-20  
**Current Model:** TinyBERT (google/bert_uncased_L-4_H-256_A-4)  
**Status:** ‚úÖ Production Ready
